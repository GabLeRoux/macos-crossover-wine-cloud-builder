From 71a22164652e8a654e77aa0c294f2637039f86b9 Mon Sep 17 00:00:00 2001
From: Dean M Greer <38226388+Gcenx@users.noreply.github.com>
Date: Sat, 3 Sep 2022 21:53:50 -0400
Subject: [PATCH] vkd3d: Import upstream release 1.4.

---
 dlls/concrt140/details.c                      |   16 -
 dlls/msvcp90/details.c                        |   16 -
 include/winnt.h                               |  115 +-
 libs/vkd3d/AUTHORS                            |    2 +
 libs/vkd3d/config.h                           |    4 +-
 libs/vkd3d/include/private/vkd3d_common.h     |   75 +
 libs/vkd3d/include/private/vkd3d_debug.h      |    1 +
 libs/vkd3d/include/private/vkd3d_version.h    |    2 +-
 libs/vkd3d/include/vkd3d.h                    |   18 +
 libs/vkd3d/include/vkd3d_shader.h             |   16 +
 libs/vkd3d/include/vkd3d_types.h              |    4 +
 libs/vkd3d/libs/vkd3d-common/debug.c          |   28 +-
 libs/vkd3d/libs/vkd3d-shader/dxbc.c           |    1 +
 libs/vkd3d/libs/vkd3d-shader/hlsl.c           |  123 +-
 libs/vkd3d/libs/vkd3d-shader/hlsl.h           |   14 +-
 libs/vkd3d/libs/vkd3d-shader/hlsl.l           |    6 +-
 libs/vkd3d/libs/vkd3d-shader/hlsl.y           |  830 ++++++++---
 libs/vkd3d/libs/vkd3d-shader/hlsl_codegen.c   |  404 +++++-
 .../libs/vkd3d-shader/hlsl_constant_ops.c     |  200 ++-
 libs/vkd3d/libs/vkd3d-shader/hlsl_sm1.c       |    6 +-
 libs/vkd3d/libs/vkd3d-shader/hlsl_sm4.c       |  648 +++++----
 libs/vkd3d/libs/vkd3d-shader/preproc.l        |    5 +-
 libs/vkd3d/libs/vkd3d-shader/preproc.y        |   34 +-
 libs/vkd3d/libs/vkd3d-shader/sm4.h            |    1 +
 libs/vkd3d/libs/vkd3d-shader/spirv.c          |   18 +-
 libs/vkd3d/libs/vkd3d-shader/trace.c          |    1 +
 .../libs/vkd3d-shader/vkd3d_shader_main.c     |   37 +
 .../libs/vkd3d-shader/vkd3d_shader_private.h  |    5 +
 libs/vkd3d/libs/vkd3d/command.c               | 1277 +++++++++++++----
 libs/vkd3d/libs/vkd3d/device.c                |  403 +++++-
 libs/vkd3d/libs/vkd3d/resource.c              |  497 ++++++-
 libs/vkd3d/libs/vkd3d/state.c                 |  234 ++-
 libs/vkd3d/libs/vkd3d/vkd3d_main.c            |    6 +
 libs/vkd3d/libs/vkd3d/vkd3d_private.h         |  189 ++-
 libs/vkd3d/libs/vkd3d/vulkan_procs.h          |    5 +
 35 files changed, 4135 insertions(+), 1106 deletions(-)

diff --git a/dlls/concrt140/details.c b/dlls/concrt140/details.c
index 72bb74474..8fd186eb7 100644
--- a/dlls/concrt140/details.c
+++ b/dlls/concrt140/details.c
@@ -193,22 +193,6 @@ static void spin_wait(int *counter)
     }
 }
 
-#ifdef _WIN64
-static size_t InterlockedIncrementSizeT(size_t volatile *dest)
-{
-    size_t v;
-
-    do
-    {
-        v = *dest;
-    } while(InterlockedCompareExchange64((LONGLONG*)dest, v+1, v) != v);
-
-    return v+1;
-}
-#else
-#define InterlockedIncrementSizeT(dest) InterlockedIncrement((LONG*)dest)
-#endif
-
 static void CALLBACK queue_push_finally(BOOL normal, void *ctx)
 {
     threadsafe_queue *queue = ctx;
diff --git a/dlls/msvcp90/details.c b/dlls/msvcp90/details.c
index d63a47667..d2a741b49 100644
--- a/dlls/msvcp90/details.c
+++ b/dlls/msvcp90/details.c
@@ -207,22 +207,6 @@ static void spin_wait(int *counter)
     }
 }
 
-#ifdef _WIN64
-static size_t InterlockedIncrementSizeT(size_t volatile *dest)
-{
-    size_t v;
-
-    do
-    {
-        v = *dest;
-    } while(InterlockedCompareExchange64((LONGLONG*)dest, v+1, v) != v);
-
-    return v+1;
-}
-#else
-#define InterlockedIncrementSizeT(dest) InterlockedIncrement((LONG*)dest)
-#endif
-
 static void CALLBACK queue_push_finally(BOOL normal, void *ctx)
 {
     threadsafe_queue *queue = ctx;
diff --git a/include/winnt.h b/include/winnt.h
index 85082ad5e..781f430d9 100644
--- a/include/winnt.h
+++ b/include/winnt.h
@@ -6272,19 +6272,24 @@ typedef enum _PROCESS_MITIGATION_POLICY
 #define BitScanReverse _BitScanReverse
 #define InterlockedAdd _InlineInterlockedAdd
 #define InterlockedAnd _InterlockedAnd
+#define InterlockedAnd64 _InterlockedAnd64
 #define InterlockedCompareExchange _InterlockedCompareExchange
 #define InterlockedCompareExchange64 _InterlockedCompareExchange64
 #define InterlockedCompareExchangePointer _InterlockedCompareExchangePointer
 #define InterlockedDecrement _InterlockedDecrement
 #define InterlockedDecrement16 _InterlockedDecrement16
+#define InterlockedDecrement64 _InterlockedDecrement64
 #define InterlockedExchange _InterlockedExchange
 #define InterlockedExchangeAdd _InterlockedExchangeAdd
 #define InterlockedExchangeAdd64 _InterlockedExchangeAdd64
 #define InterlockedExchangePointer _InterlockedExchangePointer
 #define InterlockedIncrement _InterlockedIncrement
 #define InterlockedIncrement16 _InterlockedIncrement16
+#define InterlockedIncrement64 _InterlockedIncrement64
 #define InterlockedOr _InterlockedOr
+#define InterlockedOr64 _InterlockedOr64
 #define InterlockedXor _InterlockedXor
+#define InterlockedXor64 _InterlockedXor64
 
 #ifdef _MSC_VER
 
@@ -6316,33 +6321,88 @@ short     _InterlockedIncrement16(short volatile*);
 long      _InterlockedOr(long volatile *,long);
 long      _InterlockedXor(long volatile *,long);
 
-static FORCEINLINE long InterlockedAdd( long volatile *dest, long val )
-{
-    return InterlockedExchangeAdd( dest, val ) + val;
-}
-
-#if !defined(__i386__) || _MSC_VER >= 1600
+#ifndef __i386__
 
+#pragma intrinsic(_InterlockedAnd64)
 #pragma intrinsic(_InterlockedCompareExchangePointer)
+#pragma intrinsic(_InterlockedDecrement64)
+#pragma intrinsic(_InterlockedExchangeAdd64)
 #pragma intrinsic(_InterlockedExchangePointer)
-
-void *_InterlockedCompareExchangePointer(void *volatile*,void*,void*);
-void *_InterlockedExchangePointer(void *volatile*,void*);
+#pragma intrinsic(_InterlockedIncrement64)
+#pragma intrinsic(_InterlockedOr64)
+#pragma intrinsic(_InterlockedXor64)
+
+__int64   _InterlockedAnd64(__int64 volatile *, __int64);
+void *    _InterlockedCompareExchangePointer(void *volatile*,void*,void*);
+__int64   _InterlockedDecrement64(__int64 volatile *);
+__int64   _InterlockedExchangeAdd64(__int64 volatile *, __int64);
+void *    _InterlockedExchangePointer(void *volatile*,void*);
+__int64   _InterlockedIncrement64(__int64 volatile *);
+__int64   _InterlockedOr64(__int64 volatile *, __int64);
+__int64   _InterlockedXor64(__int64 volatile *, __int64);
 
 #else
 
+static FORCEINLINE __int64 InterlockedAnd64( __int64 volatile *dest, __int64 val )
+{
+    __int64 prev;
+    do prev = *dest; while (InterlockedCompareExchange64( dest, prev & val, prev ) != prev);
+    return prev;
+}
+
 static FORCEINLINE void * WINAPI InterlockedCompareExchangePointer( void *volatile *dest, void *xchg, void *compare )
 {
     return (void *)_InterlockedCompareExchange( (long volatile*)dest, (long)xchg, (long)compare );
 }
 
+static FORCEINLINE __int64 InterlockedExchangeAdd64( __int64 volatile *dest, __int64 val )
+{
+    __int64 prev;
+    do prev = *dest; while (InterlockedCompareExchange64( dest, prev + val, prev ) != prev);
+    return prev;
+}
+
 static FORCEINLINE void * WINAPI InterlockedExchangePointer( void *volatile *dest, void *val )
 {
     return (void *)_InterlockedExchange( (long volatile*)dest, (long)val );
 }
 
+static FORCEINLINE __int64 InterlockedIncrement64( __int64 volatile *dest )
+{
+    return InterlockedExchangeAdd64( dest, 1 ) + 1;
+}
+
+static FORCEINLINE __int64 InterlockedDecrement64( __int64 volatile *dest )
+{
+    return InterlockedExchangeAdd64( dest, -1 ) - 1;
+}
+
+static FORCEINLINE __int64 InterlockedOr64( __int64 volatile *dest, __int64 val )
+{
+    __int64 prev;
+    do prev = *dest; while (InterlockedCompareExchange64( dest, prev | val, prev ) != prev);
+    return prev;
+}
+
+static FORCEINLINE __int64 InterlockedXor64( __int64 volatile *dest, __int64 val )
+{
+    __int64 prev;
+    do prev = *dest; while (InterlockedCompareExchange64( dest, prev ^ val, prev ) != prev);
+    return prev;
+}
+
 #endif /* __i386__ */
 
+static FORCEINLINE long InterlockedAdd( long volatile *dest, long val )
+{
+    return InterlockedExchangeAdd( dest, val ) + val;
+}
+
+static FORCEINLINE __int64 InterlockedAdd64( __int64 volatile *dest, __int64 val )
+{
+    return InterlockedExchangeAdd64( dest, val ) + val;
+}
+
 #ifdef __i386__
 
 static FORCEINLINE void MemoryBarrier(void)
@@ -6353,10 +6413,7 @@ static FORCEINLINE void MemoryBarrier(void)
 
 #elif defined(__x86_64__)
 
-#pragma intrinsic(_InterlockedExchangeAdd64)
 #pragma intrinsic(__faststorefence)
-
-long long _InterlockedExchangeAdd64(long long volatile *, long long);
 void __faststorefence(void);
 
 static FORCEINLINE void MemoryBarrier(void)
@@ -6399,11 +6456,21 @@ static FORCEINLINE LONG WINAPI InterlockedAdd( LONG volatile *dest, LONG val )
     return __sync_add_and_fetch( dest, val );
 }
 
+static FORCEINLINE LONGLONG WINAPI InterlockedAdd64( LONGLONG volatile *dest, LONGLONG val )
+{
+    return __sync_add_and_fetch( dest, val );
+}
+
 static FORCEINLINE LONG WINAPI InterlockedAnd( LONG volatile *dest, LONG val )
 {
     return __sync_fetch_and_and( dest, val );
 }
 
+static FORCEINLINE LONGLONG WINAPI InterlockedAnd64( LONGLONG volatile *dest, LONGLONG val )
+{
+    return __sync_fetch_and_and( dest, val );
+}
+
 static FORCEINLINE LONG WINAPI InterlockedCompareExchange( LONG volatile *dest, LONG xchg, LONG compare )
 {
     return __sync_val_compare_and_swap( dest, compare, xchg );
@@ -6453,6 +6520,11 @@ static FORCEINLINE short WINAPI InterlockedIncrement16( short volatile *dest )
     return __sync_add_and_fetch( dest, 1 );
 }
 
+static FORCEINLINE LONGLONG WINAPI InterlockedIncrement64( LONGLONG volatile *dest )
+{
+    return __sync_add_and_fetch( dest, 1 );
+}
+
 static FORCEINLINE LONG WINAPI InterlockedDecrement( LONG volatile * HOSTPTR dest )
 {
     return __sync_add_and_fetch( dest, -1 );
@@ -6463,6 +6535,11 @@ static FORCEINLINE short WINAPI InterlockedDecrement16( short volatile *dest )
     return __sync_add_and_fetch( dest, -1 );
 }
 
+static FORCEINLINE LONGLONG WINAPI InterlockedDecrement64( LONGLONG volatile *dest )
+{
+    return __sync_add_and_fetch( dest, -1 );
+}
+
 static FORCEINLINE void * WINAPI InterlockedExchangePointer( void *volatile *dest, void *val )
 {
     void *ret;
@@ -6485,11 +6562,21 @@ static FORCEINLINE LONG WINAPI InterlockedOr( LONG volatile *dest, LONG val )
     return __sync_fetch_and_or( dest, val );
 }
 
+static FORCEINLINE LONGLONG WINAPI InterlockedOr64( LONGLONG volatile *dest, LONGLONG val )
+{
+    return __sync_fetch_and_or( dest, val );
+}
+
 static FORCEINLINE LONG WINAPI InterlockedXor( LONG volatile *dest, LONG val )
 {
     return __sync_fetch_and_xor( dest, val );
 }
 
+static FORCEINLINE LONGLONG WINAPI InterlockedXor64( LONGLONG volatile *dest, LONGLONG val )
+{
+    return __sync_fetch_and_xor( dest, val );
+}
+
 static FORCEINLINE void MemoryBarrier(void)
 {
     __sync_synchronize();
@@ -6525,11 +6612,15 @@ static FORCEINLINE unsigned char InterlockedCompareExchange128( volatile __int64
 
 #endif
 
+#define InterlockedDecrementSizeT(a) InterlockeDecrement64((LONGLONG *)(a))
 #define InterlockedExchangeAddSizeT(a, b) InterlockedExchangeAdd64((LONGLONG *)(a), (b))
+#define InterlockedIncrementSizeT(a) InterlockedIncrement64((LONGLONG *)(a))
 
 #else /* _WIN64 */
 
+#define InterlockedDecrementSizeT(a) InterlockeDecrement((LONG *)(a))
 #define InterlockedExchangeAddSizeT(a, b) InterlockedExchangeAdd((LONG *)(a), (b))
+#define InterlockedIncrementSizeT(a) InterlockedIncrement((LONG *)(a))
 
 #endif /* _WIN64 */
 
diff --git a/libs/vkd3d/AUTHORS b/libs/vkd3d/AUTHORS
index bc2e08f6c..18a8b2073 100644
--- a/libs/vkd3d/AUTHORS
+++ b/libs/vkd3d/AUTHORS
@@ -5,8 +5,10 @@ Atharva Nimbalkar
 Biswapriyo Nath
 Chip Davis
 Conor McCarthy
+David Gow
 Derek Lesho
 Francisco Casas
+Francois Gouget
 Giovanni Mascellani
 Hans-Kristian Arntzen
 Henri Verbeet
diff --git a/libs/vkd3d/config.h b/libs/vkd3d/config.h
index 9b6daff5e..146adb801 100644
--- a/libs/vkd3d/config.h
+++ b/libs/vkd3d/config.h
@@ -1,5 +1,5 @@
 #define PACKAGE_NAME "vkd3d"
-#define PACKAGE_STRING "vkd3d 1.3"
-#define PACKAGE_VERSION "1.3"
+#define PACKAGE_STRING "vkd3d 1.4"
+#define PACKAGE_VERSION "1.4"
 #define PATH_MAX 1024
 #define SONAME_LIBVULKAN "vulkan-1.dll"
diff --git a/libs/vkd3d/include/private/vkd3d_common.h b/libs/vkd3d/include/private/vkd3d_common.h
index 4ee8ccd43..68eb60cf1 100644
--- a/libs/vkd3d/include/private/vkd3d_common.h
+++ b/libs/vkd3d/include/private/vkd3d_common.h
@@ -27,6 +27,7 @@
 #include <limits.h>
 #include <stdbool.h>
 #include <stdint.h>
+#include <stdio.h>
 
 #ifdef _MSC_VER
 #include <intrin.h>
@@ -210,6 +211,10 @@ static inline LONG InterlockedIncrement(LONG volatile *x)
 {
     return __sync_add_and_fetch(x, 1);
 }
+static inline LONG64 InterlockedIncrement64(LONG64 volatile *x)
+{
+    return __sync_add_and_fetch(x, 1);
+}
 static inline LONG InterlockedAdd(LONG volatile *x, LONG val)
 {
     return __sync_add_and_fetch(x, val);
@@ -242,4 +247,74 @@ static inline void vkd3d_parse_version(const char *version, int *major, int *min
 
 HRESULT hresult_from_vkd3d_result(int vkd3d_result);
 
+#ifdef _WIN32
+static inline void *vkd3d_dlopen(const char *name)
+{
+    return LoadLibraryA(name);
+}
+
+static inline void *vkd3d_dlsym(void *handle, const char *symbol)
+{
+    return GetProcAddress(handle, symbol);
+}
+
+static inline int vkd3d_dlclose(void *handle)
+{
+    return FreeLibrary(handle);
+}
+
+static inline const char *vkd3d_dlerror(void)
+{
+    unsigned int error = GetLastError();
+    static char message[256];
+
+    if (FormatMessageA(FORMAT_MESSAGE_FROM_SYSTEM, NULL, error, 0, message, sizeof(message), NULL))
+        return message;
+    sprintf(message, "Unknown error %u.\n", error);
+    return message;
+}
+#elif defined(HAVE_DLFCN_H)
+#include <dlfcn.h>
+
+static inline void *vkd3d_dlopen(const char *name)
+{
+    return dlopen(name, RTLD_NOW);
+}
+
+static inline void *vkd3d_dlsym(void *handle, const char *symbol)
+{
+    return dlsym(handle, symbol);
+}
+
+static inline int vkd3d_dlclose(void *handle)
+{
+    return dlclose(handle);
+}
+
+static inline const char *vkd3d_dlerror(void)
+{
+    return dlerror();
+}
+#else
+static inline void *vkd3d_dlopen(const char *name)
+{
+    return NULL;
+}
+
+static inline void *vkd3d_dlsym(void *handle, const char *symbol)
+{
+    return NULL;
+}
+
+static inline int vkd3d_dlclose(void *handle)
+{
+    return 0;
+}
+
+static inline const char *vkd3d_dlerror(void)
+{
+    return "Not implemented for this platform.\n";
+}
+#endif
+
 #endif  /* __VKD3D_COMMON_H */
diff --git a/libs/vkd3d/include/private/vkd3d_debug.h b/libs/vkd3d/include/private/vkd3d_debug.h
index 8ab653aef..579c33d36 100644
--- a/libs/vkd3d/include/private/vkd3d_debug.h
+++ b/libs/vkd3d/include/private/vkd3d_debug.h
@@ -47,6 +47,7 @@ enum vkd3d_dbg_level
 enum vkd3d_dbg_level vkd3d_dbg_get_level(void);
 
 void vkd3d_dbg_printf(enum vkd3d_dbg_level level, const char *function, const char *fmt, ...) VKD3D_PRINTF_FUNC(3, 4);
+void vkd3d_dbg_set_log_callback(PFN_vkd3d_log callback);
 
 const char *vkd3d_dbg_sprintf(const char *fmt, ...) VKD3D_PRINTF_FUNC(1, 2);
 const char *vkd3d_dbg_vsprintf(const char *fmt, va_list args);
diff --git a/libs/vkd3d/include/private/vkd3d_version.h b/libs/vkd3d/include/private/vkd3d_version.h
index 712ddcf84..ddf661001 100644
--- a/libs/vkd3d/include/private/vkd3d_version.h
+++ b/libs/vkd3d/include/private/vkd3d_version.h
@@ -1 +1 @@
-#define VKD3D_VCS_ID " (git d773dc05c687)"
+#define VKD3D_VCS_ID " (git 9d4df5e70468)"
diff --git a/libs/vkd3d/include/vkd3d.h b/libs/vkd3d/include/vkd3d.h
index e998ee3f6..fcbc7933f 100644
--- a/libs/vkd3d/include/vkd3d.h
+++ b/libs/vkd3d/include/vkd3d.h
@@ -60,6 +60,7 @@ enum vkd3d_api_version
     VKD3D_API_VERSION_1_1,
     VKD3D_API_VERSION_1_2,
     VKD3D_API_VERSION_1_3,
+    VKD3D_API_VERSION_1_4,
 };
 
 typedef HRESULT (*PFN_vkd3d_signal_event)(HANDLE event);
@@ -212,6 +213,20 @@ VKD3D_API HRESULT vkd3d_serialize_versioned_root_signature(const D3D12_VERSIONED
 VKD3D_API HRESULT vkd3d_create_versioned_root_signature_deserializer(const void *data, SIZE_T data_size,
         REFIID iid, void **deserializer);
 
+/**
+ * Set a callback to be called when vkd3d outputs debug logging.
+ *
+ * If NULL, or if this function has not been called, libvkd3d will print all
+ * enabled log output to stderr.
+ *
+ * Calling this function will also set the log callback for libvkd3d-shader.
+ *
+ * \param callback Callback function to set.
+ *
+ * \since 1.4
+ */
+VKD3D_API void vkd3d_set_log_callback(PFN_vkd3d_log callback);
+
 #endif  /* VKD3D_NO_PROTOTYPES */
 
 /*
@@ -255,6 +270,9 @@ typedef HRESULT (*PFN_vkd3d_serialize_versioned_root_signature)(const D3D12_VERS
 typedef HRESULT (*PFN_vkd3d_create_versioned_root_signature_deserializer)(const void *data, SIZE_T data_size,
         REFIID iid, void **deserializer);
 
+/** Type of vkd3d_set_log_callback(). \since 1.4 */
+typedef void (*PFN_vkd3d_set_log_callback)(PFN_vkd3d_log callback);
+
 #ifdef __cplusplus
 }
 #endif  /* __cplusplus */
diff --git a/libs/vkd3d/include/vkd3d_shader.h b/libs/vkd3d/include/vkd3d_shader.h
index 05d235f9d..ebddbba77 100644
--- a/libs/vkd3d/include/vkd3d_shader.h
+++ b/libs/vkd3d/include/vkd3d_shader.h
@@ -45,6 +45,7 @@ enum vkd3d_shader_api_version
     VKD3D_SHADER_API_VERSION_1_1,
     VKD3D_SHADER_API_VERSION_1_2,
     VKD3D_SHADER_API_VERSION_1_3,
+    VKD3D_SHADER_API_VERSION_1_4,
 };
 
 /** The type of a chained structure. */
@@ -1806,6 +1807,18 @@ VKD3D_SHADER_API void vkd3d_shader_free_shader_signature(struct vkd3d_shader_sig
 VKD3D_SHADER_API int vkd3d_shader_preprocess(const struct vkd3d_shader_compile_info *compile_info,
         struct vkd3d_shader_code *out, char **messages);
 
+/**
+ * Set a callback to be called when vkd3d-shader outputs debug logging.
+ *
+ * If NULL, or if this function has not been called, libvkd3d-shader will print
+ * all enabled log output to stderr.
+ *
+ * \param callback Callback function to set.
+ *
+ * \since 1.4
+ */
+VKD3D_SHADER_API void vkd3d_shader_set_log_callback(PFN_vkd3d_log callback);
+
 #endif  /* VKD3D_SHADER_NO_PROTOTYPES */
 
 /** Type of vkd3d_shader_get_version(). */
@@ -1859,6 +1872,9 @@ typedef void (*PFN_vkd3d_shader_free_shader_signature)(struct vkd3d_shader_signa
 typedef void (*PFN_vkd3d_shader_preprocess)(struct vkd3d_shader_compile_info *compile_info,
         struct vkd3d_shader_code *out, char **messages);
 
+/** Type of vkd3d_shader_set_log_callback(). \since 1.4 */
+typedef void (*PFN_vkd3d_shader_set_log_callback)(PFN_vkd3d_log callback);
+
 #ifdef __cplusplus
 }
 #endif  /* __cplusplus */
diff --git a/libs/vkd3d/include/vkd3d_types.h b/libs/vkd3d/include/vkd3d_types.h
index 0e4ff7b77..12ceef42f 100644
--- a/libs/vkd3d/include/vkd3d_types.h
+++ b/libs/vkd3d/include/vkd3d_types.h
@@ -19,6 +19,8 @@
 #ifndef __VKD3D_TYPES_H
 #define __VKD3D_TYPES_H
 
+#include <stdarg.h>
+
 #ifdef __cplusplus
 extern "C" {
 #endif  /* __cplusplus */
@@ -53,6 +55,8 @@ enum vkd3d_result
     VKD3D_FORCE_32_BIT_ENUM(VKD3D_RESULT),
 };
 
+typedef void (*PFN_vkd3d_log)(const char *format, va_list args);
+
 #ifdef _WIN32
 # define VKD3D_IMPORT
 # define VKD3D_EXPORT
diff --git a/libs/vkd3d/libs/vkd3d-common/debug.c b/libs/vkd3d/libs/vkd3d-common/debug.c
index 4868f3fba..0b9e765ac 100644
--- a/libs/vkd3d/libs/vkd3d-common/debug.c
+++ b/libs/vkd3d/libs/vkd3d-common/debug.c
@@ -68,6 +68,25 @@ enum vkd3d_dbg_level vkd3d_dbg_get_level(void)
     return level;
 }
 
+static PFN_vkd3d_log log_callback;
+
+static void vkd3d_dbg_voutput(const char *fmt, va_list args)
+{
+    if (log_callback)
+        log_callback(fmt, args);
+    else
+        vfprintf(stderr, fmt, args);
+}
+
+static void vkd3d_dbg_output(const char *fmt, ...)
+{
+    va_list args;
+
+    va_start(args, fmt);
+    vkd3d_dbg_voutput(fmt, args);
+    va_end(args);
+}
+
 void vkd3d_dbg_printf(enum vkd3d_dbg_level level, const char *function, const char *fmt, ...)
 {
     va_list args;
@@ -77,12 +96,17 @@ void vkd3d_dbg_printf(enum vkd3d_dbg_level level, const char *function, const ch
 
     assert(level < ARRAY_SIZE(debug_level_names));
 
-    fprintf(stderr, "%s:%s: ", debug_level_names[level], function);
+    vkd3d_dbg_output("%s:%s ", debug_level_names[level], function);
     va_start(args, fmt);
-    vfprintf(stderr, fmt, args);
+    vkd3d_dbg_voutput(fmt, args);
     va_end(args);
 }
 
+void vkd3d_dbg_set_log_callback(PFN_vkd3d_log callback)
+{
+    log_callback = callback;
+}
+
 static char *get_buffer(void)
 {
     static char buffers[VKD3D_DEBUG_BUFFER_COUNT][VKD3D_DEBUG_BUFFER_SIZE];
diff --git a/libs/vkd3d/libs/vkd3d-shader/dxbc.c b/libs/vkd3d/libs/vkd3d-shader/dxbc.c
index 1ab71ca47..17be23065 100644
--- a/libs/vkd3d/libs/vkd3d-shader/dxbc.c
+++ b/libs/vkd3d/libs/vkd3d-shader/dxbc.c
@@ -837,6 +837,7 @@ static const struct vkd3d_sm4_opcode_info opcode_table[] =
     {VKD3D_SM5_OP_DDIV,                             VKD3DSIH_DDIV,                             "d",    "dd"},
     {VKD3D_SM5_OP_DFMA,                             VKD3DSIH_DFMA,                             "d",    "ddd"},
     {VKD3D_SM5_OP_DRCP,                             VKD3DSIH_DRCP,                             "d",    "d"},
+    {VKD3D_SM5_OP_MSAD,                             VKD3DSIH_MSAD,                             "u",    "uuu"},
     {VKD3D_SM5_OP_DTOI,                             VKD3DSIH_DTOI,                             "i",    "d"},
     {VKD3D_SM5_OP_DTOU,                             VKD3DSIH_DTOU,                             "u",    "d"},
     {VKD3D_SM5_OP_ITOD,                             VKD3DSIH_ITOD,                             "d",    "i"},
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl.c b/libs/vkd3d/libs/vkd3d-shader/hlsl.c
index ea5e35d20..7239b1837 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl.c
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl.c
@@ -202,6 +202,13 @@ static void hlsl_type_calculate_reg_size(struct hlsl_ctx *ctx, struct hlsl_type
     }
 }
 
+/* Returns the size of a type, considered as part of an array of that type.
+ * As such it includes padding after the type. */
+unsigned int hlsl_type_get_array_element_reg_size(const struct hlsl_type *type)
+{
+    return align(type->reg_size, 4);
+}
+
 static struct hlsl_type *hlsl_new_type(struct hlsl_ctx *ctx, const char *name, enum hlsl_type_class type_class,
         enum hlsl_base_type base_type, unsigned dimx, unsigned dimy)
 {
@@ -225,6 +232,85 @@ static struct hlsl_type *hlsl_new_type(struct hlsl_ctx *ctx, const char *name, e
     return type;
 }
 
+/* Returns the register offset of a given component within a type, given its index.
+ * *comp_type will be set to the type of the component. */
+unsigned int hlsl_compute_component_offset(struct hlsl_ctx *ctx, struct hlsl_type *type,
+        unsigned int idx, struct hlsl_type **comp_type)
+{
+    switch (type->type)
+    {
+        case HLSL_CLASS_SCALAR:
+        case HLSL_CLASS_VECTOR:
+        {
+            assert(idx < type->dimx * type->dimy);
+            *comp_type = hlsl_get_scalar_type(ctx, type->base_type);
+            return idx;
+        }
+        case HLSL_CLASS_MATRIX:
+        {
+            unsigned int minor, major, x = idx % type->dimx, y = idx / type->dimx;
+
+            assert(idx < type->dimx * type->dimy);
+
+            if (hlsl_type_is_row_major(type))
+            {
+                minor = x;
+                major = y;
+            }
+            else
+            {
+                minor = y;
+                major = x;
+            }
+
+            *comp_type = hlsl_get_scalar_type(ctx, type->base_type);
+            return 4 * major + minor;
+        }
+
+        case HLSL_CLASS_ARRAY:
+        {
+            unsigned int elem_comp_count = hlsl_type_component_count(type->e.array.type);
+            unsigned int array_idx = idx / elem_comp_count;
+            unsigned int idx_in_elem = idx % elem_comp_count;
+
+            assert(array_idx < type->e.array.elements_count);
+
+            return array_idx * hlsl_type_get_array_element_reg_size(type->e.array.type) +
+                    hlsl_compute_component_offset(ctx, type->e.array.type, idx_in_elem, comp_type);
+        }
+
+        case HLSL_CLASS_STRUCT:
+        {
+            struct hlsl_struct_field *field;
+
+            LIST_FOR_EACH_ENTRY(field, type->e.elements, struct hlsl_struct_field, entry)
+            {
+                unsigned int elem_comp_count = hlsl_type_component_count(field->type);
+
+                if (idx < elem_comp_count)
+                {
+                    return field->reg_offset +
+                            hlsl_compute_component_offset(ctx, field->type, idx, comp_type);
+                }
+                idx -= elem_comp_count;
+            }
+
+            assert(0);
+            return 0;
+        }
+
+        case HLSL_CLASS_OBJECT:
+        {
+            assert(idx == 0);
+            *comp_type = type;
+            return 0;
+        }
+    }
+
+    assert(0);
+    return 0;
+}
+
 struct hlsl_type *hlsl_new_array_type(struct hlsl_ctx *ctx, struct hlsl_type *basic_type, unsigned int array_size)
 {
     struct hlsl_type *type;
@@ -556,27 +642,44 @@ struct hlsl_ir_store *hlsl_new_simple_store(struct hlsl_ctx *ctx, struct hlsl_ir
     return hlsl_new_store(ctx, lhs, NULL, rhs, 0, rhs->loc);
 }
 
-struct hlsl_ir_constant *hlsl_new_int_constant(struct hlsl_ctx *ctx, int n,
-        const struct vkd3d_shader_location loc)
+struct hlsl_ir_constant *hlsl_new_constant(struct hlsl_ctx *ctx, struct hlsl_type *type,
+        const struct vkd3d_shader_location *loc)
 {
     struct hlsl_ir_constant *c;
 
+    assert(type->type <= HLSL_CLASS_VECTOR);
+
     if (!(c = hlsl_alloc(ctx, sizeof(*c))))
         return NULL;
-    init_node(&c->node, HLSL_IR_CONSTANT, hlsl_get_scalar_type(ctx, HLSL_TYPE_INT), loc);
-    c->value[0].i = n;
+
+    init_node(&c->node, HLSL_IR_CONSTANT, type, *loc);
+
+    return c;
+}
+
+struct hlsl_ir_constant *hlsl_new_int_constant(struct hlsl_ctx *ctx, int n,
+        const struct vkd3d_shader_location *loc)
+{
+    struct hlsl_ir_constant *c;
+
+    c = hlsl_new_constant(ctx, hlsl_get_scalar_type(ctx, HLSL_TYPE_INT), loc);
+
+    if (c)
+        c->value[0].i = n;
+
     return c;
 }
 
 struct hlsl_ir_constant *hlsl_new_uint_constant(struct hlsl_ctx *ctx, unsigned int n,
-        const struct vkd3d_shader_location loc)
+        const struct vkd3d_shader_location *loc)
 {
     struct hlsl_ir_constant *c;
 
-    if (!(c = hlsl_alloc(ctx, sizeof(*c))))
-        return NULL;
-    init_node(&c->node, HLSL_IR_CONSTANT, hlsl_get_scalar_type(ctx, HLSL_TYPE_UINT), loc);
-    c->value[0].u = n;
+    c = hlsl_new_constant(ctx, hlsl_get_scalar_type(ctx, HLSL_TYPE_UINT), loc);
+
+    if (c)
+        c->value[0].u = n;
+
     return c;
 }
 
@@ -1144,7 +1247,7 @@ static void dump_ir_constant(struct vkd3d_string_buffer *buffer, const struct hl
         switch (type->base_type)
         {
             case HLSL_TYPE_BOOL:
-                vkd3d_string_buffer_printf(buffer, "%s ", value->b ? "true" : "false");
+                vkd3d_string_buffer_printf(buffer, "%s ", value->u ? "true" : "false");
                 break;
 
             case HLSL_TYPE_DOUBLE:
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl.h b/libs/vkd3d/libs/vkd3d-shader/hlsl.h
index 7fd1eb159..762be8419 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl.h
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl.h
@@ -152,6 +152,7 @@ struct hlsl_struct_field
     struct hlsl_type *type;
     const char *name;
     struct hlsl_semantic semantic;
+    unsigned int modifiers;
     unsigned int reg_offset;
 
     size_t name_bytecode_offset;
@@ -415,7 +416,6 @@ struct hlsl_ir_constant
         int32_t i;
         float f;
         double d;
-        bool b;
     } value[4];
     struct hlsl_reg reg;
 };
@@ -733,12 +733,14 @@ struct hlsl_buffer *hlsl_new_buffer(struct hlsl_ctx *ctx, enum hlsl_buffer_type
         const struct hlsl_reg_reservation *reservation, struct vkd3d_shader_location loc);
 struct hlsl_ir_expr *hlsl_new_cast(struct hlsl_ctx *ctx, struct hlsl_ir_node *node, struct hlsl_type *type,
         const struct vkd3d_shader_location *loc);
+struct hlsl_ir_constant *hlsl_new_constant(struct hlsl_ctx *ctx, struct hlsl_type *type,
+        const struct vkd3d_shader_location *loc);
 struct hlsl_ir_expr *hlsl_new_copy(struct hlsl_ctx *ctx, struct hlsl_ir_node *node);
 struct hlsl_ir_function_decl *hlsl_new_func_decl(struct hlsl_ctx *ctx, struct hlsl_type *return_type,
         struct list *parameters, const struct hlsl_semantic *semantic, struct vkd3d_shader_location loc);
 struct hlsl_ir_if *hlsl_new_if(struct hlsl_ctx *ctx, struct hlsl_ir_node *condition, struct vkd3d_shader_location loc);
 struct hlsl_ir_constant *hlsl_new_int_constant(struct hlsl_ctx *ctx, int n,
-        const struct vkd3d_shader_location loc);
+        const struct vkd3d_shader_location *loc);
 struct hlsl_ir_jump *hlsl_new_jump(struct hlsl_ctx *ctx, enum hlsl_ir_jump_type type, struct vkd3d_shader_location loc);
 struct hlsl_ir_load *hlsl_new_load(struct hlsl_ctx *ctx, struct hlsl_ir_var *var, struct hlsl_ir_node *offset,
         struct hlsl_type *type, struct vkd3d_shader_location loc);
@@ -757,7 +759,7 @@ struct hlsl_ir_var *hlsl_new_synthetic_var(struct hlsl_ctx *ctx, const char *nam
         const struct vkd3d_shader_location loc);
 struct hlsl_type *hlsl_new_texture_type(struct hlsl_ctx *ctx, enum hlsl_sampler_dim dim, struct hlsl_type *format);
 struct hlsl_ir_constant *hlsl_new_uint_constant(struct hlsl_ctx *ctx, unsigned int n,
-        const struct vkd3d_shader_location loc);
+        const struct vkd3d_shader_location *loc);
 struct hlsl_ir_node *hlsl_new_unary_expr(struct hlsl_ctx *ctx, enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg,
         struct vkd3d_shader_location loc);
 struct hlsl_ir_var *hlsl_new_var(struct hlsl_ctx *ctx, const char *name, struct hlsl_type *type,
@@ -783,6 +785,9 @@ bool hlsl_scope_add_type(struct hlsl_scope *scope, struct hlsl_type *type);
 struct hlsl_type *hlsl_type_clone(struct hlsl_ctx *ctx, struct hlsl_type *old,
         unsigned int default_majority, unsigned int modifiers);
 unsigned int hlsl_type_component_count(struct hlsl_type *type);
+unsigned int hlsl_type_get_array_element_reg_size(const struct hlsl_type *type);
+unsigned int hlsl_compute_component_offset(struct hlsl_ctx *ctx, struct hlsl_type *type,
+        unsigned int idx, struct hlsl_type **comp_type);
 unsigned int hlsl_type_get_sm4_offset(const struct hlsl_type *type, unsigned int offset);
 bool hlsl_types_are_equal(const struct hlsl_type *t1, const struct hlsl_type *t2);
 
@@ -793,8 +798,7 @@ unsigned int hlsl_swizzle_from_writemask(unsigned int writemask);
 
 bool hlsl_offset_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref, unsigned int *offset);
 unsigned int hlsl_offset_from_deref_safe(struct hlsl_ctx *ctx, const struct hlsl_deref *deref);
-struct hlsl_reg hlsl_reg_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref,
-        const struct hlsl_type *type);
+struct hlsl_reg hlsl_reg_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref);
 
 bool hlsl_fold_constants(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void *context);
 
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl.l b/libs/vkd3d/libs/vkd3d-shader/hlsl.l
index 267c8c304..ff7b712f7 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl.l
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl.l
@@ -197,15 +197,15 @@ row_major               {return KW_ROW_MAJOR;           }
                             return C_FLOAT;
                         }
 0x[0-9a-fA-F]+          {
-                            sscanf(yytext, "0x%x", &yylval->intval);
+                            yylval->intval = vkd3d_parse_integer(yytext);
                             return C_INTEGER;
                         }
 0[0-7]+                 {
-                            sscanf(yytext, "0%o", &yylval->intval);
+                            yylval->intval = vkd3d_parse_integer(yytext);
                             return C_INTEGER;
                         }
 [0-9]+                  {
-                            yylval->intval = (atoi(yytext));
+                            yylval->intval = vkd3d_parse_integer(yytext);
                             return C_INTEGER;
                         }
 
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl.y b/libs/vkd3d/libs/vkd3d-shader/hlsl.y
index 92dcb3f2a..df5fda472 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl.y
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl.y
@@ -48,6 +48,7 @@ struct parse_initializer
     struct hlsl_ir_node **args;
     unsigned int args_count;
     struct list *instrs;
+    bool braces;
 };
 
 struct parse_array_sizes
@@ -265,7 +266,10 @@ static bool implicit_compatible_data_types(struct hlsl_type *t1, struct hlsl_typ
     return false;
 }
 
-static struct hlsl_ir_node *add_implicit_conversion(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_load *add_load(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_node *var_node,
+        struct hlsl_ir_node *offset, struct hlsl_type *data_type, const struct vkd3d_shader_location loc);
+
+static struct hlsl_ir_node *add_cast(struct hlsl_ctx *ctx, struct list *instrs,
         struct hlsl_ir_node *node, struct hlsl_type *dst_type, const struct vkd3d_shader_location *loc)
 {
     struct hlsl_type *src_type = node->data_type;
@@ -274,6 +278,99 @@ static struct hlsl_ir_node *add_implicit_conversion(struct hlsl_ctx *ctx, struct
     if (hlsl_types_are_equal(src_type, dst_type))
         return node;
 
+    if ((src_type->type == HLSL_CLASS_MATRIX || dst_type->type == HLSL_CLASS_MATRIX)
+        && src_type->type <= HLSL_CLASS_LAST_NUMERIC && dst_type->type <= HLSL_CLASS_LAST_NUMERIC)
+    {
+        struct vkd3d_string_buffer *name;
+        static unsigned int counter = 0;
+        struct hlsl_ir_load *load;
+        struct hlsl_ir_var *var;
+        unsigned int dst_idx;
+        bool broadcast;
+
+        broadcast = src_type->dimx == 1 && src_type->dimy == 1;
+        assert(dst_type->dimx * dst_type->dimy <= src_type->dimx * src_type->dimy || broadcast);
+        if (src_type->type == HLSL_CLASS_MATRIX && dst_type->type == HLSL_CLASS_MATRIX && !broadcast)
+        {
+            assert(dst_type->dimx <= src_type->dimx);
+            assert(dst_type->dimy <= src_type->dimy);
+        }
+
+        name = vkd3d_string_buffer_get(&ctx->string_buffers);
+        vkd3d_string_buffer_printf(name, "<cast-%u>", counter++);
+        var = hlsl_new_synthetic_var(ctx, name->buffer, dst_type, *loc);
+        vkd3d_string_buffer_release(&ctx->string_buffers, name);
+        if (!var)
+            return NULL;
+
+        for (dst_idx = 0; dst_idx < dst_type->dimx * dst_type->dimy; ++dst_idx)
+        {
+            struct hlsl_type *src_scalar_type, *dst_scalar_type;
+            unsigned int src_idx, src_offset, dst_offset;
+            struct hlsl_ir_store *store;
+            struct hlsl_ir_constant *c;
+
+            if (broadcast)
+            {
+                src_idx = 0;
+            }
+            else
+            {
+                if (src_type->type == HLSL_CLASS_MATRIX && dst_type->type == HLSL_CLASS_MATRIX)
+                {
+                    unsigned int x = dst_idx % dst_type->dimx, y = dst_idx / dst_type->dimx;
+
+                    src_idx = y * src_type->dimx + x;
+                }
+                else
+                {
+                    src_idx = dst_idx;
+                }
+            }
+
+            dst_offset = hlsl_compute_component_offset(ctx, dst_type, dst_idx, &dst_scalar_type);
+            src_offset = hlsl_compute_component_offset(ctx, src_type, src_idx, &src_scalar_type);
+
+            if (!(c = hlsl_new_uint_constant(ctx, src_offset, loc)))
+                return NULL;
+            list_add_tail(instrs, &c->node.entry);
+
+            if (!(load = add_load(ctx, instrs, node, &c->node, src_scalar_type, *loc)))
+                return NULL;
+
+            if (!(cast = hlsl_new_cast(ctx, &load->node, dst_scalar_type, loc)))
+                return NULL;
+            list_add_tail(instrs, &cast->node.entry);
+
+            if (!(c = hlsl_new_uint_constant(ctx, dst_offset, loc)))
+                return NULL;
+            list_add_tail(instrs, &c->node.entry);
+
+            if (!(store = hlsl_new_store(ctx, var, &c->node, &cast->node, 0, *loc)))
+                return NULL;
+            list_add_tail(instrs, &store->node.entry);
+        }
+
+        if (!(load = hlsl_new_load(ctx, var, NULL, dst_type, *loc)))
+            return NULL;
+        list_add_tail(instrs, &load->node.entry);
+
+        return &load->node;
+    }
+    else
+    {
+        if (!(cast = hlsl_new_cast(ctx, node, dst_type, loc)))
+            return NULL;
+        list_add_tail(instrs, &cast->node.entry);
+        return &cast->node;
+    }
+}
+
+static struct hlsl_ir_node *add_implicit_conversion(struct hlsl_ctx *ctx, struct list *instrs,
+        struct hlsl_ir_node *node, struct hlsl_type *dst_type, const struct vkd3d_shader_location *loc)
+{
+    struct hlsl_type *src_type = node->data_type;
+
     if (!implicit_compatible_data_types(src_type, dst_type))
     {
         struct vkd3d_string_buffer *src_string, *dst_string;
@@ -292,10 +389,7 @@ static struct hlsl_ir_node *add_implicit_conversion(struct hlsl_ctx *ctx, struct
         hlsl_warning(ctx, loc, VKD3D_SHADER_WARNING_HLSL_IMPLICIT_TRUNCATION, "Implicit truncation of %s type.",
                 src_type->type == HLSL_CLASS_VECTOR ? "vector" : "matrix");
 
-    if (!(cast = hlsl_new_cast(ctx, node, dst_type, loc)))
-        return NULL;
-    list_add_tail(instrs, &cast->node.entry);
-    return &cast->node;
+    return add_cast(ctx, instrs, node, dst_type, loc);
 }
 
 static DWORD add_modifiers(struct hlsl_ctx *ctx, DWORD modifiers, DWORD mod, const struct vkd3d_shader_location loc)
@@ -568,35 +662,130 @@ static struct hlsl_ir_load *add_load(struct hlsl_ctx *ctx, struct list *instrs,
     return load;
 }
 
-static struct hlsl_ir_load *add_record_load(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_node *record,
+static bool add_record_load(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_node *record,
         const struct hlsl_struct_field *field, const struct vkd3d_shader_location loc)
 {
     struct hlsl_ir_constant *c;
 
-    if (!(c = hlsl_new_uint_constant(ctx, field->reg_offset, loc)))
-        return NULL;
+    if (!(c = hlsl_new_uint_constant(ctx, field->reg_offset, &loc)))
+        return false;
     list_add_tail(instrs, &c->node.entry);
 
-    return add_load(ctx, instrs, record, &c->node, field->type, loc);
+    return !!add_load(ctx, instrs, record, &c->node, field->type, loc);
+}
+
+static struct hlsl_ir_node *add_binary_arithmetic_expr(struct hlsl_ctx *ctx, struct list *instrs,
+        enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg1, struct hlsl_ir_node *arg2,
+        const struct vkd3d_shader_location *loc);
+
+static struct hlsl_ir_node *add_matrix_scalar_load(struct hlsl_ctx *ctx, struct list *instrs,
+        struct hlsl_ir_node *matrix, struct hlsl_ir_node *x, struct hlsl_ir_node *y,
+        const struct vkd3d_shader_location *loc)
+{
+    struct hlsl_ir_node *major, *minor, *mul, *add;
+    struct hlsl_ir_constant *four;
+    struct hlsl_ir_load *load;
+    struct hlsl_type *type = matrix->data_type, *scalar_type;
+
+    scalar_type = hlsl_get_scalar_type(ctx, type->base_type);
+
+    if (type->modifiers & HLSL_MODIFIER_ROW_MAJOR)
+    {
+        minor = x;
+        major = y;
+    }
+    else
+    {
+        minor = y;
+        major = x;
+    }
+
+    if (!(four = hlsl_new_uint_constant(ctx, 4, loc)))
+        return NULL;
+    list_add_tail(instrs, &four->node.entry);
+
+    if (!(mul = add_binary_arithmetic_expr(ctx, instrs, HLSL_OP2_MUL, &four->node, major, loc)))
+        return NULL;
+
+    if (!(add = add_binary_arithmetic_expr(ctx, instrs, HLSL_OP2_ADD, mul, minor, loc)))
+        return NULL;
+
+    if (!(load = add_load(ctx, instrs, matrix, add, scalar_type, *loc)))
+        return NULL;
+
+    return &load->node;
+}
+
+static bool add_matrix_index(struct hlsl_ctx *ctx, struct list *instrs,
+        struct hlsl_ir_node *matrix, struct hlsl_ir_node *index, const struct vkd3d_shader_location *loc)
+{
+    struct hlsl_type *mat_type = matrix->data_type, *ret_type;
+    struct vkd3d_string_buffer *name;
+    static unsigned int counter = 0;
+    struct hlsl_ir_load *load;
+    struct hlsl_ir_var *var;
+    unsigned int i;
+
+    ret_type = hlsl_get_vector_type(ctx, mat_type->base_type, mat_type->dimx);
+
+    name = vkd3d_string_buffer_get(&ctx->string_buffers);
+    vkd3d_string_buffer_printf(name, "<index-%u>", counter++);
+    var = hlsl_new_synthetic_var(ctx, name->buffer, ret_type, *loc);
+    vkd3d_string_buffer_release(&ctx->string_buffers, name);
+    if (!var)
+        return false;
+
+    for (i = 0; i < mat_type->dimx; ++i)
+    {
+        struct hlsl_ir_store *store;
+        struct hlsl_ir_node *value;
+        struct hlsl_ir_constant *c;
+
+        if (!(c = hlsl_new_uint_constant(ctx, i, loc)))
+            return false;
+        list_add_tail(instrs, &c->node.entry);
+
+        if (!(value = add_matrix_scalar_load(ctx, instrs, matrix, &c->node, index, loc)))
+            return false;
+
+        if (!(store = hlsl_new_store(ctx, var, &c->node, value, 0, *loc)))
+            return false;
+        list_add_tail(instrs, &store->node.entry);
+    }
+
+    if (!(load = hlsl_new_var_load(ctx, var, *loc)))
+        return false;
+    list_add_tail(instrs, &load->node.entry);
+
+    return true;
 }
 
-static struct hlsl_ir_load *add_array_load(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_node *array,
+static bool add_array_load(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_node *array,
         struct hlsl_ir_node *index, const struct vkd3d_shader_location loc)
 {
     const struct hlsl_type *expr_type = array->data_type;
     struct hlsl_type *data_type;
     struct hlsl_ir_constant *c;
-    struct hlsl_ir_node *mul;
 
     if (expr_type->type == HLSL_CLASS_ARRAY)
     {
         data_type = expr_type->e.array.type;
+
+        if (!(c = hlsl_new_uint_constant(ctx, hlsl_type_get_array_element_reg_size(data_type), &loc)))
+            return false;
+        list_add_tail(instrs, &c->node.entry);
+
+        if (!(index = hlsl_new_binary_expr(ctx, HLSL_OP2_MUL, index, &c->node)))
+            return false;
+        list_add_tail(instrs, &index->entry);
     }
-    else if (expr_type->type == HLSL_CLASS_MATRIX || expr_type->type == HLSL_CLASS_VECTOR)
+    else if (expr_type->type == HLSL_CLASS_MATRIX)
     {
-        /* This needs to be lowered now, while we still have type information. */
-        FIXME("Index of matrix or vector type.\n");
-        return NULL;
+        return add_matrix_index(ctx, instrs, array, index, &loc);
+    }
+    else if (expr_type->type == HLSL_CLASS_VECTOR)
+    {
+        data_type = hlsl_get_scalar_type(ctx, expr_type->base_type);
     }
     else
     {
@@ -604,18 +793,10 @@ static struct hlsl_ir_load *add_array_load(struct hlsl_ctx *ctx, struct list *in
             hlsl_error(ctx, &loc, VKD3D_SHADER_ERROR_HLSL_INVALID_INDEX, "Scalar expressions cannot be array-indexed.");
         else
             hlsl_error(ctx, &loc, VKD3D_SHADER_ERROR_HLSL_INVALID_INDEX, "Expression cannot be array-indexed.");
-        return NULL;
+        return false;
     }
 
-    if (!(c = hlsl_new_uint_constant(ctx, data_type->reg_size, loc)))
-        return NULL;
-    list_add_tail(instrs, &c->node.entry);
-    if (!(mul = hlsl_new_binary_expr(ctx, HLSL_OP2_MUL, index, &c->node)))
-        return NULL;
-    list_add_tail(instrs, &mul->entry);
-    index = mul;
-
-    return add_load(ctx, instrs, array, index, data_type, loc);
+    return !!add_load(ctx, instrs, array, index, data_type, loc);
 }
 
 static struct hlsl_struct_field *get_struct_field(struct list *fields, const char *name)
@@ -667,7 +848,17 @@ static struct hlsl_type *apply_type_modifiers(struct hlsl_ctx *ctx, struct hlsl_
     return new_type;
 }
 
-static struct list *gen_struct_fields(struct hlsl_ctx *ctx, struct hlsl_type *type, struct list *fields)
+static void free_parse_variable_def(struct parse_variable_def *v)
+{
+    free_parse_initializer(&v->initializer);
+    vkd3d_free(v->arrays.sizes);
+    vkd3d_free(v->name);
+    vkd3d_free((void *)v->semantic.name);
+    vkd3d_free(v);
+}
+
+static struct list *gen_struct_fields(struct hlsl_ctx *ctx,
+        struct hlsl_type *type, unsigned int modifiers, struct list *fields)
 {
     struct parse_variable_def *v, *v_next;
     struct hlsl_struct_field *field;
@@ -684,16 +875,18 @@ static struct list *gen_struct_fields(struct hlsl_ctx *ctx, struct hlsl_type *ty
 
         if (!(field = hlsl_alloc(ctx, sizeof(*field))))
         {
-            vkd3d_free(v);
-            return list;
+            free_parse_variable_def(v);
+            continue;
         }
 
         field->type = type;
         for (i = 0; i < v->arrays.count; ++i)
             field->type = hlsl_new_array_type(ctx, field->type, v->arrays.sizes[i]);
+        vkd3d_free(v->arrays.sizes);
         field->loc = v->loc;
         field->name = v->name;
         field->semantic = v->semantic;
+        field->modifiers = modifiers;
         if (v->initializer.args_count)
         {
             hlsl_error(ctx, &v->loc, VKD3D_SHADER_ERROR_HLSL_INVALID_SYNTAX, "Illegal initializer on a struct field.");
@@ -718,18 +911,29 @@ static bool add_typedef(struct hlsl_ctx *ctx, DWORD modifiers, struct hlsl_type
         if (!v->arrays.count)
         {
             if (!(type = hlsl_type_clone(ctx, orig_type, 0, modifiers)))
-                return false;
+            {
+                free_parse_variable_def(v);
+                continue;
+            }
         }
         else
         {
             type = orig_type;
         }
 
+        ret = true;
         for (i = 0; i < v->arrays.count; ++i)
         {
             if (!(type = hlsl_new_array_type(ctx, type, v->arrays.sizes[i])))
-                return false;
+            {
+                free_parse_variable_def(v);
+                ret = false;
+                break;
+            }
         }
+        if (!ret)
+            continue;
+        vkd3d_free(v->arrays.sizes);
 
         vkd3d_free((void *)type->name);
         type->name = v->name;
@@ -746,6 +950,7 @@ static bool add_typedef(struct hlsl_ctx *ctx, DWORD modifiers, struct hlsl_type
         if (!ret)
             hlsl_error(ctx, &v->loc, VKD3D_SHADER_ERROR_HLSL_REDEFINED,
                     "Type '%s' is already defined.", v->name);
+        free_parse_initializer(&v->initializer);
         vkd3d_free(v);
     }
     vkd3d_free(list);
@@ -842,7 +1047,7 @@ static unsigned int evaluate_array_dimension(struct hlsl_ir_node *node)
                 case HLSL_TYPE_DOUBLE:
                     return value->d;
                 case HLSL_TYPE_BOOL:
-                    return value->b;
+                    return !!value->u;
                 default:
                     assert(0);
                     return 0;
@@ -996,13 +1201,87 @@ static bool expr_common_shape(struct hlsl_ctx *ctx, struct hlsl_type *t1, struct
     return true;
 }
 
-static struct hlsl_ir_expr *add_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static unsigned int minor_size(const struct hlsl_type *type)
+{
+    if (type->modifiers & HLSL_MODIFIER_ROW_MAJOR)
+        return type->dimx;
+    else
+        return type->dimy;
+}
+
+static unsigned int major_size(const struct hlsl_type *type)
+{
+    if (type->modifiers & HLSL_MODIFIER_ROW_MAJOR)
+        return type->dimy;
+    else
+        return type->dimx;
+}
+
+static struct hlsl_ir_node *add_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *operands[HLSL_MAX_OPERANDS],
         struct hlsl_type *type, const struct vkd3d_shader_location *loc)
 {
     struct hlsl_ir_expr *expr;
     unsigned int i;
 
+    if (type->type == HLSL_CLASS_MATRIX)
+    {
+        struct vkd3d_string_buffer *name;
+        static unsigned int counter = 0;
+        struct hlsl_type *vector_type;
+        struct hlsl_ir_load *load;
+        struct hlsl_ir_var *var;
+
+        vector_type = hlsl_get_vector_type(ctx, type->base_type, minor_size(type));
+
+        name = vkd3d_string_buffer_get(&ctx->string_buffers);
+        vkd3d_string_buffer_printf(name, "<split_op-%u>", counter++);
+        var = hlsl_new_synthetic_var(ctx, name->buffer, type, *loc);
+        vkd3d_string_buffer_release(&ctx->string_buffers, name);
+        if (!var)
+            return NULL;
+
+        for (i = 0; i < major_size(type); i++)
+        {
+            struct hlsl_ir_node *value, *vector_operands[HLSL_MAX_OPERANDS] = { NULL };
+            struct hlsl_ir_store *store;
+            struct hlsl_ir_constant *c;
+            unsigned int j;
+
+            if (!(c = hlsl_new_uint_constant(ctx, 4 * i, loc)))
+                return NULL;
+            list_add_tail(instrs, &c->node.entry);
+
+            for (j = 0; j < HLSL_MAX_OPERANDS; j++)
+            {
+                if (operands[j])
+                {
+                    struct hlsl_type *vector_arg_type;
+                    struct hlsl_ir_load *load;
+
+                    vector_arg_type = hlsl_get_vector_type(ctx, operands[j]->data_type->base_type, minor_size(type));
+
+                    if (!(load = add_load(ctx, instrs, operands[j], &c->node, vector_arg_type, *loc)))
+                        return NULL;
+                    vector_operands[j] = &load->node;
+                }
+            }
+
+            if (!(value = add_expr(ctx, instrs, op, vector_operands, vector_type, loc)))
+                return NULL;
+
+            if (!(store = hlsl_new_store(ctx, var, &c->node, value, 0, *loc)))
+                return NULL;
+            list_add_tail(instrs, &store->node.entry);
+        }
+
+        if (!(load = hlsl_new_load(ctx, var, NULL, type, *loc)))
+            return NULL;
+        list_add_tail(instrs, &load->node.entry);
+
+        return &load->node;
+    }
+
     if (!(expr = hlsl_alloc(ctx, sizeof(*expr))))
         return NULL;
     init_node(&expr->node, HLSL_IR_EXPR, type, *loc);
@@ -1011,7 +1290,7 @@ static struct hlsl_ir_expr *add_expr(struct hlsl_ctx *ctx, struct list *instrs,
         hlsl_src_from_node(&expr->operands[i], operands[i]);
     list_add_tail(instrs, &expr->node.entry);
 
-    return expr;
+    return &expr->node;
 }
 
 static void check_integer_type(struct hlsl_ctx *ctx, const struct hlsl_ir_node *instr)
@@ -1035,7 +1314,7 @@ static void check_integer_type(struct hlsl_ctx *ctx, const struct hlsl_ir_node *
     }
 }
 
-static struct hlsl_ir_expr *add_unary_arithmetic_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_unary_arithmetic_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg, const struct vkd3d_shader_location *loc)
 {
     struct hlsl_ir_node *args[HLSL_MAX_OPERANDS] = {arg};
@@ -1043,7 +1322,7 @@ static struct hlsl_ir_expr *add_unary_arithmetic_expr(struct hlsl_ctx *ctx, stru
     return add_expr(ctx, instrs, op, args, arg->data_type, loc);
 }
 
-static struct hlsl_ir_expr *add_unary_bitwise_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_unary_bitwise_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg, const struct vkd3d_shader_location *loc)
 {
     check_integer_type(ctx, arg);
@@ -1051,7 +1330,7 @@ static struct hlsl_ir_expr *add_unary_bitwise_expr(struct hlsl_ctx *ctx, struct
     return add_unary_arithmetic_expr(ctx, instrs, op, arg, loc);
 }
 
-static struct hlsl_ir_expr *add_unary_logical_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_unary_logical_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg, const struct vkd3d_shader_location *loc)
 {
     struct hlsl_ir_node *args[HLSL_MAX_OPERANDS] = {0};
@@ -1066,7 +1345,7 @@ static struct hlsl_ir_expr *add_unary_logical_expr(struct hlsl_ctx *ctx, struct
     return add_expr(ctx, instrs, op, args, bool_type, loc);
 }
 
-static struct hlsl_ir_expr *add_binary_arithmetic_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_binary_arithmetic_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg1, struct hlsl_ir_node *arg2,
         const struct vkd3d_shader_location *loc)
 {
@@ -1101,7 +1380,7 @@ static struct list *add_binary_arithmetic_expr_merge(struct hlsl_ctx *ctx, struc
     return list1;
 }
 
-static struct hlsl_ir_expr *add_binary_bitwise_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_binary_bitwise_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg1, struct hlsl_ir_node *arg2,
         const struct vkd3d_shader_location *loc)
 {
@@ -1123,7 +1402,7 @@ static struct list *add_binary_bitwise_expr_merge(struct hlsl_ctx *ctx, struct l
     return list1;
 }
 
-static struct hlsl_ir_expr *add_binary_comparison_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_binary_comparison_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg1, struct hlsl_ir_node *arg2,
         struct vkd3d_shader_location *loc)
 {
@@ -1159,7 +1438,7 @@ static struct list *add_binary_comparison_expr_merge(struct hlsl_ctx *ctx, struc
     return list1;
 }
 
-static struct hlsl_ir_expr *add_binary_logical_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_binary_logical_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg1, struct hlsl_ir_node *arg2,
         const struct vkd3d_shader_location *loc)
 {
@@ -1194,7 +1473,7 @@ static struct list *add_binary_logical_expr_merge(struct hlsl_ctx *ctx, struct l
     return list1;
 }
 
-static struct hlsl_ir_expr *add_binary_shift_expr(struct hlsl_ctx *ctx, struct list *instrs,
+static struct hlsl_ir_node *add_binary_shift_expr(struct hlsl_ctx *ctx, struct list *instrs,
         enum hlsl_ir_expr_op op, struct hlsl_ir_node *arg1, struct hlsl_ir_node *arg2,
         const struct vkd3d_shader_location *loc)
 {
@@ -1303,22 +1582,17 @@ static struct hlsl_ir_node *add_assignment(struct hlsl_ctx *ctx, struct list *in
 
     if (assign_op == ASSIGN_OP_SUB)
     {
-        struct hlsl_ir_expr *expr;
-
-        if (!(expr = add_unary_arithmetic_expr(ctx, instrs, HLSL_OP1_NEG, rhs, &rhs->loc)))
+        if (!(rhs = add_unary_arithmetic_expr(ctx, instrs, HLSL_OP1_NEG, rhs, &rhs->loc)))
             return NULL;
-        rhs = &expr->node;
         assign_op = ASSIGN_OP_ADD;
     }
     if (assign_op != ASSIGN_OP_ASSIGN)
     {
         enum hlsl_ir_expr_op op = op_from_assignment(assign_op);
-        struct hlsl_ir_expr *expr;
 
         assert(op);
-        if (!(expr = add_binary_arithmetic_expr(ctx, instrs, op, lhs, rhs, &rhs->loc)))
+        if (!(rhs = add_binary_arithmetic_expr(ctx, instrs, op, lhs, rhs, &rhs->loc)))
             return NULL;
-        rhs = &expr->node;
     }
 
     if (lhs_type->type <= HLSL_CLASS_LAST_NUMERIC)
@@ -1399,7 +1673,7 @@ static bool add_increment(struct hlsl_ctx *ctx, struct list *instrs, bool decrem
         hlsl_error(ctx, &loc, VKD3D_SHADER_ERROR_HLSL_MODIFIES_CONST,
                 "Argument to %s%screment operator is const.", post ? "post" : "pre", decrement ? "de" : "in");
 
-    if (!(one = hlsl_new_int_constant(ctx, 1, loc)))
+    if (!(one = hlsl_new_int_constant(ctx, 1, &loc)))
         return false;
     list_add_tail(instrs, &one->node.entry);
 
@@ -1422,60 +1696,44 @@ static bool add_increment(struct hlsl_ctx *ctx, struct list *instrs, bool decrem
     return true;
 }
 
-static void struct_var_initializer(struct hlsl_ctx *ctx, struct list *list, struct hlsl_ir_var *var,
-        struct parse_initializer *initializer)
+static void initialize_var_components(struct hlsl_ctx *ctx, struct list *instrs,
+        struct hlsl_ir_var *dst, unsigned int *store_index, struct hlsl_ir_node *src)
 {
-    struct hlsl_type *type = var->data_type;
-    struct hlsl_struct_field *field;
-    unsigned int i = 0;
+    unsigned int src_comp_count = hlsl_type_component_count(src->data_type);
+    unsigned int k;
 
-    if (initializer_size(initializer) != hlsl_type_component_count(type))
+    for (k = 0; k < src_comp_count; ++k)
     {
-        hlsl_error(ctx, &var->loc, VKD3D_SHADER_ERROR_HLSL_WRONG_PARAMETER_COUNT,
-                "Expected %u components in initializer, but got %u.",
-                hlsl_type_component_count(type), initializer_size(initializer));
-        free_parse_initializer(initializer);
-        return;
-    }
-
-    list_move_tail(list, initializer->instrs);
-    vkd3d_free(initializer->instrs);
-
-    LIST_FOR_EACH_ENTRY(field, type->e.elements, struct hlsl_struct_field, entry)
-    {
-        struct hlsl_ir_node *node = initializer->args[i];
+        struct hlsl_type *dst_comp_type, *src_comp_type;
+        unsigned int dst_reg_offset, src_reg_offset;
         struct hlsl_ir_store *store;
         struct hlsl_ir_constant *c;
+        struct hlsl_ir_load *load;
+        struct hlsl_ir_node *conv;
 
-        if (i++ >= initializer->args_count)
-            break;
+        dst_reg_offset = hlsl_compute_component_offset(ctx, dst->data_type, *store_index, &dst_comp_type);
+        src_reg_offset = hlsl_compute_component_offset(ctx, src->data_type, k, &src_comp_type);
 
-        if (hlsl_type_component_count(field->type) == hlsl_type_component_count(node->data_type))
-        {
-            if (!(c = hlsl_new_uint_constant(ctx, field->reg_offset, node->loc)))
-                break;
-            list_add_tail(list, &c->node.entry);
+        if (!(c = hlsl_new_uint_constant(ctx, src_reg_offset, &src->loc)))
+            return;
+        list_add_tail(instrs, &c->node.entry);
 
-            if (!(store = hlsl_new_store(ctx, var, &c->node, node, 0, node->loc)))
-                break;
-            list_add_tail(list, &store->node.entry);
-        }
-        else
-        {
-            hlsl_fixme(ctx, &node->loc, "Implicit cast in structure initializer.");
-        }
-    }
+        if (!(load = add_load(ctx, instrs, src, &c->node, src_comp_type, src->loc)))
+            return;
 
-    vkd3d_free(initializer->args);
-}
+        if (!(conv = add_implicit_conversion(ctx, instrs, &load->node, dst_comp_type, &src->loc)))
+            return;
 
-static void free_parse_variable_def(struct parse_variable_def *v)
-{
-    free_parse_initializer(&v->initializer);
-    vkd3d_free(v->arrays.sizes);
-    vkd3d_free(v->name);
-    vkd3d_free((void *)v->semantic.name);
-    vkd3d_free(v);
+        if (!(c = hlsl_new_uint_constant(ctx, dst_reg_offset, &src->loc)))
+            return;
+        list_add_tail(instrs, &c->node.entry);
+
+        if (!(store = hlsl_new_store(ctx, dst, &c->node, conv, 0, src->loc)))
+            return;
+        list_add_tail(instrs, &store->node.entry);
+
+        ++*store_index;
+    }
 }
 
 static struct list *declare_vars(struct hlsl_ctx *ctx, struct hlsl_type *basic_type,
@@ -1509,10 +1767,21 @@ static struct list *declare_vars(struct hlsl_ctx *ctx, struct hlsl_type *basic_t
         type = basic_type;
         for (i = 0; i < v->arrays.count; ++i)
             type = hlsl_new_array_type(ctx, type, v->arrays.sizes[i]);
+        vkd3d_free(v->arrays.sizes);
 
         if (type->type != HLSL_CLASS_MATRIX)
             check_invalid_matrix_modifiers(ctx, modifiers, v->loc);
 
+        if (modifiers & (HLSL_STORAGE_IN | HLSL_STORAGE_OUT))
+        {
+            struct vkd3d_string_buffer *string;
+
+            if ((string = hlsl_modifiers_to_string(ctx, modifiers & (HLSL_STORAGE_IN | HLSL_STORAGE_OUT))))
+                hlsl_error(ctx, &v->loc, VKD3D_SHADER_ERROR_HLSL_INVALID_MODIFIER,
+                        "Modifiers '%s' are not allowed on non-parameter variables.", string->buffer);
+            hlsl_release_string_buffer(ctx, string);
+        }
+
         if (!(var = hlsl_new_var(ctx, v->name, type, v->loc, &v->semantic, modifiers, &v->reg_reservation)))
         {
             free_parse_variable_def(v);
@@ -1568,6 +1837,7 @@ static struct list *declare_vars(struct hlsl_ctx *ctx, struct hlsl_type *basic_t
             hlsl_error(ctx, &v->loc, VKD3D_SHADER_ERROR_HLSL_MISSING_INITIALIZER,
                     "Const variable \"%s\" is missing an initializer.", var->name);
             hlsl_free_var(var);
+            free_parse_initializer(&v->initializer);
             vkd3d_free(v);
             continue;
         }
@@ -1580,75 +1850,49 @@ static struct list *declare_vars(struct hlsl_ctx *ctx, struct hlsl_type *basic_t
                     "Variable \"%s\" was already declared in this scope.", var->name);
             hlsl_note(ctx, &old->loc, VKD3D_SHADER_LOG_ERROR, "\"%s\" was previously declared here.", old->name);
             hlsl_free_var(var);
+            free_parse_initializer(&v->initializer);
             vkd3d_free(v);
             continue;
         }
 
         if (v->initializer.args_count)
         {
-            unsigned int size = initializer_size(&v->initializer);
-            struct hlsl_ir_load *load;
-
-            if (type->type <= HLSL_CLASS_LAST_NUMERIC
-                    && type->dimx * type->dimy != size && size != 1)
+            if (v->initializer.braces)
             {
-                if (size < type->dimx * type->dimy)
+                unsigned int size = initializer_size(&v->initializer);
+                unsigned int store_index = 0;
+                unsigned int k;
+
+                if (hlsl_type_component_count(type) != size)
                 {
                     hlsl_error(ctx, &v->loc, VKD3D_SHADER_ERROR_HLSL_WRONG_PARAMETER_COUNT,
-                            "Expected %u components in numeric initializer, but got %u.",
-                            type->dimx * type->dimy, size);
+                            "Expected %u components in initializer, but got %u.",
+                            hlsl_type_component_count(type), size);
                     free_parse_initializer(&v->initializer);
                     vkd3d_free(v);
                     continue;
                 }
-            }
-            if ((type->type == HLSL_CLASS_STRUCT || type->type == HLSL_CLASS_ARRAY)
-                    && hlsl_type_component_count(type) != size)
-            {
-                hlsl_error(ctx, &v->loc, VKD3D_SHADER_ERROR_HLSL_WRONG_PARAMETER_COUNT,
-                        "Expected %u components in initializer, but got %u.", hlsl_type_component_count(type), size);
-                free_parse_initializer(&v->initializer);
-                vkd3d_free(v);
-                continue;
-            }
 
-            if (type->type == HLSL_CLASS_STRUCT)
-            {
-                struct_var_initializer(ctx, statements_list, var, &v->initializer);
-                vkd3d_free(v);
-                continue;
-            }
-            if (type->type > HLSL_CLASS_LAST_NUMERIC)
-            {
-                FIXME("Initializers for non scalar/struct variables not supported yet.\n");
-                free_parse_initializer(&v->initializer);
-                vkd3d_free(v);
-                continue;
-            }
-            if (v->arrays.count)
-            {
-                hlsl_fixme(ctx, &v->loc, "Array initializer.");
-                free_parse_initializer(&v->initializer);
-                vkd3d_free(v);
-                continue;
+                for (k = 0; k < v->initializer.args_count; ++k)
+                {
+                    initialize_var_components(ctx, v->initializer.instrs, var,
+                            &store_index, v->initializer.args[k]);
+                }
             }
-            if (v->initializer.args_count > 1)
+            else
             {
-                hlsl_fixme(ctx, &v->loc, "Complex initializer.");
-                free_parse_initializer(&v->initializer);
-                vkd3d_free(v);
-                continue;
-            }
+                struct hlsl_ir_load *load = hlsl_new_var_load(ctx, var, var->loc);
 
-            load = hlsl_new_var_load(ctx, var, var->loc);
-            list_add_tail(v->initializer.instrs, &load->node.entry);
-            add_assignment(ctx, v->initializer.instrs, &load->node, ASSIGN_OP_ASSIGN, v->initializer.args[0]);
-            vkd3d_free(v->initializer.args);
+                assert(v->initializer.args_count == 1);
+                list_add_tail(v->initializer.instrs, &load->node.entry);
+                add_assignment(ctx, v->initializer.instrs, &load->node, ASSIGN_OP_ASSIGN, v->initializer.args[0]);
+            }
 
             if (modifiers & HLSL_STORAGE_STATIC)
                 list_move_tail(&ctx->static_initializers, v->initializer.instrs);
             else
                 list_move_tail(statements_list, v->initializer.instrs);
+            vkd3d_free(v->initializer.args);
             vkd3d_free(v->initializer.instrs);
         }
         vkd3d_free(v);
@@ -1718,12 +1962,12 @@ static bool intrinsic_abs(struct hlsl_ctx *ctx,
 static bool intrinsic_clamp(struct hlsl_ctx *ctx,
         const struct parse_initializer *params, const struct vkd3d_shader_location *loc)
 {
-    struct hlsl_ir_expr *max;
+    struct hlsl_ir_node *max;
 
     if (!(max = add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MAX, params->args[0], params->args[1], loc)))
         return false;
 
-    return !!add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MIN, &max->node, params->args[2], loc);
+    return !!add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MIN, max, params->args[2], loc);
 }
 
 static bool intrinsic_cross(struct hlsl_ctx *ctx,
@@ -1731,8 +1975,7 @@ static bool intrinsic_cross(struct hlsl_ctx *ctx,
 {
     struct hlsl_ir_swizzle *arg1_swzl1, *arg1_swzl2, *arg2_swzl1, *arg2_swzl2;
     struct hlsl_ir_node *arg1 = params->args[0], *arg2 = params->args[1];
-    struct hlsl_ir_node *arg1_cast, *arg2_cast, *mul1_neg;
-    struct hlsl_ir_expr *mul1, *mul2;
+    struct hlsl_ir_node *arg1_cast, *arg2_cast, *mul1_neg, *mul1, *mul2;
     struct hlsl_type *cast_type;
     enum hlsl_base_type base;
 
@@ -1761,7 +2004,7 @@ static bool intrinsic_cross(struct hlsl_ctx *ctx,
             &arg1_swzl1->node, &arg2_swzl1->node, loc)))
         return false;
 
-    if (!(mul1_neg = hlsl_new_unary_expr(ctx, HLSL_OP1_NEG, &mul1->node, *loc)))
+    if (!(mul1_neg = hlsl_new_unary_expr(ctx, HLSL_OP1_NEG, mul1, *loc)))
         return false;
     list_add_tail(params->instrs, &mul1_neg->entry);
 
@@ -1777,7 +2020,7 @@ static bool intrinsic_cross(struct hlsl_ctx *ctx,
             &arg1_swzl2->node, &arg2_swzl2->node, loc)))
         return false;
 
-    return !!add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_ADD, &mul2->node, mul1_neg, loc);
+    return !!add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_ADD, mul2, mul1_neg, loc);
 }
 
 static bool intrinsic_floor(struct hlsl_ctx *ctx,
@@ -1803,20 +2046,139 @@ static bool intrinsic_min(struct hlsl_ctx *ctx,
     return !!add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MIN, params->args[0], params->args[1], loc);
 }
 
+static bool intrinsic_mul(struct hlsl_ctx *ctx,
+        const struct parse_initializer *params, const struct vkd3d_shader_location *loc)
+{
+    struct hlsl_ir_node *arg1 = params->args[0], *arg2 = params->args[1], *cast1, *cast2;
+    enum hlsl_base_type base = expr_common_base_type(arg1->data_type->base_type, arg2->data_type->base_type);
+    struct hlsl_type *cast_type1 = arg1->data_type, *cast_type2 = arg2->data_type, *matrix_type, *ret_type;
+    unsigned int i, j, k, vect_count = 0;
+    struct vkd3d_string_buffer *name;
+    static unsigned int counter = 0;
+    struct hlsl_ir_load *load;
+    struct hlsl_ir_var *var;
+
+    if (arg1->data_type->type == HLSL_CLASS_SCALAR || arg2->data_type->type == HLSL_CLASS_SCALAR)
+        return !!add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MUL, arg1, arg2, loc);
+
+    if (arg1->data_type->type == HLSL_CLASS_VECTOR)
+    {
+        vect_count++;
+        cast_type1 = hlsl_get_matrix_type(ctx, base, arg1->data_type->dimx, 1);
+    }
+    if (arg2->data_type->type == HLSL_CLASS_VECTOR)
+    {
+        vect_count++;
+        cast_type2 = hlsl_get_matrix_type(ctx, base, 1, arg2->data_type->dimx);
+    }
+
+    matrix_type = hlsl_get_matrix_type(ctx, base, cast_type2->dimx, cast_type1->dimy);
+
+    if (vect_count == 0)
+    {
+        ret_type = matrix_type;
+    }
+    else if (vect_count == 1)
+    {
+        assert(matrix_type->dimx == 1 || matrix_type->dimy == 1);
+        ret_type = hlsl_get_vector_type(ctx, base, matrix_type->dimx * matrix_type->dimy);
+    }
+    else
+    {
+        assert(matrix_type->dimx == 1 && matrix_type->dimy == 1);
+        ret_type = hlsl_get_scalar_type(ctx, base);
+    }
+
+    if (!(cast1 = add_implicit_conversion(ctx, params->instrs, arg1, cast_type1, loc)))
+        return false;
+
+    if (!(cast2 = add_implicit_conversion(ctx, params->instrs, arg2, cast_type2, loc)))
+        return false;
+
+    name = vkd3d_string_buffer_get(&ctx->string_buffers);
+    vkd3d_string_buffer_printf(name, "<mul-%u>", counter++);
+    var = hlsl_new_synthetic_var(ctx, name->buffer, matrix_type, *loc);
+    vkd3d_string_buffer_release(&ctx->string_buffers, name);
+    if (!var)
+        return false;
+
+    for (i = 0; i < matrix_type->dimx; ++i)
+        for (j = 0; j < matrix_type->dimy; ++j)
+        {
+            struct hlsl_ir_node *node = NULL;
+            struct hlsl_type *scalar_type;
+            struct hlsl_ir_store *store;
+            struct hlsl_ir_constant *c;
+            unsigned int offset;
+
+            for (k = 0; k < cast_type1->dimx && k < cast_type2->dimy; ++k)
+            {
+                struct hlsl_ir_load *value1, *value2;
+                struct hlsl_ir_node *mul;
+
+                offset = hlsl_compute_component_offset(ctx, cast_type1, j * cast_type1->dimx + k, &scalar_type);
+                if (!(c = hlsl_new_uint_constant(ctx, offset, loc)))
+                    return false;
+                list_add_tail(params->instrs, &c->node.entry);
+
+                if (!(value1 = add_load(ctx, params->instrs, cast1, &c->node, scalar_type, *loc)))
+                    return false;
+
+                offset = hlsl_compute_component_offset(ctx, cast_type2, k * cast_type2->dimx + i, &scalar_type);
+                if (!(c = hlsl_new_uint_constant(ctx, offset, loc)))
+                    return false;
+                list_add_tail(params->instrs, &c->node.entry);
+
+                if (!(value2 = add_load(ctx, params->instrs, cast2, &c->node, scalar_type, *loc)))
+                    return false;
+
+                if (!(mul = add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MUL, &value1->node, &value2->node, loc)))
+                    return false;
+
+                if (node)
+                {
+                    if (!(node = add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_ADD, node, mul, loc)))
+                        return false;
+                }
+                else
+                {
+                    node = mul;
+                }
+            }
+
+            offset = hlsl_compute_component_offset(ctx, matrix_type, j * matrix_type->dimx + i, &scalar_type);
+            if (!(c = hlsl_new_uint_constant(ctx, offset, loc)))
+                return false;
+            list_add_tail(params->instrs, &c->node.entry);
+
+            if (!(store = hlsl_new_store(ctx, var, &c->node, node, 0, *loc)))
+                return false;
+            list_add_tail(params->instrs, &store->node.entry);
+        }
+
+    if (!(load = hlsl_new_load(ctx, var, NULL, matrix_type, *loc)))
+        return false;
+    list_add_tail(params->instrs, &load->node.entry);
+
+    return !!add_implicit_conversion(ctx, params->instrs, &load->node, ret_type, loc);
+}
+
 static bool intrinsic_pow(struct hlsl_ctx *ctx,
         const struct parse_initializer *params, const struct vkd3d_shader_location *loc)
 {
-    struct hlsl_ir_node *log, *exp;
-    struct hlsl_ir_expr *mul;
+    struct hlsl_ir_node *log, *exp, *arg, *mul;
 
-    if (!(log = hlsl_new_unary_expr(ctx, HLSL_OP1_LOG2, params->args[0], *loc)))
+    if (!(arg = intrinsic_float_convert_arg(ctx, params, params->args[0], loc)))
+        return false;
+
+    if (!(log = hlsl_new_unary_expr(ctx, HLSL_OP1_LOG2, arg, *loc)))
         return false;
     list_add_tail(params->instrs, &log->entry);
 
     if (!(mul = add_binary_arithmetic_expr(ctx, params->instrs, HLSL_OP2_MUL, params->args[1], log, loc)))
         return false;
 
-    if (!(exp = hlsl_new_unary_expr(ctx, HLSL_OP1_EXP2, &mul->node, *loc)))
+    if (!(exp = hlsl_new_unary_expr(ctx, HLSL_OP1_EXP2, mul, *loc)))
         return false;
     list_add_tail(params->instrs, &exp->entry);
     return true;
@@ -1861,6 +2223,7 @@ intrinsic_functions[] =
     {"floor",                               1, true,  intrinsic_floor},
     {"max",                                 2, true,  intrinsic_max},
     {"min",                                 2, true,  intrinsic_min},
+    {"mul",                                 2, true,  intrinsic_mul},
     {"pow",                                 2, true,  intrinsic_pow},
     {"round",                               1, true,  intrinsic_round},
     {"saturate",                            1, true,  intrinsic_saturate},
@@ -1891,7 +2254,7 @@ static struct list *add_call(struct hlsl_ctx *ctx, const char *name,
         if (intrinsic->param_count >= 0 && params->args_count != intrinsic->param_count)
         {
             hlsl_error(ctx, &loc, VKD3D_SHADER_ERROR_HLSL_WRONG_PARAMETER_COUNT,
-                    "Wrong number of arguments to function '%s': expected %u, but got %u.\n",
+                    "Wrong number of arguments to function '%s': expected %u, but got %u.",
                     name, intrinsic->param_count, params->args_count);
             free_parse_initializer(params);
             return NULL;
@@ -1909,7 +2272,7 @@ static struct list *add_call(struct hlsl_ctx *ctx, const char *name,
 
                     if ((string = hlsl_type_to_string(ctx, params->args[i]->data_type)))
                         hlsl_error(ctx, &loc, VKD3D_SHADER_ERROR_HLSL_INVALID_TYPE,
-                                "Wrong type for argument %u of '%s': expected a numeric type, but got '%s'.\n",
+                                "Wrong type for argument %u of '%s': expected a numeric type, but got '%s'.",
                                 i + 1, name, string->buffer);
                     hlsl_release_string_buffer(ctx, string);
                     free_parse_initializer(params);
@@ -1937,16 +2300,12 @@ static struct list *add_call(struct hlsl_ctx *ctx, const char *name,
 static struct list *add_constructor(struct hlsl_ctx *ctx, struct hlsl_type *type,
         struct parse_initializer *params, struct vkd3d_shader_location loc)
 {
-    unsigned int i, writemask_offset = 0;
-    struct hlsl_ir_store *store;
     static unsigned int counter;
     struct hlsl_ir_load *load;
     struct hlsl_ir_var *var;
+    unsigned int i, idx = 0;
     char name[23];
 
-    if (type->type == HLSL_CLASS_MATRIX)
-        hlsl_fixme(ctx, &loc, "Matrix constructor.");
-
     sprintf(name, "<constructor-%x>", counter++);
     if (!(var = hlsl_new_synthetic_var(ctx, name, type, loc)))
         return NULL;
@@ -1954,7 +2313,6 @@ static struct list *add_constructor(struct hlsl_ctx *ctx, struct hlsl_type *type
     for (i = 0; i < params->args_count; ++i)
     {
         struct hlsl_ir_node *arg = params->args[i];
-        unsigned int width;
 
         if (arg->data_type->type == HLSL_CLASS_OBJECT)
         {
@@ -1966,24 +2324,8 @@ static struct list *add_constructor(struct hlsl_ctx *ctx, struct hlsl_type *type
             hlsl_release_string_buffer(ctx, string);
             continue;
         }
-        width = hlsl_type_component_count(arg->data_type);
-
-        if (width > 4)
-        {
-            FIXME("Constructor argument with %u components.\n", width);
-            continue;
-        }
-
-        if (!(arg = add_implicit_conversion(ctx, params->instrs, arg,
-                hlsl_get_vector_type(ctx, type->base_type, width), &arg->loc)))
-            continue;
-
-        if (!(store = hlsl_new_store(ctx, var, NULL, arg,
-                ((1u << width) - 1) << writemask_offset, arg->loc)))
-            return NULL;
-        list_add_tail(params->instrs, &store->node.entry);
 
-        writemask_offset += width;
+        initialize_var_components(ctx, params->instrs, var, &idx, arg);
     }
 
     if (!(load = hlsl_new_var_load(ctx, var, loc)))
@@ -2411,13 +2753,12 @@ static bool add_method_call(struct hlsl_ctx *ctx, struct list *instrs, struct hl
 %type <function> func_prototype
 
 %type <initializer> complex_initializer
+%type <initializer> complex_initializer_list
 %type <initializer> func_arguments
 %type <initializer> initializer_expr_list
 
 %type <if_body> if_body
 
-%type <modifiers> input_mod
-%type <modifiers> input_mods
 %type <modifiers> var_modifiers
 
 %type <name> any_identifier
@@ -2631,7 +2972,7 @@ field:
 
             if (!(type = apply_type_modifiers(ctx, $2, &modifiers, @1)))
                 YYABORT;
-            if (modifiers)
+            if (modifiers & ~HLSL_STORAGE_NOINTERPOLATION)
             {
                 struct vkd3d_string_buffer *string;
 
@@ -2640,7 +2981,7 @@ field:
                             "Modifiers '%s' are not allowed on struct fields.", string->buffer);
                 hlsl_release_string_buffer(ctx, string);
             }
-            $$ = gen_struct_fields(ctx, type, $3);
+            $$ = gen_struct_fields(ctx, type, modifiers, $3);
         }
 
 func_declaration:
@@ -2662,14 +3003,18 @@ func_prototype:
     /* var_modifiers is necessary to avoid shift/reduce conflicts. */
       var_modifiers type var_identifier '(' parameters ')' colon_attribute
         {
+            unsigned int modifiers = $1;
             struct hlsl_ir_var *var;
+            struct hlsl_type *type;
 
-            if ($1)
+            if (modifiers & ~HLSL_MODIFIERS_MAJORITY_MASK)
             {
                 hlsl_error(ctx, &@1, VKD3D_SHADER_ERROR_HLSL_INVALID_MODIFIER,
-                        "Modifiers are not allowed on functions.");
+                        "Only majority modifiers are allowed on functions.");
                 YYABORT;
             }
+            if (!(type = apply_type_modifiers(ctx, $2, &modifiers, @1)))
+                YYABORT;
             if ((var = hlsl_get_var(ctx->globals, $3)))
             {
                 hlsl_error(ctx, &@3, VKD3D_SHADER_ERROR_HLSL_REDEFINED,
@@ -2678,7 +3023,7 @@ func_prototype:
                         "\"%s\" was previously declared here.", $3);
                 YYABORT;
             }
-            if (hlsl_types_are_equal($2, ctx->builtin_types.Void) && $7.semantic.name)
+            if (hlsl_types_are_equal(type, ctx->builtin_types.Void) && $7.semantic.name)
             {
                 hlsl_error(ctx, &@7, VKD3D_SHADER_ERROR_HLSL_INVALID_SEMANTIC,
                         "Semantics are not allowed on void functions.");
@@ -2687,7 +3032,7 @@ func_prototype:
             if ($7.reg_reservation.type)
                 FIXME("Unexpected register reservation for a function.\n");
 
-            if (!($$.decl = hlsl_new_func_decl(ctx, $2, $5, &$7.semantic, @3)))
+            if (!($$.decl = hlsl_new_func_decl(ctx, type, $5, &$7.semantic, @3)))
                 YYABORT;
             $$.name = $3;
             ctx->cur_function = $$.decl;
@@ -2794,54 +3139,21 @@ param_list:
         }
 
 parameter:
-      input_mods var_modifiers type any_identifier colon_attribute
+      var_modifiers type any_identifier colon_attribute
         {
             struct hlsl_type *type;
-            unsigned int modifiers = $2;
+            unsigned int modifiers = $1;
 
-            if (!(type = apply_type_modifiers(ctx, $3, &modifiers, @2)))
+            if (!(type = apply_type_modifiers(ctx, $2, &modifiers, @1)))
                 YYABORT;
 
-            $$.modifiers = $1 ? $1 : HLSL_STORAGE_IN;
-            $$.modifiers |= modifiers;
+            $$.modifiers = modifiers;
+            if (!($$.modifiers & (HLSL_STORAGE_IN | HLSL_STORAGE_OUT)))
+                $$.modifiers |= HLSL_STORAGE_IN;
             $$.type = type;
-            $$.name = $4;
-            $$.semantic = $5.semantic;
-            $$.reg_reservation = $5.reg_reservation;
-        }
-
-input_mods:
-      %empty
-        {
-            $$ = 0;
-        }
-    | input_mods input_mod
-        {
-            if ($1 & $2)
-            {
-                struct vkd3d_string_buffer *string;
-
-                if ((string = hlsl_modifiers_to_string(ctx, $2)))
-                    hlsl_error(ctx, &@2, VKD3D_SHADER_ERROR_HLSL_INVALID_MODIFIER,
-                            "Modifier \"%s\" was already specified.", string->buffer);
-                hlsl_release_string_buffer(ctx, string);
-                YYABORT;
-            }
-            $$ = $1 | $2;
-        }
-
-input_mod:
-      KW_IN
-        {
-            $$ = HLSL_STORAGE_IN;
-        }
-    | KW_OUT
-        {
-            $$ = HLSL_STORAGE_OUT;
-        }
-    | KW_INOUT
-        {
-            $$ = HLSL_STORAGE_IN | HLSL_STORAGE_OUT;
+            $$.name = $3;
+            $$.semantic = $4.semantic;
+            $$.reg_reservation = $4.reg_reservation;
         }
 
 texture_type:
@@ -2974,7 +3286,7 @@ type:
                 string = hlsl_type_to_string(ctx, $3);
                 if (string)
                     hlsl_error(ctx, &@3, VKD3D_SHADER_ERROR_HLSL_INVALID_TYPE,
-                            "Texture data type %s is not scalar or vector.\n", string->buffer);
+                            "Texture data type %s is not scalar or vector.", string->buffer);
                 hlsl_release_string_buffer(ctx, string);
             }
             $$ = hlsl_new_texture_type(ctx, $1, $3);
@@ -3205,23 +3517,63 @@ var_modifiers:
         {
             $$ = add_modifiers(ctx, $2, HLSL_MODIFIER_COLUMN_MAJOR, @1);
         }
+    | KW_IN var_modifiers
+        {
+            $$ = add_modifiers(ctx, $2, HLSL_STORAGE_IN, @1);
+        }
+    | KW_OUT var_modifiers
+        {
+            $$ = add_modifiers(ctx, $2, HLSL_STORAGE_OUT, @1);
+        }
+    | KW_INOUT var_modifiers
+        {
+            $$ = add_modifiers(ctx, $2, HLSL_STORAGE_IN | HLSL_STORAGE_OUT, @1);
+        }
+
 
 complex_initializer:
       initializer_expr
         {
             $$.args_count = 1;
             if (!($$.args = hlsl_alloc(ctx, sizeof(*$$.args))))
+            {
+                destroy_instr_list($1);
                 YYABORT;
+            }
             $$.args[0] = node_from_list($1);
             $$.instrs = $1;
+            $$.braces = false;
         }
-    | '{' initializer_expr_list '}'
+    | '{' complex_initializer_list '}'
         {
             $$ = $2;
+            $$.braces = true;
         }
-    | '{' initializer_expr_list ',' '}'
+    | '{' complex_initializer_list ',' '}'
         {
             $$ = $2;
+            $$.braces = true;
+        }
+
+complex_initializer_list:
+      complex_initializer
+    | complex_initializer_list ',' complex_initializer
+        {
+            struct hlsl_ir_node **new_args;
+            unsigned int i;
+
+            $$ = $1;
+            if (!(new_args = hlsl_realloc(ctx, $$.args, ($$.args_count + $3.args_count) * sizeof(*$$.args))))
+            {
+                free_parse_initializer(&$$);
+                free_parse_initializer(&$3);
+                YYABORT;
+            }
+            $$.args = new_args;
+            for (i = 0; i < $3.args_count; ++i)
+                $$.args[$$.args_count++] = $3.args[i];
+            list_move_tail($$.instrs, $3.instrs);
+            free_parse_initializer(&$3);
         }
 
 initializer_expr:
@@ -3232,15 +3584,26 @@ initializer_expr_list:
         {
             $$.args_count = 1;
             if (!($$.args = hlsl_alloc(ctx, sizeof(*$$.args))))
+            {
+                destroy_instr_list($1);
                 YYABORT;
+            }
             $$.args[0] = node_from_list($1);
             $$.instrs = $1;
+            $$.braces = false;
         }
     | initializer_expr_list ',' initializer_expr
         {
+            struct hlsl_ir_node **new_args;
+
             $$ = $1;
-            if (!($$.args = hlsl_realloc(ctx, $$.args, ($$.args_count + 1) * sizeof(*$$.args))))
+            if (!(new_args = hlsl_realloc(ctx, $$.args, ($$.args_count + 1) * sizeof(*$$.args))))
+            {
+                free_parse_initializer(&$$);
+                destroy_instr_list($3);
                 YYABORT;
+            }
+            $$.args = new_args;
             $$.args[$$.args_count++] = node_from_list($3);
             list_move_tail($$.instrs, $3);
             vkd3d_free($3);
@@ -3364,6 +3727,7 @@ func_arguments:
             $$.args_count = 0;
             if (!($$.instrs = make_empty_list(ctx)))
                 YYABORT;
+            $$.braces = false;
         }
     | initializer_expr_list
 
@@ -3397,7 +3761,7 @@ primary_expr:
             if (!(c = hlsl_alloc(ctx, sizeof(*c))))
                 YYABORT;
             init_node(&c->node, HLSL_IR_CONSTANT, hlsl_get_scalar_type(ctx, HLSL_TYPE_BOOL), @1);
-            c->value[0].b = $1;
+            c->value[0].u = $1 ? ~0u : 0;
             if (!($$ = make_list(ctx, &c->node)))
                 YYABORT;
         }
@@ -3442,7 +3806,7 @@ primary_expr:
             }
             else
             {
-                hlsl_error(ctx, &@1, VKD3D_SHADER_ERROR_HLSL_NOT_DEFINED, "Identifier \"%s\" is not declared.\n", $1);
+                hlsl_error(ctx, &@1, VKD3D_SHADER_ERROR_HLSL_NOT_DEFINED, "Identifier \"%s\" is not declared.", $1);
                 YYABORT;
             }
         }
@@ -3631,7 +3995,6 @@ unary_expr:
         {
             struct hlsl_type *src_type = node_from_list($6)->data_type;
             struct hlsl_type *dst_type;
-            struct hlsl_ir_expr *cast;
             unsigned int i;
 
             if ($2)
@@ -3659,12 +4022,11 @@ unary_expr:
                 YYABORT;
             }
 
-            if (!(cast = hlsl_new_cast(ctx, node_from_list($6), dst_type, &@3)))
+            if (!add_cast(ctx, $6, node_from_list($6), dst_type, &@3))
             {
                 hlsl_free_instr_list($6);
                 YYABORT;
             }
-            list_add_tail($6, &cast->node.entry);
             $$ = $6;
         }
 
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl_codegen.c b/libs/vkd3d/libs/vkd3d-shader/hlsl_codegen.c
index 62dbce5e4..1a5e9055a 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl_codegen.c
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl_codegen.c
@@ -21,6 +21,22 @@
 #include "hlsl.h"
 #include <stdio.h>
 
+static unsigned int minor_size(const struct hlsl_type *type)
+{
+    if (type->modifiers & HLSL_MODIFIER_ROW_MAJOR)
+        return type->dimx;
+    else
+        return type->dimy;
+}
+
+static unsigned int major_size(const struct hlsl_type *type)
+{
+    if (type->modifiers & HLSL_MODIFIER_ROW_MAJOR)
+        return type->dimy;
+    else
+        return type->dimx;
+}
+
 /* Split uniforms into two variables representing the constant and temp
  * registers, and copy the former to the latter, so that writes to uniforms
  * work. */
@@ -34,7 +50,8 @@ static void prepend_uniform_copy(struct hlsl_ctx *ctx, struct list *instrs, stru
     /* Use the synthetic name for the temp, rather than the uniform, so that we
      * can write the uniform name into the shader reflection data. */
 
-    if (!(uniform = hlsl_new_var(ctx, temp->name, temp->data_type, temp->loc, NULL, 0, &temp->reg_reservation)))
+    if (!(uniform = hlsl_new_var(ctx, temp->name, temp->data_type,
+            temp->loc, NULL, temp->modifiers, &temp->reg_reservation)))
         return;
     list_add_before(&temp->scope_entry, &uniform->scope_entry);
     list_add_tail(&ctx->extern_vars, &uniform->extern_entry);
@@ -58,7 +75,7 @@ static void prepend_uniform_copy(struct hlsl_ctx *ctx, struct list *instrs, stru
 }
 
 static void prepend_input_copy(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_var *var,
-        struct hlsl_type *type, unsigned int field_offset, const struct hlsl_semantic *semantic)
+        struct hlsl_type *type, unsigned int field_offset, unsigned int modifiers, const struct hlsl_semantic *semantic)
 {
     struct vkd3d_string_buffer *name;
     struct hlsl_semantic new_semantic;
@@ -67,6 +84,21 @@ static void prepend_input_copy(struct hlsl_ctx *ctx, struct list *instrs, struct
     struct hlsl_ir_load *load;
     struct hlsl_ir_var *input;
 
+    if (type->type == HLSL_CLASS_MATRIX)
+    {
+        struct hlsl_type *vector_type = hlsl_get_vector_type(ctx, type->base_type, minor_size(type));
+        struct hlsl_semantic vector_semantic = *semantic;
+        unsigned int i;
+
+        for (i = 0; i < major_size(type); ++i)
+        {
+            prepend_input_copy(ctx, instrs, var, vector_type, 4 * i, modifiers, &vector_semantic);
+            ++vector_semantic.index;
+        }
+
+        return;
+    }
+
     if (!(name = hlsl_get_string_buffer(ctx)))
         return;
     vkd3d_string_buffer_printf(name, "<input-%s%u>", semantic->name, semantic->index);
@@ -76,7 +108,8 @@ static void prepend_input_copy(struct hlsl_ctx *ctx, struct list *instrs, struct
         return;
     }
     new_semantic.index = semantic->index;
-    if (!(input = hlsl_new_var(ctx, hlsl_strdup(ctx, name->buffer), type, var->loc, &new_semantic, 0, NULL)))
+    if (!(input = hlsl_new_var(ctx, hlsl_strdup(ctx, name->buffer),
+            type, var->loc, &new_semantic, modifiers, NULL)))
     {
         hlsl_release_string_buffer(ctx, name);
         vkd3d_free((void *)new_semantic.name);
@@ -92,7 +125,7 @@ static void prepend_input_copy(struct hlsl_ctx *ctx, struct list *instrs, struct
         return;
     list_add_head(instrs, &load->node.entry);
 
-    if (!(offset = hlsl_new_uint_constant(ctx, field_offset, var->loc)))
+    if (!(offset = hlsl_new_uint_constant(ctx, field_offset, &var->loc)))
         return;
     list_add_after(&load->node.entry, &offset->node.entry);
 
@@ -111,7 +144,8 @@ static void prepend_input_struct_copy(struct hlsl_ctx *ctx, struct list *instrs,
         if (field->type->type == HLSL_CLASS_STRUCT)
             prepend_input_struct_copy(ctx, instrs, var, field->type, field_offset + field->reg_offset);
         else if (field->semantic.name)
-            prepend_input_copy(ctx, instrs, var, field->type, field_offset + field->reg_offset, &field->semantic);
+            prepend_input_copy(ctx, instrs, var, field->type,
+                    field_offset + field->reg_offset, field->modifiers, &field->semantic);
         else
             hlsl_error(ctx, &field->loc, VKD3D_SHADER_ERROR_HLSL_MISSING_SEMANTIC,
                     "Field '%s' is missing a semantic.", field->name);
@@ -125,11 +159,11 @@ static void prepend_input_var_copy(struct hlsl_ctx *ctx, struct list *instrs, st
     if (var->data_type->type == HLSL_CLASS_STRUCT)
         prepend_input_struct_copy(ctx, instrs, var, var->data_type, 0);
     else if (var->semantic.name)
-        prepend_input_copy(ctx, instrs, var, var->data_type, 0, &var->semantic);
+        prepend_input_copy(ctx, instrs, var, var->data_type, 0, var->modifiers, &var->semantic);
 }
 
 static void append_output_copy(struct hlsl_ctx *ctx, struct list *instrs, struct hlsl_ir_var *var,
-        struct hlsl_type *type, unsigned int field_offset, const struct hlsl_semantic *semantic)
+        struct hlsl_type *type, unsigned int field_offset, unsigned int modifiers, const struct hlsl_semantic *semantic)
 {
     struct vkd3d_string_buffer *name;
     struct hlsl_semantic new_semantic;
@@ -138,6 +172,21 @@ static void append_output_copy(struct hlsl_ctx *ctx, struct list *instrs, struct
     struct hlsl_ir_var *output;
     struct hlsl_ir_load *load;
 
+    if (type->type == HLSL_CLASS_MATRIX)
+    {
+        struct hlsl_type *vector_type = hlsl_get_vector_type(ctx, type->base_type, minor_size(type));
+        struct hlsl_semantic vector_semantic = *semantic;
+        unsigned int i;
+
+        for (i = 0; i < major_size(type); ++i)
+        {
+            append_output_copy(ctx, instrs, var, vector_type, 4 * i, modifiers, &vector_semantic);
+            ++vector_semantic.index;
+        }
+
+        return;
+    }
+
     if (!(name = hlsl_get_string_buffer(ctx)))
         return;
     vkd3d_string_buffer_printf(name, "<output-%s%u>", semantic->name, semantic->index);
@@ -147,7 +196,8 @@ static void append_output_copy(struct hlsl_ctx *ctx, struct list *instrs, struct
         return;
     }
     new_semantic.index = semantic->index;
-    if (!(output = hlsl_new_var(ctx, hlsl_strdup(ctx, name->buffer), type, var->loc, &new_semantic, 0, NULL)))
+    if (!(output = hlsl_new_var(ctx, hlsl_strdup(ctx, name->buffer),
+            type, var->loc, &new_semantic, modifiers, NULL)))
     {
         vkd3d_free((void *)new_semantic.name);
         hlsl_release_string_buffer(ctx, name);
@@ -159,7 +209,7 @@ static void append_output_copy(struct hlsl_ctx *ctx, struct list *instrs, struct
     list_add_before(&var->scope_entry, &output->scope_entry);
     list_add_tail(&ctx->extern_vars, &output->extern_entry);
 
-    if (!(offset = hlsl_new_uint_constant(ctx, field_offset, var->loc)))
+    if (!(offset = hlsl_new_uint_constant(ctx, field_offset, &var->loc)))
         return;
     list_add_tail(instrs, &offset->node.entry);
 
@@ -182,7 +232,8 @@ static void append_output_struct_copy(struct hlsl_ctx *ctx, struct list *instrs,
         if (field->type->type == HLSL_CLASS_STRUCT)
             append_output_struct_copy(ctx, instrs, var, field->type, field_offset + field->reg_offset);
         else if (field->semantic.name)
-            append_output_copy(ctx, instrs, var, field->type, field_offset + field->reg_offset, &field->semantic);
+            append_output_copy(ctx, instrs, var, field->type,
+                    field_offset + field->reg_offset, field->modifiers, &field->semantic);
         else
             hlsl_error(ctx, &field->loc, VKD3D_SHADER_ERROR_HLSL_MISSING_SEMANTIC,
                     "Field '%s' is missing a semantic.", field->name);
@@ -197,7 +248,7 @@ static void append_output_var_copy(struct hlsl_ctx *ctx, struct list *instrs, st
     if (var->data_type->type == HLSL_CLASS_STRUCT)
         append_output_struct_copy(ctx, instrs, var, var->data_type, 0);
     else if (var->semantic.name)
-        append_output_copy(ctx, instrs, var, var->data_type, 0, &var->semantic);
+        append_output_copy(ctx, instrs, var, var->data_type, 0, var->modifiers, &var->semantic);
 }
 
 static bool transform_ir(struct hlsl_ctx *ctx, bool (*func)(struct hlsl_ctx *ctx, struct hlsl_ir_node *, void *),
@@ -241,6 +292,7 @@ static bool lower_broadcasts(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, v
             && src_type->type <= HLSL_CLASS_VECTOR && dst_type->type <= HLSL_CLASS_VECTOR
             && src_type->dimx == 1)
     {
+        struct hlsl_ir_node *replacement;
         struct hlsl_ir_swizzle *swizzle;
         struct hlsl_ir_expr *new_cast;
 
@@ -250,19 +302,33 @@ static bool lower_broadcasts(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, v
         if (!(new_cast = hlsl_new_cast(ctx, cast->operands[0].node, dst_scalar_type, &cast->node.loc)))
             return false;
         list_add_after(&cast->node.entry, &new_cast->node.entry);
-        if (!(swizzle = hlsl_new_swizzle(ctx, HLSL_SWIZZLE(X, X, X, X), dst_type->dimx, &new_cast->node, &cast->node.loc)))
-            return false;
-        list_add_after(&new_cast->node.entry, &swizzle->node.entry);
+        replacement = &new_cast->node;
 
-        hlsl_replace_node(&cast->node, &swizzle->node);
+        if (dst_type->dimx != 1)
+        {
+            if (!(swizzle = hlsl_new_swizzle(ctx, HLSL_SWIZZLE(X, X, X, X), dst_type->dimx, replacement, &cast->node.loc)))
+                return false;
+            list_add_after(&new_cast->node.entry, &swizzle->node.entry);
+            replacement = &swizzle->node;
+        }
+
+        hlsl_replace_node(&cast->node, replacement);
         return true;
     }
 
     return false;
 }
 
+enum copy_propagation_value_state
+{
+    VALUE_STATE_NOT_WRITTEN = 0,
+    VALUE_STATE_STATICALLY_WRITTEN,
+    VALUE_STATE_DYNAMICALLY_WRITTEN,
+};
+
 struct copy_propagation_value
 {
+    enum copy_propagation_value_state state;
     struct hlsl_ir_node *node;
     unsigned int component;
 };
@@ -277,6 +343,7 @@ struct copy_propagation_var_def
 struct copy_propagation_state
 {
     struct rb_tree var_defs;
+    struct copy_propagation_state *parent;
 };
 
 static int copy_propagation_var_def_compare(const void *key, const struct rb_entry *entry)
@@ -294,15 +361,32 @@ static void copy_propagation_var_def_destroy(struct rb_entry *entry, void *conte
     vkd3d_free(var_def);
 }
 
-static struct copy_propagation_var_def *copy_propagation_get_var_def(const struct copy_propagation_state *state,
-        const struct hlsl_ir_var *var)
+static struct copy_propagation_value *copy_propagation_get_value(const struct copy_propagation_state *state,
+        const struct hlsl_ir_var *var, unsigned component)
 {
-    struct rb_entry *entry = rb_get(&state->var_defs, var);
+    for (; state; state = state->parent)
+    {
+        struct rb_entry *entry = rb_get(&state->var_defs, var);
+        if (entry)
+        {
+            struct copy_propagation_var_def *var_def = RB_ENTRY_VALUE(entry, struct copy_propagation_var_def, entry);
+            enum copy_propagation_value_state state = var_def->values[component].state;
 
-    if (entry)
-        return RB_ENTRY_VALUE(entry, struct copy_propagation_var_def, entry);
-    else
-        return NULL;
+            assert(component < var_def->var->data_type->reg_size);
+
+            switch (state)
+            {
+                case VALUE_STATE_STATICALLY_WRITTEN:
+                    return &var_def->values[component];
+                case VALUE_STATE_DYNAMICALLY_WRITTEN:
+                    return NULL;
+                case VALUE_STATE_NOT_WRITTEN:
+                    break;
+            }
+        }
+    }
+
+    return NULL;
 }
 
 static struct copy_propagation_var_def *copy_propagation_create_var_def(struct hlsl_ctx *ctx,
@@ -326,10 +410,28 @@ static struct copy_propagation_var_def *copy_propagation_create_var_def(struct h
     return var_def;
 }
 
+static void copy_propagation_invalidate_variable(struct copy_propagation_var_def *var_def,
+        unsigned offset, unsigned char writemask)
+{
+    unsigned i;
+
+    TRACE("Invalidate variable %s[%u]%s.\n", var_def->var->name, offset, debug_hlsl_writemask(writemask));
+
+    for (i = 0; i < 4; ++i)
+    {
+        if (writemask & (1u << i))
+            var_def->values[offset + i].state = VALUE_STATE_DYNAMICALLY_WRITTEN;
+    }
+}
+
 static void copy_propagation_invalidate_whole_variable(struct copy_propagation_var_def *var_def)
 {
+    unsigned i;
+
     TRACE("Invalidate variable %s.\n", var_def->var->name);
-    memset(var_def->values, 0, sizeof(*var_def->values) * var_def->var->data_type->reg_size);
+
+    for (i = 0; i < var_def->var->data_type->reg_size; ++i)
+        var_def->values[i].state = VALUE_STATE_DYNAMICALLY_WRITTEN;
 }
 
 static void copy_propagation_set_value(struct copy_propagation_var_def *var_def, unsigned int offset,
@@ -343,6 +445,7 @@ static void copy_propagation_set_value(struct copy_propagation_var_def *var_def,
         {
             TRACE("Variable %s[%u] is written by instruction %p%s.\n",
                     var_def->var->name, offset + i, node, debug_hlsl_writemask(1u << i));
+            var_def->values[offset + i].state = VALUE_STATE_STATICALLY_WRITTEN;
             var_def->values[offset + i].node = node;
             var_def->values[offset + i].component = j++;
         }
@@ -354,32 +457,34 @@ static struct hlsl_ir_node *copy_propagation_compute_replacement(struct hlsl_ctx
         unsigned int count, unsigned int *swizzle)
 {
     const struct hlsl_ir_var *var = deref->var;
-    struct copy_propagation_var_def *var_def;
     struct hlsl_ir_node *node = NULL;
     unsigned int offset, i;
 
     if (!hlsl_offset_from_deref(ctx, deref, &offset))
         return NULL;
 
-    if (!(var_def = copy_propagation_get_var_def(state, var)))
-        return NULL;
-
-    assert(offset + count <= var_def->var->data_type->reg_size);
+    if (var->data_type->type != HLSL_CLASS_OBJECT)
+        assert(offset + count <= var->data_type->reg_size);
 
     *swizzle = 0;
 
     for (i = 0; i < count; ++i)
     {
+        struct copy_propagation_value *value = copy_propagation_get_value(state, var, offset + i);
+
+        if (!value)
+            return NULL;
+
         if (!node)
         {
-            node = var_def->values[offset + i].node;
+            node = value->node;
         }
-        else if (node != var_def->values[offset + i].node)
+        else if (node != value->node)
         {
             TRACE("No single source for propagating load from %s[%u-%u].\n", var->name, offset, offset + count);
             return NULL;
         }
-        *swizzle |= var_def->values[offset + i].component << i * 2;
+        *swizzle |= value->component << i * 2;
     }
 
     TRACE("Load from %s[%u-%u] propagated as instruction %p%s.\n",
@@ -483,6 +588,113 @@ static void copy_propagation_record_store(struct hlsl_ctx *ctx, struct hlsl_ir_s
     }
 }
 
+static void copy_propagation_state_init(struct hlsl_ctx *ctx, struct copy_propagation_state *state,
+        struct copy_propagation_state *parent)
+{
+    rb_init(&state->var_defs, copy_propagation_var_def_compare);
+    state->parent = parent;
+}
+
+static void copy_propagation_state_destroy(struct copy_propagation_state *state)
+{
+    rb_destroy(&state->var_defs, copy_propagation_var_def_destroy, NULL);
+}
+
+static void copy_propagation_invalidate_from_block(struct hlsl_ctx *ctx, struct copy_propagation_state *state,
+        struct hlsl_block *block)
+{
+    struct hlsl_ir_node *instr;
+
+    LIST_FOR_EACH_ENTRY(instr, &block->instrs, struct hlsl_ir_node, entry)
+    {
+        switch (instr->type)
+        {
+            case HLSL_IR_STORE:
+            {
+                struct hlsl_ir_store *store = hlsl_ir_store(instr);
+                struct copy_propagation_var_def *var_def;
+                struct hlsl_deref *lhs = &store->lhs;
+                struct hlsl_ir_var *var = lhs->var;
+                unsigned int offset;
+
+                if (!(var_def = copy_propagation_create_var_def(ctx, state, var)))
+                    continue;
+
+                if (hlsl_offset_from_deref(ctx, lhs, &offset))
+                    copy_propagation_invalidate_variable(var_def, offset, store->writemask);
+                else
+                    copy_propagation_invalidate_whole_variable(var_def);
+
+                break;
+            }
+
+            case HLSL_IR_IF:
+            {
+                struct hlsl_ir_if *iff = hlsl_ir_if(instr);
+
+                copy_propagation_invalidate_from_block(ctx, state, &iff->then_instrs);
+                copy_propagation_invalidate_from_block(ctx, state, &iff->else_instrs);
+
+                break;
+            }
+
+            case HLSL_IR_LOOP:
+            {
+                struct hlsl_ir_loop *loop = hlsl_ir_loop(instr);
+
+                copy_propagation_invalidate_from_block(ctx, state, &loop->body);
+
+                break;
+            }
+
+            default:
+                break;
+        }
+    }
+}
+
+static bool copy_propagation_transform_block(struct hlsl_ctx *ctx, struct hlsl_block *block,
+        struct copy_propagation_state *state);
+
+static bool copy_propagation_process_if(struct hlsl_ctx *ctx, struct hlsl_ir_if *iff,
+        struct copy_propagation_state *state)
+{
+    struct copy_propagation_state inner_state;
+    bool progress = false;
+
+    copy_propagation_state_init(ctx, &inner_state, state);
+    progress |= copy_propagation_transform_block(ctx, &iff->then_instrs, &inner_state);
+    copy_propagation_state_destroy(&inner_state);
+
+    copy_propagation_state_init(ctx, &inner_state, state);
+    progress |= copy_propagation_transform_block(ctx, &iff->else_instrs, &inner_state);
+    copy_propagation_state_destroy(&inner_state);
+
+    /* Ideally we'd invalidate the outer state looking at what was
+     * touched in the two inner states, but this doesn't work for
+     * loops (because we need to know what is invalidated in advance),
+     * so we need copy_propagation_invalidate_from_block() anyway. */
+    copy_propagation_invalidate_from_block(ctx, state, &iff->then_instrs);
+    copy_propagation_invalidate_from_block(ctx, state, &iff->else_instrs);
+
+    return progress;
+}
+
+static bool copy_propagation_process_loop(struct hlsl_ctx *ctx, struct hlsl_ir_loop *loop,
+        struct copy_propagation_state *state)
+{
+    struct copy_propagation_state inner_state;
+    bool progress = false;
+
+    copy_propagation_invalidate_from_block(ctx, state, &loop->body);
+
+    copy_propagation_state_init(ctx, &inner_state, state);
+    progress |= copy_propagation_transform_block(ctx, &loop->body, &inner_state);
+    copy_propagation_state_destroy(&inner_state);
+
+    return progress;
+}
+
 static bool copy_propagation_transform_block(struct hlsl_ctx *ctx, struct hlsl_block *block,
         struct copy_propagation_state *state)
 {
@@ -506,12 +718,12 @@ static bool copy_propagation_transform_block(struct hlsl_ctx *ctx, struct hlsl_b
                 break;
 
             case HLSL_IR_IF:
-                FIXME("Copy propagation doesn't support conditionals yet, leaving.\n");
-                return progress;
+                progress |= copy_propagation_process_if(ctx, hlsl_ir_if(instr), state);
+                break;
 
             case HLSL_IR_LOOP:
-                FIXME("Copy propagation doesn't support loops yet, leaving.\n");
-                return progress;
+                progress |= copy_propagation_process_loop(ctx, hlsl_ir_loop(instr), state);
+                break;
 
             default:
                 break;
@@ -526,11 +738,11 @@ static bool copy_propagation_execute(struct hlsl_ctx *ctx, struct hlsl_block *bl
     struct copy_propagation_state state;
     bool progress;
 
-    rb_init(&state.var_defs, copy_propagation_var_def_compare);
+    copy_propagation_state_init(ctx, &state, NULL);
 
     progress = copy_propagation_transform_block(ctx, block, &state);
 
-    rb_destroy(&state.var_defs, copy_propagation_var_def_destroy, NULL);
+    copy_propagation_state_destroy(&state);
 
     return progress;
 }
@@ -562,8 +774,10 @@ static bool fold_redundant_casts(struct hlsl_ctx *ctx, struct hlsl_ir_node *inst
     return false;
 }
 
-/* Helper for split_array_copies() and split_struct_copies(). Inserts new
- * instructions right before "store". */
+/* Copy an element of a complex variable. Helper for
+ * split_array_copies(), split_struct_copies() and
+ * split_matrix_copies(). Inserts new instructions right before
+ * "store". */
 static bool split_copy(struct hlsl_ctx *ctx, struct hlsl_ir_store *store,
         const struct hlsl_ir_load *load, const unsigned int offset, struct hlsl_type *type)
 {
@@ -572,7 +786,7 @@ static bool split_copy(struct hlsl_ctx *ctx, struct hlsl_ir_store *store,
     struct hlsl_ir_load *split_load;
     struct hlsl_ir_constant *c;
 
-    if (!(c = hlsl_new_uint_constant(ctx, offset, store->node.loc)))
+    if (!(c = hlsl_new_uint_constant(ctx, offset, &store->node.loc)))
         return false;
     list_add_before(&store->node.entry, &c->node.entry);
 
@@ -621,7 +835,13 @@ static bool split_array_copies(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr,
     if (type->type != HLSL_CLASS_ARRAY)
         return false;
     element_type = type->e.array.type;
-    element_size = element_type->reg_size;
+    element_size = hlsl_type_get_array_element_reg_size(element_type);
+
+    if (rhs->type != HLSL_IR_LOAD)
+    {
+        hlsl_fixme(ctx, &instr->loc, "Array store rhs is not HLSL_IR_LOAD. Broadcast may be missing.");
+        return false;
+    }
 
     for (i = 0; i < type->e.array.elements_count; ++i)
     {
@@ -653,6 +873,12 @@ static bool split_struct_copies(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr
     if (type->type != HLSL_CLASS_STRUCT)
         return false;
 
+    if (rhs->type != HLSL_IR_LOAD)
+    {
+        hlsl_fixme(ctx, &instr->loc, "Struct store rhs is not HLSL_IR_LOAD. Broadcast may be missing.");
+        return false;
+    }
+
     LIST_FOR_EACH_ENTRY(field, type->e.elements, struct hlsl_struct_field, entry)
     {
         if (!split_copy(ctx, store, hlsl_ir_load(rhs), field->reg_offset, field->type))
@@ -667,6 +893,41 @@ static bool split_struct_copies(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr
     return true;
 }
 
+static bool split_matrix_copies(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void *context)
+{
+    const struct hlsl_ir_node *rhs;
+    struct hlsl_type *element_type;
+    const struct hlsl_type *type;
+    unsigned int i;
+    struct hlsl_ir_store *store;
+
+    if (instr->type != HLSL_IR_STORE)
+        return false;
+
+    store = hlsl_ir_store(instr);
+    rhs = store->rhs.node;
+    type = rhs->data_type;
+    if (type->type != HLSL_CLASS_MATRIX)
+        return false;
+    element_type = hlsl_get_vector_type(ctx, type->base_type, minor_size(type));
+
+    if (rhs->type != HLSL_IR_LOAD)
+    {
+        hlsl_fixme(ctx, &instr->loc, "Copying from unsupported node type.\n");
+        return false;
+    }
+
+    for (i = 0; i < major_size(type); ++i)
+    {
+        if (!split_copy(ctx, store, hlsl_ir_load(rhs), 4 * i, element_type))
+            return false;
+    }
+
+    list_remove(&store->node.entry);
+    hlsl_free_instr(&store->node);
+    return true;
+}
+
 static bool lower_narrowing_casts(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void *context)
 {
     const struct hlsl_type *src_type, *dst_type;
@@ -745,6 +1006,37 @@ static bool lower_division(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, voi
     return true;
 }
 
+static bool lower_casts_to_bool(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void *context)
+{
+    struct hlsl_type *type = instr->data_type, *arg_type;
+    struct hlsl_ir_constant *zero;
+    struct hlsl_ir_expr *expr;
+
+    if (instr->type != HLSL_IR_EXPR)
+        return false;
+    expr = hlsl_ir_expr(instr);
+    if (expr->op != HLSL_OP1_CAST)
+        return false;
+    arg_type = expr->operands[0].node->data_type;
+    if (type->type > HLSL_CLASS_VECTOR || arg_type->type > HLSL_CLASS_VECTOR)
+        return false;
+    if (type->base_type != HLSL_TYPE_BOOL)
+        return false;
+
+    /* Narrowing casts should have already been lowered. */
+    assert(type->dimx == arg_type->dimx);
+
+    zero = hlsl_new_constant(ctx, arg_type, &instr->loc);
+    if (!zero)
+        return false;
+    list_add_before(&instr->entry, &zero->node.entry);
+
+    expr->op = HLSL_OP2_NEQUAL;
+    hlsl_src_from_node(&expr->operands[1], &zero->node);
+
+    return true;
+}
+
 static bool dce(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void *context)
 {
     switch (instr->type)
@@ -1086,7 +1378,7 @@ static void allocate_variable_temp_register(struct hlsl_ctx *ctx, struct hlsl_ir
                     var->last_read, var->data_type->reg_size);
         else
             var->reg = allocate_register(ctx, liveness, var->first_write,
-                    var->last_read, var->data_type->dimx);
+                    var->last_read, hlsl_type_component_count(var->data_type));
         TRACE("Allocated %s to %s (liveness %u-%u).\n", var->name,
                 debug_register('r', var->reg, var->data_type), var->first_write, var->last_read);
     }
@@ -1200,7 +1492,7 @@ static void allocate_const_registers_recurse(struct hlsl_ctx *ctx, struct hlsl_b
                         switch (type->base_type)
                         {
                             case HLSL_TYPE_BOOL:
-                                f = value->b;
+                                f = !!value->u;
                                 break;
 
                             case HLSL_TYPE_FLOAT:
@@ -1542,11 +1834,6 @@ static void allocate_objects(struct hlsl_ctx *ctx, enum hlsl_base_type type)
     }
 }
 
-static bool type_is_single_reg(const struct hlsl_type *type)
-{
-    return type->type == HLSL_CLASS_SCALAR || type->type == HLSL_CLASS_VECTOR;
-}
-
 bool hlsl_offset_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref, unsigned int *offset)
 {
     struct hlsl_ir_node *offset_node = deref->offset.node;
@@ -1569,7 +1856,7 @@ bool hlsl_offset_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref
     if (*offset >= deref->var->data_type->reg_size)
     {
         hlsl_error(ctx, &deref->offset.node->loc, VKD3D_SHADER_ERROR_HLSL_OFFSET_OUT_OF_BOUNDS,
-                "Dereference is out of bounds.");
+                "Dereference is out of bounds. %u/%u", *offset, deref->var->data_type->reg_size);
         return false;
     }
 
@@ -1589,8 +1876,7 @@ unsigned int hlsl_offset_from_deref_safe(struct hlsl_ctx *ctx, const struct hlsl
     return 0;
 }
 
-struct hlsl_reg hlsl_reg_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref,
-        const struct hlsl_type *type)
+struct hlsl_reg hlsl_reg_from_deref(struct hlsl_ctx *ctx, const struct hlsl_deref *deref)
 {
     const struct hlsl_ir_var *var = deref->var;
     struct hlsl_reg ret = var->reg;
@@ -1598,16 +1884,10 @@ struct hlsl_reg hlsl_reg_from_deref(struct hlsl_ctx *ctx, const struct hlsl_dere
 
     ret.id += offset / 4;
 
-    if (type_is_single_reg(var->data_type))
-    {
-        assert(!offset);
-        ret.writemask = var->reg.writemask;
-    }
-    else
-    {
-        assert(type_is_single_reg(type));
-        ret.writemask = ((1 << type->dimx) - 1) << (offset % 4);
-    }
+    ret.writemask = 0xf & (0xf << (offset % 4));
+    if (var->reg.writemask)
+        ret.writemask = hlsl_combine_writemasks(var->reg.writemask, ret.writemask);
+
     return ret;
 }
 
@@ -1661,7 +1941,9 @@ int hlsl_emit_bytecode(struct hlsl_ctx *ctx, struct hlsl_ir_function_decl *entry
         progress |= transform_ir(ctx, split_struct_copies, body, NULL);
     }
     while (progress);
+    transform_ir(ctx, split_matrix_copies, body, NULL);
     transform_ir(ctx, lower_narrowing_casts, body, NULL);
+    transform_ir(ctx, lower_casts_to_bool, body, NULL);
     do
     {
         progress = transform_ir(ctx, hlsl_fold_constants, body, NULL);
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl_constant_ops.c b/libs/vkd3d/libs/vkd3d-shader/hlsl_constant_ops.c
index 51cee179e..d8787c21f 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl_constant_ops.c
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl_constant_ops.c
@@ -18,6 +18,8 @@
  * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301, USA
  */
 
+#include <math.h>
+
 #include "hlsl.h"
 
 static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct hlsl_ir_constant *src)
@@ -27,7 +29,6 @@ static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct
     int32_t i;
     double d;
     float f;
-    bool b;
 
     if (dst->node.data_type->dimx != src->node.data_type->dimx
             || dst->node.data_type->dimy != src->node.data_type->dimy)
@@ -47,7 +48,6 @@ static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct
                 i = src->value[k].f;
                 f = src->value[k].f;
                 d = src->value[k].f;
-                b = src->value[k].f;
                 break;
 
             case HLSL_TYPE_DOUBLE:
@@ -55,7 +55,6 @@ static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct
                 i = src->value[k].d;
                 f = src->value[k].d;
                 d = src->value[k].d;
-                b = src->value[k].d;
                 break;
 
             case HLSL_TYPE_INT:
@@ -63,7 +62,6 @@ static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct
                 i = src->value[k].i;
                 f = src->value[k].i;
                 d = src->value[k].i;
-                b = src->value[k].i;
                 break;
 
             case HLSL_TYPE_UINT:
@@ -71,20 +69,17 @@ static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct
                 i = src->value[k].u;
                 f = src->value[k].u;
                 d = src->value[k].u;
-                b = src->value[k].u;
                 break;
 
             case HLSL_TYPE_BOOL:
-                u = src->value[k].b;
-                i = src->value[k].b;
-                f = src->value[k].b;
-                d = src->value[k].b;
-                b = src->value[k].b;
+                u = !!src->value[k].u;
+                i = !!src->value[k].u;
+                f = !!src->value[k].u;
+                d = !!src->value[k].u;
                 break;
 
             default:
-                FIXME("Cast from %s to %s.\n", debug_hlsl_type(ctx, src->node.data_type),
-                        debug_hlsl_type(ctx, dst->node.data_type));
+                assert(0);
                 return false;
         }
 
@@ -108,12 +103,12 @@ static bool fold_cast(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst, struct
                 break;
 
             case HLSL_TYPE_BOOL:
-                dst->value[k].b = b;
+                /* Casts to bool should have already been lowered. */
+                assert(0);
                 break;
 
             default:
-                FIXME("Cast from %s to %s.\n", debug_hlsl_type(ctx, src->node.data_type),
-                        debug_hlsl_type(ctx, dst->node.data_type));
+                assert(0);
                 return false;
         }
     }
@@ -194,11 +189,12 @@ static bool fold_mul(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst,
         struct hlsl_ir_constant *src1, struct hlsl_ir_constant *src2)
 {
     enum hlsl_base_type type = dst->node.data_type->base_type;
+    unsigned int k;
 
     assert(type == src1->node.data_type->base_type);
     assert(type == src2->node.data_type->base_type);
 
-    for (int k = 0; k < 4; ++k)
+    for (k = 0; k < 4; ++k)
     {
         switch (type)
         {
@@ -224,6 +220,155 @@ static bool fold_mul(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst,
     return true;
 }
 
+static bool fold_nequal(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst,
+        struct hlsl_ir_constant *src1, struct hlsl_ir_constant *src2)
+{
+    unsigned int k;
+
+    assert(dst->node.data_type->base_type == HLSL_TYPE_BOOL);
+    assert(src1->node.data_type->base_type == src2->node.data_type->base_type);
+
+    for (k = 0; k < 4; ++k)
+    {
+        switch (src1->node.data_type->base_type)
+        {
+            case HLSL_TYPE_FLOAT:
+            case HLSL_TYPE_HALF:
+                dst->value[k].u = src1->value[k].f != src2->value[k].f;
+                break;
+
+            case HLSL_TYPE_DOUBLE:
+                dst->value[k].u = src1->value[k].d != src2->value[k].d;
+                break;
+
+            case HLSL_TYPE_INT:
+            case HLSL_TYPE_UINT:
+            case HLSL_TYPE_BOOL:
+                dst->value[k].u = src1->value[k].u != src2->value[k].u;
+                break;
+
+            default:
+                assert(0);
+                return false;
+        }
+
+        dst->value[k].u *= ~0u;
+    }
+    return true;
+}
+
+static bool fold_div(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst,
+        struct hlsl_ir_constant *src1, struct hlsl_ir_constant *src2)
+{
+    enum hlsl_base_type type = dst->node.data_type->base_type;
+    unsigned int k;
+
+    assert(type == src1->node.data_type->base_type);
+    assert(type == src2->node.data_type->base_type);
+
+    for (k = 0; k < dst->node.data_type->dimx; ++k)
+    {
+        switch (type)
+        {
+            case HLSL_TYPE_FLOAT:
+            case HLSL_TYPE_HALF:
+                if (ctx->profile->major_version >= 4 && src2->value[k].f == 0)
+                {
+                    hlsl_warning(ctx, &dst->node.loc, VKD3D_SHADER_WARNING_HLSL_DIVISION_BY_ZERO,
+                            "Floating point division by zero.");
+                }
+                dst->value[k].f = src1->value[k].f / src2->value[k].f;
+                if (ctx->profile->major_version < 4 && !isfinite(dst->value[k].f))
+                {
+                    hlsl_error(ctx, &dst->node.loc, VKD3D_SHADER_ERROR_HLSL_DIVISION_BY_ZERO,
+                            "Infinities and NaNs are not allowed by the shader model.");
+                }
+                break;
+
+            case HLSL_TYPE_DOUBLE:
+                if (src2->value[k].d == 0)
+                {
+                    hlsl_warning(ctx, &dst->node.loc, VKD3D_SHADER_WARNING_HLSL_DIVISION_BY_ZERO,
+                            "Floating point division by zero.");
+                }
+                dst->value[k].d = src1->value[k].d / src2->value[k].d;
+                break;
+
+            case HLSL_TYPE_INT:
+                if (src2->value[k].i == 0)
+                {
+                    hlsl_error(ctx, &dst->node.loc, VKD3D_SHADER_ERROR_HLSL_DIVISION_BY_ZERO,
+                            "Division by zero.");
+                    return false;
+                }
+                if (src1->value[k].i == INT_MIN && src2->value[k].i == -1)
+                    dst->value[k].i = INT_MIN;
+                else
+                    dst->value[k].i = src1->value[k].i / src2->value[k].i;
+                break;
+
+            case HLSL_TYPE_UINT:
+                if (src2->value[k].u == 0)
+                {
+                    hlsl_error(ctx, &dst->node.loc, VKD3D_SHADER_ERROR_HLSL_DIVISION_BY_ZERO,
+                            "Division by zero.");
+                    return false;
+                }
+                dst->value[k].u = src1->value[k].u / src2->value[k].u;
+                break;
+
+            default:
+                FIXME("Fold division for type %s.\n", debug_hlsl_type(ctx, dst->node.data_type));
+                return false;
+        }
+    }
+    return true;
+}
+
+static bool fold_mod(struct hlsl_ctx *ctx, struct hlsl_ir_constant *dst,
+        struct hlsl_ir_constant *src1, struct hlsl_ir_constant *src2)
+{
+    enum hlsl_base_type type = dst->node.data_type->base_type;
+    unsigned int k;
+
+    assert(type == src1->node.data_type->base_type);
+    assert(type == src2->node.data_type->base_type);
+
+    for (k = 0; k < dst->node.data_type->dimx; ++k)
+    {
+        switch (type)
+        {
+            case HLSL_TYPE_INT:
+                if (src2->value[k].i == 0)
+                {
+                    hlsl_error(ctx, &dst->node.loc, VKD3D_SHADER_ERROR_HLSL_DIVISION_BY_ZERO,
+                            "Division by zero.");
+                    return false;
+                }
+                if (src1->value[k].i == INT_MIN && src2->value[k].i == -1)
+                    dst->value[k].i = 0;
+                else
+                    dst->value[k].i = src1->value[k].i % src2->value[k].i;
+                break;
+
+            case HLSL_TYPE_UINT:
+                if (src2->value[k].u == 0)
+                {
+                    hlsl_error(ctx, &dst->node.loc, VKD3D_SHADER_ERROR_HLSL_DIVISION_BY_ZERO,
+                            "Division by zero.");
+                    return false;
+                }
+                dst->value[k].u = src1->value[k].u % src2->value[k].u;
+                break;
+
+            default:
+                FIXME("Fold modulus for type %s.\n", debug_hlsl_type(ctx, dst->node.data_type));
+                return false;
+        }
+    }
+    return true;
+}
+
 bool hlsl_fold_constants(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void *context)
 {
     struct hlsl_ir_constant *arg1, *arg2 = NULL, *res;
@@ -235,10 +380,17 @@ bool hlsl_fold_constants(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void
         return false;
     expr = hlsl_ir_expr(instr);
 
+    if (instr->data_type->type > HLSL_CLASS_VECTOR)
+        return false;
+
     for (i = 0; i < ARRAY_SIZE(expr->operands); ++i)
     {
-        if (expr->operands[i].node && expr->operands[i].node->type != HLSL_IR_CONSTANT)
-            return false;
+        if (expr->operands[i].node)
+        {
+            if (expr->operands[i].node->type != HLSL_IR_CONSTANT)
+                return false;
+            assert(expr->operands[i].node->data_type->type <= HLSL_CLASS_VECTOR);
+        }
     }
     arg1 = hlsl_ir_constant(expr->operands[0].node);
     if (expr->operands[1].node)
@@ -266,6 +418,18 @@ bool hlsl_fold_constants(struct hlsl_ctx *ctx, struct hlsl_ir_node *instr, void
             success = fold_mul(ctx, res, arg1, arg2);
             break;
 
+        case HLSL_OP2_NEQUAL:
+            success = fold_nequal(ctx, res, arg1, arg2);
+            break;
+
+        case HLSL_OP2_DIV:
+            success = fold_div(ctx, res, arg1, arg2);
+            break;
+
+        case HLSL_OP2_MOD:
+            success = fold_mod(ctx, res, arg1, arg2);
+            break;
+
         default:
             FIXME("Fold \"%s\" expression.\n", debug_hlsl_expr_op(expr->op));
             success = false;
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl_sm1.c b/libs/vkd3d/libs/vkd3d-shader/hlsl_sm1.c
index 90ec6058c..0cdd39174 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl_sm1.c
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl_sm1.c
@@ -663,7 +663,7 @@ static void write_sm1_expr(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *b
 static void write_sm1_load(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *buffer, const struct hlsl_ir_node *instr)
 {
     const struct hlsl_ir_load *load = hlsl_ir_load(instr);
-    const struct hlsl_reg reg = hlsl_reg_from_deref(ctx, &load->src, instr->data_type);
+    const struct hlsl_reg reg = hlsl_reg_from_deref(ctx, &load->src);
     struct sm1_instruction sm1_instr =
     {
         .opcode = D3DSIO_MOV,
@@ -707,7 +707,7 @@ static void write_sm1_store(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *
 {
     const struct hlsl_ir_store *store = hlsl_ir_store(instr);
     const struct hlsl_ir_node *rhs = store->rhs.node;
-    const struct hlsl_reg reg = hlsl_reg_from_deref(ctx, &store->lhs, rhs->data_type);
+    const struct hlsl_reg reg = hlsl_reg_from_deref(ctx, &store->lhs);
     struct sm1_instruction sm1_instr =
     {
         .opcode = D3DSIO_MOV,
@@ -790,7 +790,7 @@ static void write_sm1_instructions(struct hlsl_ctx *ctx, struct vkd3d_bytecode_b
             }
             else if (instr->data_type->type == HLSL_CLASS_OBJECT)
             {
-                hlsl_fixme(ctx, &instr->loc, "Object copy.\n");
+                hlsl_fixme(ctx, &instr->loc, "Object copy.");
                 break;
             }
 
diff --git a/libs/vkd3d/libs/vkd3d-shader/hlsl_sm4.c b/libs/vkd3d/libs/vkd3d-shader/hlsl_sm4.c
index fc07e6ea7..3558d236b 100644
--- a/libs/vkd3d/libs/vkd3d-shader/hlsl_sm4.c
+++ b/libs/vkd3d/libs/vkd3d-shader/hlsl_sm4.c
@@ -25,6 +25,20 @@
 
 static void write_sm4_block(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *buffer, const struct hlsl_block *block);
 
+static bool type_is_integer(const struct hlsl_type *type)
+{
+    switch (type->base_type)
+    {
+        case HLSL_TYPE_BOOL:
+        case HLSL_TYPE_INT:
+        case HLSL_TYPE_UINT:
+            return true;
+
+        default:
+            return false;
+    }
+}
+
 bool hlsl_sm4_register_from_semantic(struct hlsl_ctx *ctx, const struct hlsl_semantic *semantic,
         bool output, enum vkd3d_sm4_register_type *type, enum vkd3d_sm4_swizzle_type *swizzle_type, bool *has_idx)
 {
@@ -100,6 +114,7 @@ bool hlsl_sm4_usage_from_semantic(struct hlsl_ctx *ctx, const struct hlsl_semant
         {"sv_depth",                    true,  VKD3D_SHADER_TYPE_PIXEL,     D3D_NAME_DEPTH},
 
         {"sv_position",                 false, VKD3D_SHADER_TYPE_VERTEX,    D3D_NAME_UNDEFINED},
+        {"sv_vertexid",                 false, VKD3D_SHADER_TYPE_VERTEX,    D3D_NAME_VERTEX_ID},
 
         {"position",                    true,  VKD3D_SHADER_TYPE_VERTEX,    D3D_NAME_POSITION},
         {"sv_position",                 true,  VKD3D_SHADER_TYPE_VERTEX,    D3D_NAME_POSITION},
@@ -275,6 +290,8 @@ static D3D_SHADER_VARIABLE_TYPE sm4_base_type(const struct hlsl_type *type)
     {
         case HLSL_TYPE_BOOL:
             return D3D_SVT_BOOL;
+        case HLSL_TYPE_DOUBLE:
+            return D3D_SVT_DOUBLE;
         case HLSL_TYPE_FLOAT:
         case HLSL_TYPE_HALF:
             return D3D_SVT_FLOAT;
@@ -870,18 +887,20 @@ static void sm4_register_from_deref(struct hlsl_ctx *ctx, struct sm4_register *r
 
         if (hlsl_sm4_register_from_semantic(ctx, &var->semantic, false, &reg->type, swizzle_type, &has_idx))
         {
+            unsigned int offset = hlsl_offset_from_deref_safe(ctx, deref);
+
             if (has_idx)
             {
-                reg->idx[0] = var->semantic.index;
+                reg->idx[0] = var->semantic.index + offset / 4;
                 reg->idx_count = 1;
             }
 
             reg->dim = VKD3D_SM4_DIMENSION_VEC4;
-            *writemask = (1u << data_type->dimx) - 1;
+            *writemask = ((1u << data_type->dimx) - 1) << (offset % 4);
         }
         else
         {
-            struct hlsl_reg hlsl_reg = hlsl_reg_from_deref(ctx, deref, data_type);
+            struct hlsl_reg hlsl_reg = hlsl_reg_from_deref(ctx, deref);
 
             assert(hlsl_reg.allocated);
             reg->type = VKD3D_SM4_RT_INPUT;
@@ -899,9 +918,11 @@ static void sm4_register_from_deref(struct hlsl_ctx *ctx, struct sm4_register *r
 
         if (hlsl_sm4_register_from_semantic(ctx, &var->semantic, true, &reg->type, swizzle_type, &has_idx))
         {
+            unsigned int offset = hlsl_offset_from_deref_safe(ctx, deref);
+
             if (has_idx)
             {
-                reg->idx[0] = var->semantic.index;
+                reg->idx[0] = var->semantic.index + offset / 4;
                 reg->idx_count = 1;
             }
 
@@ -909,11 +930,11 @@ static void sm4_register_from_deref(struct hlsl_ctx *ctx, struct sm4_register *r
                 reg->dim = VKD3D_SM4_DIMENSION_SCALAR;
             else
                 reg->dim = VKD3D_SM4_DIMENSION_VEC4;
-            *writemask = (1u << data_type->dimx) - 1;
+            *writemask = ((1u << data_type->dimx) - 1) << (offset % 4);
         }
         else
         {
-            struct hlsl_reg hlsl_reg = hlsl_reg_from_deref(ctx, deref, data_type);
+            struct hlsl_reg hlsl_reg = hlsl_reg_from_deref(ctx, deref);
 
             assert(hlsl_reg.allocated);
             reg->type = VKD3D_SM4_RT_OUTPUT;
@@ -925,7 +946,7 @@ static void sm4_register_from_deref(struct hlsl_ctx *ctx, struct sm4_register *r
     }
     else
     {
-        struct hlsl_reg hlsl_reg = hlsl_reg_from_deref(ctx, deref, data_type);
+        struct hlsl_reg hlsl_reg = hlsl_reg_from_deref(ctx, deref);
 
         assert(hlsl_reg.allocated);
         reg->type = VKD3D_SM4_RT_TEMP;
@@ -1198,7 +1219,14 @@ static void write_sm4_dcl_semantic(struct hlsl_ctx *ctx, struct vkd3d_bytecode_b
         }
 
         if (profile->type == VKD3D_SHADER_TYPE_PIXEL)
-            instr.opcode |= VKD3DSIM_LINEAR << VKD3D_SM4_INTERPOLATION_MODE_SHIFT;
+        {
+            enum vkd3d_shader_interpolation_mode mode = VKD3DSIM_LINEAR;
+
+            if ((var->modifiers & HLSL_STORAGE_NOINTERPOLATION) || type_is_integer(var->data_type))
+                mode = VKD3DSIM_CONSTANT;
+
+            instr.opcode |= mode << VKD3D_SM4_INTERPOLATION_MODE_SHIFT;
+        }
     }
     else
     {
@@ -1391,7 +1419,7 @@ static void write_sm4_sample(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer
         if (!encode_texel_offset_as_aoffimmi(&instr, texel_offset))
         {
             hlsl_error(ctx, &texel_offset->loc, VKD3D_SHADER_ERROR_HLSL_INVALID_TEXEL_OFFSET,
-                    "Offset must resolve to integer literal in the range -8 to 7.\n");
+                    "Offset must resolve to integer literal in the range -8 to 7.");
             return;
         }
     }
@@ -1407,350 +1435,449 @@ static void write_sm4_sample(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer
     write_sm4_instruction(buffer, &instr);
 }
 
-static void write_sm4_expr(struct hlsl_ctx *ctx,
+static bool type_is_float(const struct hlsl_type *type)
+{
+    return type->base_type == HLSL_TYPE_FLOAT || type->base_type == HLSL_TYPE_HALF;
+}
+
+static void write_sm4_cast(struct hlsl_ctx *ctx,
         struct vkd3d_bytecode_buffer *buffer, const struct hlsl_ir_expr *expr)
 {
     const struct hlsl_ir_node *arg1 = expr->operands[0].node;
-    const struct hlsl_ir_node *arg2 = expr->operands[1].node;
+    const struct hlsl_type *dst_type = expr->node.data_type;
+    const struct hlsl_type *src_type = arg1->data_type;
 
-    assert(expr->node.reg.allocated);
+    /* Narrowing casts were already lowered. */
+    assert(src_type->dimx == dst_type->dimx);
 
-    switch (expr->node.data_type->base_type)
+    switch (dst_type->base_type)
     {
         case HLSL_TYPE_FLOAT:
-        {
-            switch (expr->op)
+            switch (src_type->base_type)
             {
-                case HLSL_OP1_ABS:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, VKD3D_SM4_REGISTER_MODIFIER_ABS);
+                case HLSL_TYPE_HALF:
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, 0);
                     break;
 
-                case HLSL_OP1_CAST:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
+                case HLSL_TYPE_INT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_ITOF, &expr->node, arg1, 0);
+                    break;
 
-                    /* Narrowing casts were already lowered. */
-                    assert(src_type->dimx == expr->node.data_type->dimx);
+                case HLSL_TYPE_UINT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_UTOF, &expr->node, arg1, 0);
+                    break;
 
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_HALF:
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, 0);
-                            break;
+                case HLSL_TYPE_BOOL:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from bool to float.");
+                    break;
 
-                        case HLSL_TYPE_INT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_ITOF, &expr->node, arg1, 0);
-                            break;
+                case HLSL_TYPE_DOUBLE:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from double to float.");
+                    break;
 
-                        case HLSL_TYPE_UINT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_UTOF, &expr->node, arg1, 0);
-                            break;
+                default:
+                    assert(0);
+            }
+            break;
 
-                        case HLSL_TYPE_BOOL:
-                            hlsl_fixme(ctx, &expr->node.loc, "Casts from bool to float are not implemented.\n");
-                            break;
+        case HLSL_TYPE_INT:
+            switch (src_type->base_type)
+            {
+                case HLSL_TYPE_HALF:
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_FTOI, &expr->node, arg1, 0);
+                    break;
 
-                        case HLSL_TYPE_DOUBLE:
-                            hlsl_fixme(ctx, &expr->node.loc, "Casts from double to float are not implemented.\n");
-                            break;
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, 0);
+                    break;
 
-                        default:
-                            break;
-                    }
+                case HLSL_TYPE_BOOL:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from bool to int.");
                     break;
-                }
 
-                case HLSL_OP1_EXP2:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_EXP, &expr->node, arg1, 0);
+                case HLSL_TYPE_DOUBLE:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from double to int.");
                     break;
 
-                case HLSL_OP1_FLOOR:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_ROUND_NI, &expr->node, arg1, 0);
+                default:
+                    assert(0);
+            }
+            break;
+
+        case HLSL_TYPE_UINT:
+            switch (src_type->base_type)
+            {
+                case HLSL_TYPE_HALF:
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_FTOU, &expr->node, arg1, 0);
                     break;
 
-                case HLSL_OP1_LOG2:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_LOG, &expr->node, arg1, 0);
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, 0);
                     break;
 
-                case HLSL_OP1_NEG:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, VKD3D_SM4_REGISTER_MODIFIER_NEGATE);
+                case HLSL_TYPE_BOOL:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from bool to uint.");
+                    break;
+
+                case HLSL_TYPE_DOUBLE:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from double to uint.");
+                    break;
+
+                default:
+                    assert(0);
+            }
+            break;
+
+        case HLSL_TYPE_HALF:
+            hlsl_fixme(ctx, &expr->node.loc, "SM4 cast to half.");
+            break;
+
+        case HLSL_TYPE_DOUBLE:
+            hlsl_fixme(ctx, &expr->node.loc, "SM4 cast to double.");
+            break;
+
+        case HLSL_TYPE_BOOL:
+            /* Casts to bool should have already been lowered. */
+            assert(0);
+            break;
+
+        default:
+            assert(0);
+    }
+}
+
+static void write_sm4_expr(struct hlsl_ctx *ctx,
+        struct vkd3d_bytecode_buffer *buffer, const struct hlsl_ir_expr *expr)
+{
+    const struct hlsl_ir_node *arg1 = expr->operands[0].node;
+    const struct hlsl_ir_node *arg2 = expr->operands[1].node;
+    const struct hlsl_type *dst_type = expr->node.data_type;
+    struct vkd3d_string_buffer *dst_type_string;
+
+    assert(expr->node.reg.allocated);
+
+    if (!(dst_type_string = hlsl_type_to_string(ctx, dst_type)))
+        return;
+
+    switch (expr->op)
+    {
+        case HLSL_OP1_ABS:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, VKD3D_SM4_REGISTER_MODIFIER_ABS);
                     break;
 
-                case HLSL_OP1_ROUND:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_ROUND_NE, &expr->node, arg1, 0);
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s absolute value expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP1_BIT_NOT:
+            assert(type_is_integer(dst_type));
+            write_sm4_unary_op(buffer, VKD3D_SM4_OP_NOT, &expr->node, arg1, 0);
+            break;
+
+        case HLSL_OP1_CAST:
+            write_sm4_cast(ctx, buffer, expr);
+            break;
+
+        case HLSL_OP1_EXP2:
+            assert(type_is_float(dst_type));
+            write_sm4_unary_op(buffer, VKD3D_SM4_OP_EXP, &expr->node, arg1, 0);
+            break;
+
+        case HLSL_OP1_FLOOR:
+            assert(type_is_float(dst_type));
+            write_sm4_unary_op(buffer, VKD3D_SM4_OP_ROUND_NI, &expr->node, arg1, 0);
+            break;
+
+        case HLSL_OP1_LOG2:
+            assert(type_is_float(dst_type));
+            write_sm4_unary_op(buffer, VKD3D_SM4_OP_LOG, &expr->node, arg1, 0);
+            break;
+
+        case HLSL_OP1_NEG:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, VKD3D_SM4_REGISTER_MODIFIER_NEGATE);
                     break;
 
-                case HLSL_OP1_SAT:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV
-                            | (VKD3D_SM4_INSTRUCTION_FLAG_SATURATE << VKD3D_SM4_INSTRUCTION_FLAGS_SHIFT),
-                            &expr->node, arg1, 0);
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
+                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_INEG, &expr->node, arg1, 0);
                     break;
 
-                case HLSL_OP2_ADD:
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s negation expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP1_ROUND:
+            assert(type_is_float(dst_type));
+            write_sm4_unary_op(buffer, VKD3D_SM4_OP_ROUND_NE, &expr->node, arg1, 0);
+            break;
+
+        case HLSL_OP1_SAT:
+            assert(type_is_float(dst_type));
+            write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV
+                    | (VKD3D_SM4_INSTRUCTION_FLAG_SATURATE << VKD3D_SM4_INSTRUCTION_FLAGS_SHIFT),
+                    &expr->node, arg1, 0);
+            break;
+
+        case HLSL_OP2_ADD:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
                     write_sm4_binary_op(buffer, VKD3D_SM4_OP_ADD, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_DIV:
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IADD, &expr->node, arg1, arg2);
+                    break;
+
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s addition expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP2_BIT_AND:
+            assert(type_is_integer(dst_type));
+            write_sm4_binary_op(buffer, VKD3D_SM4_OP_AND, &expr->node, arg1, arg2);
+            break;
+
+        case HLSL_OP2_BIT_OR:
+            assert(type_is_integer(dst_type));
+            write_sm4_binary_op(buffer, VKD3D_SM4_OP_OR, &expr->node, arg1, arg2);
+            break;
+
+        case HLSL_OP2_BIT_XOR:
+            assert(type_is_integer(dst_type));
+            write_sm4_binary_op(buffer, VKD3D_SM4_OP_XOR, &expr->node, arg1, arg2);
+            break;
+
+        case HLSL_OP2_DIV:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
                     write_sm4_binary_op(buffer, VKD3D_SM4_OP_DIV, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MAX:
-                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_MAX, &expr->node, arg1, arg2);
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op_with_two_destinations(buffer, VKD3D_SM4_OP_UDIV, &expr->node, 0, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MIN:
-                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_MIN, &expr->node, arg1, arg2);
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s division expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP2_EQUAL:
+        {
+            const struct hlsl_type *src_type = arg1->data_type;
+
+            assert(dst_type->base_type == HLSL_TYPE_BOOL);
+
+            switch (src_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_EQ, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MUL:
-                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_MUL, &expr->node, arg1, arg2);
+                case HLSL_TYPE_BOOL:
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IEQ, &expr->node, arg1, arg2);
                     break;
 
                 default:
-                    hlsl_fixme(ctx, &expr->node.loc, "SM4 float \"%s\" expression.", debug_hlsl_expr_op(expr->op));
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 equality between \"%s\" operands.",
+                            debug_hlsl_type(ctx, src_type));
                     break;
             }
             break;
         }
 
-        case HLSL_TYPE_INT:
+        case HLSL_OP2_GEQUAL:
         {
-            switch (expr->op)
+            const struct hlsl_type *src_type = arg1->data_type;
+
+            assert(dst_type->base_type == HLSL_TYPE_BOOL);
+
+            switch (src_type->base_type)
             {
-                case HLSL_OP1_CAST:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
-
-                    /* Narrowing casts were already lowered. */
-                    assert(src_type->dimx == expr->node.data_type->dimx);
-
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_HALF:
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_FTOI, &expr->node, arg1, 0);
-                            break;
-
-                        case HLSL_TYPE_INT:
-                        case HLSL_TYPE_UINT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, 0);
-                            break;
-
-                        case HLSL_TYPE_BOOL:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from bool to int.");
-                            break;
-
-                        case HLSL_TYPE_DOUBLE:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from double to int.");
-                            break;
-
-                        default:
-                            break;
-                    }
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_GE, &expr->node, arg1, arg2);
                     break;
-                }
 
-                case HLSL_OP1_NEG:
-                    write_sm4_unary_op(buffer, VKD3D_SM4_OP_INEG, &expr->node, arg1, 0);
+                case HLSL_TYPE_INT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IGE, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MAX:
-                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IMAX, &expr->node, arg1, arg2);
+                case HLSL_TYPE_BOOL:
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_UGE, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MIN:
-                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IMIN, &expr->node, arg1, arg2);
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 greater-than-or-equal between \"%s\" operands.",
+                            debug_hlsl_type(ctx, src_type));
+                    break;
+            }
+            break;
+        }
+
+        case HLSL_OP2_LESS:
+        {
+            const struct hlsl_type *src_type = arg1->data_type;
+
+            assert(dst_type->base_type == HLSL_TYPE_BOOL);
+
+            switch (src_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_LT, &expr->node, arg1, arg2);
+                    break;
+
+                case HLSL_TYPE_INT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_ILT, &expr->node, arg1, arg2);
+                    break;
+
+                case HLSL_TYPE_BOOL:
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_ULT, &expr->node, arg1, arg2);
                     break;
 
                 default:
-                    hlsl_fixme(ctx, &expr->node.loc, "SM4 int \"%s\" expression.", debug_hlsl_expr_op(expr->op));
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 less-than between \"%s\" operands.",
+                            debug_hlsl_type(ctx, src_type));
                     break;
             }
             break;
         }
 
-        case HLSL_TYPE_UINT:
-        {
-            switch (expr->op)
+        case HLSL_OP2_LSHIFT:
+            assert(type_is_integer(dst_type));
+            assert(dst_type->base_type != HLSL_TYPE_BOOL);
+            write_sm4_binary_op(buffer, VKD3D_SM4_OP_ISHL, &expr->node, arg1, arg2);
+            break;
+
+        case HLSL_OP2_MAX:
+            switch (dst_type->base_type)
             {
-                case HLSL_OP1_CAST:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
-
-                    /* Narrowing casts were already lowered. */
-                    assert(src_type->dimx == expr->node.data_type->dimx);
-
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_HALF:
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_FTOU, &expr->node, arg1, 0);
-                            break;
-
-                        case HLSL_TYPE_INT:
-                        case HLSL_TYPE_UINT:
-                            write_sm4_unary_op(buffer, VKD3D_SM4_OP_MOV, &expr->node, arg1, 0);
-                            break;
-
-                        case HLSL_TYPE_BOOL:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from bool to uint.\n");
-                            break;
-
-                        case HLSL_TYPE_DOUBLE:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 cast from double to uint.\n");
-                            break;
-
-                        default:
-                            break;
-                    }
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_MAX, &expr->node, arg1, arg2);
                     break;
-                }
 
-                case HLSL_OP2_MAX:
+                case HLSL_TYPE_INT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IMAX, &expr->node, arg1, arg2);
+                    break;
+
+                case HLSL_TYPE_UINT:
                     write_sm4_binary_op(buffer, VKD3D_SM4_OP_UMAX, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MIN:
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s maximum expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP2_MIN:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_MIN, &expr->node, arg1, arg2);
+                    break;
+
+                case HLSL_TYPE_INT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_IMIN, &expr->node, arg1, arg2);
+                    break;
+
+                case HLSL_TYPE_UINT:
                     write_sm4_binary_op(buffer, VKD3D_SM4_OP_UMIN, &expr->node, arg1, arg2);
                     break;
 
-                case HLSL_OP2_MUL:
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s minimum expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP2_MOD:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op_with_two_destinations(buffer, VKD3D_SM4_OP_UDIV, &expr->node, 1, arg1, arg2);
+                    break;
+
+                default:
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s modulus expression.", dst_type_string->buffer);
+            }
+            break;
+
+        case HLSL_OP2_MUL:
+            switch (dst_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_MUL, &expr->node, arg1, arg2);
+                    break;
+
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
                     /* Using IMUL instead of UMUL because we're taking the low
                      * bits, and the native compiler generates IMUL. */
                     write_sm4_binary_op_with_two_destinations(buffer, VKD3D_SM4_OP_IMUL, &expr->node, 1, arg1, arg2);
                     break;
 
                 default:
-                    hlsl_fixme(ctx, &expr->node.loc, "SM4 uint \"%s\" expression.\n", debug_hlsl_expr_op(expr->op));
-                    break;
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 %s multiplication expression.", dst_type_string->buffer);
             }
             break;
-        }
 
-        case HLSL_TYPE_BOOL:
+        case HLSL_OP2_NEQUAL:
         {
-            switch (expr->op)
-            {
-                case HLSL_OP2_EQUAL:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
-
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_EQ, &expr->node, arg1, arg2);
-                            break;
-
-                        case HLSL_TYPE_BOOL:
-                        case HLSL_TYPE_INT:
-                        case HLSL_TYPE_UINT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_IEQ, &expr->node, arg1, arg2);
-                            break;
-
-                        default:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 equality between \"%s\" operands.",
-                                    debug_hlsl_type(ctx, src_type));
-                            break;
-                    }
-                    break;
-                }
+            const struct hlsl_type *src_type = arg1->data_type;
 
-                case HLSL_OP2_NEQUAL:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
-
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_NE, &expr->node, arg1, arg2);
-                            break;
-
-                        case HLSL_TYPE_BOOL:
-                        case HLSL_TYPE_INT:
-                        case HLSL_TYPE_UINT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_INE, &expr->node, arg1, arg2);
-                            break;
-
-                        default:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 inequality between \"%s\" operands.",
-                                    debug_hlsl_type(ctx, src_type));
-                            break;
-                    }
-                    break;
-                }
+            assert(dst_type->base_type == HLSL_TYPE_BOOL);
 
-                case HLSL_OP2_LESS:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
-
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_LT, &expr->node, arg1, arg2);
-                            break;
-
-                        case HLSL_TYPE_INT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_ILT, &expr->node, arg1, arg2);
-                            break;
-
-                        case HLSL_TYPE_BOOL:
-                        case HLSL_TYPE_UINT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_ULT, &expr->node, arg1, arg2);
-                            break;
-
-                        default:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 less-than between \"%s\" operands.",
-                                    debug_hlsl_type(ctx, src_type));
-                            break;
-                    }
+            switch (src_type->base_type)
+            {
+                case HLSL_TYPE_FLOAT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_NE, &expr->node, arg1, arg2);
                     break;
-                }
 
-                case HLSL_OP2_GEQUAL:
-                {
-                    const struct hlsl_type *src_type = arg1->data_type;
-
-                    switch (src_type->base_type)
-                    {
-                        case HLSL_TYPE_FLOAT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_GE, &expr->node, arg1, arg2);
-                            break;
-
-                        case HLSL_TYPE_INT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_IGE, &expr->node, arg1, arg2);
-                            break;
-
-                        case HLSL_TYPE_BOOL:
-                        case HLSL_TYPE_UINT:
-                            write_sm4_binary_op(buffer, VKD3D_SM4_OP_UGE, &expr->node, arg1, arg2);
-                            break;
-
-                        default:
-                            hlsl_fixme(ctx, &expr->node.loc, "SM4 greater-than-or-equal between \"%s\" operands.",
-                                    debug_hlsl_type(ctx, src_type));
-                            break;
-                    }
+                case HLSL_TYPE_BOOL:
+                case HLSL_TYPE_INT:
+                case HLSL_TYPE_UINT:
+                    write_sm4_binary_op(buffer, VKD3D_SM4_OP_INE, &expr->node, arg1, arg2);
                     break;
-                }
 
                 default:
-                    hlsl_fixme(ctx, &expr->node.loc, "SM4 bool \"%s\" expression.", debug_hlsl_expr_op(expr->op));
+                    hlsl_fixme(ctx, &expr->node.loc, "SM4 inequality between \"%s\" operands.",
+                            debug_hlsl_type(ctx, src_type));
                     break;
             }
             break;
         }
 
-        default:
-        {
-            struct vkd3d_string_buffer *string;
-
-            if ((string = hlsl_type_to_string(ctx, expr->node.data_type)))
-                hlsl_fixme(ctx, &expr->node.loc, "SM4 %s expression.", string->buffer);
-            hlsl_release_string_buffer(ctx, string);
+        case HLSL_OP2_RSHIFT:
+            assert(type_is_integer(dst_type));
+            assert(dst_type->base_type != HLSL_TYPE_BOOL);
+            write_sm4_binary_op(buffer, dst_type->base_type == HLSL_TYPE_INT ? VKD3D_SM4_OP_ISHR : VKD3D_SM4_OP_USHR,
+                    &expr->node, arg1, arg2);
             break;
-        }
+
+        default:
+            hlsl_fixme(ctx, &expr->node.loc, "SM4 %s expression.", debug_hlsl_expr_op(expr->op));
     }
+
+    hlsl_release_string_buffer(ctx, dst_type_string);
 }
 
 static void write_sm4_if(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *buffer, const struct hlsl_ir_if *iff)
@@ -1778,6 +1905,7 @@ static void write_sm4_if(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *buf
     }
 
     instr.opcode = VKD3D_SM4_OP_ENDIF;
+    instr.src_count = 0;
     write_sm4_instruction(buffer, &instr);
 }
 
@@ -1919,12 +2047,6 @@ static void write_sm4_store(struct hlsl_ctx *ctx,
     struct sm4_instruction instr;
     unsigned int writemask;
 
-    if (store->lhs.var->data_type->type == HLSL_CLASS_MATRIX)
-    {
-        hlsl_fixme(ctx, &store->node.loc, "Store to a matrix variable.\n");
-        return;
-    }
-
     memset(&instr, 0, sizeof(instr));
     instr.opcode = VKD3D_SM4_OP_MOV;
 
@@ -1969,12 +2091,12 @@ static void write_sm4_block(struct hlsl_ctx *ctx, struct vkd3d_bytecode_buffer *
         {
             if (instr->data_type->type == HLSL_CLASS_MATRIX)
             {
-                FIXME("Matrix operations need to be lowered.\n");
+                hlsl_fixme(ctx, &instr->loc, "Matrix operations need to be lowered.");
                 break;
             }
             else if (instr->data_type->type == HLSL_CLASS_OBJECT)
             {
-                hlsl_fixme(ctx, &instr->loc, "Object copy.\n");
+                hlsl_fixme(ctx, &instr->loc, "Object copy.");
                 break;
             }
 
diff --git a/libs/vkd3d/libs/vkd3d-shader/preproc.l b/libs/vkd3d/libs/vkd3d-shader/preproc.l
index 3c072a698..bb5a6b61d 100644
--- a/libs/vkd3d/libs/vkd3d-shader/preproc.l
+++ b/libs/vkd3d/libs/vkd3d-shader/preproc.l
@@ -579,7 +579,10 @@ int yylex(YYSTYPE *lval, YYLTYPE *lloc, yyscan_t scanner)
                 if (ctx->current_directive)
                     return return_token(token, lval, text);
 
-                vkd3d_string_buffer_printf(&ctx->buffer, "%s ", text);
+                if (isspace(text[0]))
+                    vkd3d_string_buffer_printf(&ctx->buffer, "%s", text);
+                else
+                    vkd3d_string_buffer_printf(&ctx->buffer, "%s ", text);
                 break;
             }
 
diff --git a/libs/vkd3d/libs/vkd3d-shader/preproc.y b/libs/vkd3d/libs/vkd3d-shader/preproc.y
index 3f02ac036..009c35ffb 100644
--- a/libs/vkd3d/libs/vkd3d-shader/preproc.y
+++ b/libs/vkd3d/libs/vkd3d-shader/preproc.y
@@ -165,38 +165,6 @@ static bool preproc_push_if(struct preproc_ctx *ctx, bool condition)
     return true;
 }
 
-static int char_to_int(char c)
-{
-    if ('0' <= c && c <= '9')
-        return c - '0';
-    if ('A' <= c && c <= 'F')
-        return c - 'A' + 10;
-    if ('a' <= c && c <= 'f')
-        return c - 'a' + 10;
-    return -1;
-}
-
-static uint32_t preproc_parse_integer(const char *s)
-{
-    uint32_t base = 10, ret = 0;
-    int digit;
-
-    if (*s == '0')
-    {
-        base = 8;
-        ++s;
-        if (*s == 'x' || *s == 'X')
-        {
-            base = 16;
-            ++s;
-        }
-    }
-
-    while ((digit = char_to_int(*s++)) >= 0)
-        ret = ret * base + (uint32_t)digit;
-    return ret;
-}
-
 static int default_open_include(const char *filename, bool local,
         const char *parent_data, void *context, struct vkd3d_shader_code *out)
 {
@@ -691,7 +659,7 @@ directive
 primary_expr
     : T_INTEGER
         {
-            $$ = preproc_parse_integer($1);
+            $$ = vkd3d_parse_integer($1);
             vkd3d_free($1);
         }
     | T_IDENTIFIER
diff --git a/libs/vkd3d/libs/vkd3d-shader/sm4.h b/libs/vkd3d/libs/vkd3d-shader/sm4.h
index 62bb4d12b..fd388c815 100644
--- a/libs/vkd3d/libs/vkd3d-shader/sm4.h
+++ b/libs/vkd3d/libs/vkd3d-shader/sm4.h
@@ -350,6 +350,7 @@ enum vkd3d_sm4_opcode
     VKD3D_SM5_OP_DDIV                             = 0xd2,
     VKD3D_SM5_OP_DFMA                             = 0xd3,
     VKD3D_SM5_OP_DRCP                             = 0xd4,
+    VKD3D_SM5_OP_MSAD                             = 0xd5,
     VKD3D_SM5_OP_DTOI                             = 0xd6,
     VKD3D_SM5_OP_DTOU                             = 0xd7,
     VKD3D_SM5_OP_ITOD                             = 0xd8,
diff --git a/libs/vkd3d/libs/vkd3d-shader/spirv.c b/libs/vkd3d/libs/vkd3d-shader/spirv.c
index c87a7b80e..99d247653 100644
--- a/libs/vkd3d/libs/vkd3d-shader/spirv.c
+++ b/libs/vkd3d/libs/vkd3d-shader/spirv.c
@@ -1,6 +1,6 @@
 /*
  * Copyright 2017 Jzef Kucia for CodeWeavers
- * Copyright 2021 Conor McCarthy for Codeweavers
+ * Copyright 2021 Conor McCarthy for CodeWeavers
  *
  * This library is free software; you can redistribute it and/or
  * modify it under the terms of the GNU Lesser General Public
@@ -206,7 +206,7 @@ static enum vkd3d_shader_input_sysval_semantic vkd3d_siv_from_sysval(enum vkd3d_
 
 #define VKD3D_SPIRV_VERSION 0x00010000
 #define VKD3D_SPIRV_GENERATOR_ID 18
-#define VKD3D_SPIRV_GENERATOR_VERSION 3
+#define VKD3D_SPIRV_GENERATOR_VERSION 4
 #define VKD3D_SPIRV_GENERATOR_MAGIC vkd3d_make_u32(VKD3D_SPIRV_GENERATOR_VERSION, VKD3D_SPIRV_GENERATOR_ID)
 
 struct vkd3d_spirv_stream
@@ -2376,6 +2376,8 @@ struct vkd3d_dxbc_compiler *vkd3d_dxbc_compiler_create(const struct vkd3d_shader
 
             default:
                 WARN("Ignoring unrecognised option %#x with value %#x.\n", option->name, option->value);
+
+            case VKD3D_SHADER_COMPILE_OPTION_API_VERSION:
                 break;
         }
     }
@@ -7425,7 +7427,7 @@ static void vkd3d_dxbc_compiler_emit_f16tof32(struct vkd3d_dxbc_compiler *compil
     type_id = vkd3d_spirv_get_type_id(builder, VKD3D_SHADER_COMPONENT_FLOAT, 2);
     scalar_type_id = vkd3d_spirv_get_type_id(builder, VKD3D_SHADER_COMPONENT_FLOAT, 1);
 
-    /* FIXME: Consider a single UnpackHalf2x16 intruction per 2 components. */
+    /* FIXME: Consider a single UnpackHalf2x16 instruction per 2 components. */
     assert(dst->write_mask & VKD3DSP_WRITEMASK_ALL);
     for (i = 0, j = 0; i < VKD3D_VEC4_SIZE; ++i)
     {
@@ -7459,7 +7461,7 @@ static void vkd3d_dxbc_compiler_emit_f32tof16(struct vkd3d_dxbc_compiler *compil
     scalar_type_id = vkd3d_spirv_get_type_id(builder, VKD3D_SHADER_COMPONENT_UINT, 1);
     zero_id = vkd3d_dxbc_compiler_get_constant_float(compiler, 0.0f);
 
-    /* FIXME: Consider a single PackHalf2x16 intruction per 2 components. */
+    /* FIXME: Consider a single PackHalf2x16 instruction per 2 components. */
     assert(dst->write_mask & VKD3DSP_WRITEMASK_ALL);
     for (i = 0, j = 0; i < VKD3D_VEC4_SIZE; ++i)
     {
@@ -7802,7 +7804,7 @@ static int vkd3d_dxbc_compiler_emit_control_flow_instruction(struct vkd3d_dxbc_c
 
             /* The OpSwitch instruction is inserted when the endswitch
              * instruction is processed because we do not know the number
-             * of case statments in advance.*/
+             * of case statements in advance.*/
             vkd3d_spirv_begin_function_stream_insertion(builder, cf_info->u.switch_.stream_location);
             vkd3d_spirv_build_op_switch(builder, cf_info->u.switch_.selector_id,
                     cf_info->u.switch_.default_block_id, cf_info->u.switch_.case_blocks,
@@ -8336,10 +8338,10 @@ static void vkd3d_dxbc_compiler_emit_sample(struct vkd3d_dxbc_compiler *compiler
 static void vkd3d_dxbc_compiler_emit_sample_c(struct vkd3d_dxbc_compiler *compiler,
         const struct vkd3d_shader_instruction *instruction)
 {
-    uint32_t sampled_type_id, coordinate_id, dref_id, val_id, type_id;
     struct vkd3d_spirv_builder *builder = &compiler->spirv_builder;
     const struct vkd3d_shader_dst_param *dst = instruction->dst;
     const struct vkd3d_shader_src_param *src = instruction->src;
+    uint32_t sampled_type_id, coordinate_id, dref_id, val_id;
     SpvImageOperandsMask operands_mask = 0;
     unsigned int image_operand_count = 0;
     struct vkd3d_shader_image image;
@@ -8371,10 +8373,6 @@ static void vkd3d_dxbc_compiler_emit_sample_c(struct vkd3d_dxbc_compiler *compil
     sampled_type_id = vkd3d_spirv_get_type_id(builder, image.sampled_type, 1);
     coordinate_id = vkd3d_dxbc_compiler_emit_load_src(compiler, &src[0], VKD3DSP_WRITEMASK_ALL);
     dref_id = vkd3d_dxbc_compiler_emit_load_src(compiler, &src[3], VKD3DSP_WRITEMASK_0);
-    /* XXX: Nvidia is broken and expects that the D_ref is packed together with coordinates. */
-    type_id = vkd3d_spirv_get_type_id(builder, VKD3D_SHADER_COMPONENT_FLOAT, VKD3D_VEC4_SIZE);
-    coordinate_id = vkd3d_spirv_build_op_composite_insert1(builder,
-            type_id, dref_id, coordinate_id, image.resource_type_info->coordinate_component_count);
     val_id = vkd3d_spirv_build_op_image_sample_dref(builder, op, sampled_type_id,
             image.sampled_image_id, coordinate_id, dref_id, operands_mask,
             image_operands, image_operand_count);
diff --git a/libs/vkd3d/libs/vkd3d-shader/trace.c b/libs/vkd3d/libs/vkd3d-shader/trace.c
index c292c8471..6c30edc9d 100644
--- a/libs/vkd3d/libs/vkd3d-shader/trace.c
+++ b/libs/vkd3d/libs/vkd3d-shader/trace.c
@@ -225,6 +225,7 @@ static const char * const shader_opcode_names[] =
     [VKD3DSIH_MOV                             ] = "mov",
     [VKD3DSIH_MOVA                            ] = "mova",
     [VKD3DSIH_MOVC                            ] = "movc",
+    [VKD3DSIH_MSAD                            ] = "msad",
     [VKD3DSIH_MUL                             ] = "mul",
     [VKD3DSIH_NE                              ] = "ne",
     [VKD3DSIH_NOP                             ] = "nop",
diff --git a/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_main.c b/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_main.c
index a5fc1e003..e25895b00 100644
--- a/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_main.c
+++ b/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_main.c
@@ -22,6 +22,38 @@
 #include <stdio.h>
 #include <math.h>
 
+static inline int char_to_int(char c)
+{
+    if ('0' <= c && c <= '9')
+        return c - '0';
+    if ('A' <= c && c <= 'F')
+        return c - 'A' + 10;
+    if ('a' <= c && c <= 'f')
+        return c - 'a' + 10;
+    return -1;
+}
+
+uint32_t vkd3d_parse_integer(const char *s)
+{
+    uint32_t base = 10, ret = 0;
+    int digit;
+
+    if (*s == '0')
+    {
+        base = 8;
+        ++s;
+        if (*s == 'x' || *s == 'X')
+        {
+            base = 16;
+            ++s;
+        }
+    }
+
+    while ((digit = char_to_int(*s++)) >= 0)
+        ret = ret * base + (uint32_t)digit;
+    return ret;
+}
+
 void vkd3d_string_buffer_init(struct vkd3d_string_buffer *buffer)
 {
     buffer->buffer_size = 16;
@@ -1516,3 +1548,8 @@ int vkd3d_shader_preprocess(const struct vkd3d_shader_compile_info *compile_info
     vkd3d_shader_message_context_cleanup(&message_context);
     return ret;
 }
+
+void vkd3d_shader_set_log_callback(PFN_vkd3d_log callback)
+{
+    vkd3d_dbg_set_log_callback(callback);
+}
diff --git a/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_private.h b/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_private.h
index 0f584d3fb..102427e49 100644
--- a/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_private.h
+++ b/libs/vkd3d/libs/vkd3d-shader/vkd3d_shader_private.h
@@ -117,8 +117,10 @@ enum vkd3d_shader_error
     VKD3D_SHADER_ERROR_HLSL_INVALID_TEXEL_OFFSET        = 5018,
     VKD3D_SHADER_ERROR_HLSL_OFFSET_OUT_OF_BOUNDS        = 5019,
     VKD3D_SHADER_ERROR_HLSL_INCOMPATIBLE_PROFILE        = 5020,
+    VKD3D_SHADER_ERROR_HLSL_DIVISION_BY_ZERO            = 5021,
 
     VKD3D_SHADER_WARNING_HLSL_IMPLICIT_TRUNCATION       = 5300,
+    VKD3D_SHADER_WARNING_HLSL_DIVISION_BY_ZERO          = 5301,
 
     VKD3D_SHADER_ERROR_GLSL_INTERNAL                    = 6000,
 
@@ -328,6 +330,7 @@ enum vkd3d_shader_opcode
     VKD3DSIH_MOV,
     VKD3DSIH_MOVA,
     VKD3DSIH_MOVC,
+    VKD3DSIH_MSAD,
     VKD3DSIH_MUL,
     VKD3DSIH_NE,
     VKD3DSIH_NOP,
@@ -1032,6 +1035,8 @@ static inline size_t bytecode_get_size(struct vkd3d_bytecode_buffer *buffer)
     return buffer->size;
 }
 
+uint32_t vkd3d_parse_integer(const char *s);
+
 struct vkd3d_shader_message_context
 {
     enum vkd3d_shader_log_level log_level;
diff --git a/libs/vkd3d/libs/vkd3d/command.c b/libs/vkd3d/libs/vkd3d/command.c
index e7375fb80..d0782e5a7 100644
--- a/libs/vkd3d/libs/vkd3d/command.c
+++ b/libs/vkd3d/libs/vkd3d/command.c
@@ -20,7 +20,13 @@
 
 #include "vkd3d_private.h"
 
-static HRESULT d3d12_fence_signal(struct d3d12_fence *fence, uint64_t value, VkFence vk_fence);
+static void d3d12_fence_incref(struct d3d12_fence *fence);
+static void d3d12_fence_decref(struct d3d12_fence *fence);
+static HRESULT d3d12_fence_signal(struct d3d12_fence *fence, uint64_t value, VkFence vk_fence, bool on_cpu);
+static void d3d12_fence_signal_timeline_semaphore(struct d3d12_fence *fence, uint64_t timeline_value);
+static HRESULT d3d12_command_queue_signal(struct d3d12_command_queue *command_queue,
+        struct d3d12_fence *fence, uint64_t value);
+static bool d3d12_command_queue_flush_ops(struct d3d12_command_queue *queue, bool *flushed_any);
 
 HRESULT vkd3d_queue_create(struct d3d12_device *device,
         uint32_t family_index, const VkQueueFamilyProperties *properties, struct vkd3d_queue **queue)
@@ -259,23 +265,21 @@ static HRESULT vkd3d_enqueue_gpu_fence(struct vkd3d_fence_worker *worker,
         return hresult_from_errno(rc);
     }
 
-    if (!vkd3d_array_reserve((void **)&worker->enqueued_fences, &worker->enqueued_fences_size,
-            worker->enqueued_fence_count + 1, sizeof(*worker->enqueued_fences)))
+    if (!vkd3d_array_reserve((void **)&worker->fences, &worker->fences_size,
+            worker->fence_count + 1, sizeof(*worker->fences)))
     {
         ERR("Failed to add GPU fence.\n");
         vkd3d_mutex_unlock(&worker->mutex);
         return E_OUTOFMEMORY;
     }
 
-    worker->enqueued_fences[worker->enqueued_fence_count].vk_fence = vk_fence;
-    waiting_fence = &worker->enqueued_fences[worker->enqueued_fence_count].waiting_fence;
+    waiting_fence = &worker->fences[worker->fence_count++];
     waiting_fence->fence = fence;
     waiting_fence->value = value;
-    waiting_fence->queue = queue;
+    waiting_fence->u.vk_fence = vk_fence;
     waiting_fence->queue_sequence_number = queue_sequence_number;
-    ++worker->enqueued_fence_count;
 
-    InterlockedIncrement(&fence->pending_worker_operation_count);
+    d3d12_fence_incref(fence);
 
     vkd3d_cond_signal(&worker->cond);
     vkd3d_mutex_unlock(&worker->mutex);
@@ -283,176 +287,113 @@ static HRESULT vkd3d_enqueue_gpu_fence(struct vkd3d_fence_worker *worker,
     return S_OK;
 }
 
-static void vkd3d_fence_worker_remove_fence(struct vkd3d_fence_worker *worker, struct d3d12_fence *fence)
+static void vkd3d_wait_for_gpu_timeline_semaphore(struct vkd3d_fence_worker *worker,
+        const struct vkd3d_waiting_fence *waiting_fence)
 {
-    LONG count;
-    int rc;
-
-    if (!(count = InterlockedAdd(&fence->pending_worker_operation_count, 0)))
-        return;
-
-    WARN("Waiting for %u pending fence operations (fence %p).\n", count, fence);
-
-    if ((rc = vkd3d_mutex_lock(&worker->mutex)))
-    {
-        ERR("Failed to lock mutex, error %d.\n", rc);
-        return;
-    }
-
-    while ((count = InterlockedAdd(&fence->pending_worker_operation_count, 0)))
-    {
-        TRACE("Still waiting for %u pending fence operations (fence %p).\n", count, fence);
-
-        worker->pending_fence_destruction = true;
-        vkd3d_cond_signal(&worker->cond);
-
-        vkd3d_cond_wait(&worker->fence_destruction_cond, &worker->mutex);
-    }
-
-    TRACE("Removed fence %p.\n", fence);
-
-    vkd3d_mutex_unlock(&worker->mutex);
-}
+    const struct d3d12_device *device = worker->device;
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkSemaphoreWaitInfoKHR wait_info;
+    VkResult vr;
 
-static void vkd3d_fence_worker_move_enqueued_fences_locked(struct vkd3d_fence_worker *worker)
-{
-    unsigned int i;
-    size_t count;
-    bool ret;
+    wait_info.sType = VK_STRUCTURE_TYPE_SEMAPHORE_WAIT_INFO_KHR;
+    wait_info.pNext = NULL;
+    wait_info.flags = 0;
+    wait_info.semaphoreCount = 1;
+    wait_info.pSemaphores = &waiting_fence->u.vk_semaphore;
+    wait_info.pValues = &waiting_fence->value;
 
-    if (!worker->enqueued_fence_count)
+    vr = VK_CALL(vkWaitSemaphoresKHR(device->vk_device, &wait_info, ~(uint64_t)0));
+    if (vr == VK_TIMEOUT)
         return;
-
-    count = worker->fence_count + worker->enqueued_fence_count;
-
-    ret = vkd3d_array_reserve((void **)&worker->vk_fences, &worker->vk_fences_size,
-            count, sizeof(*worker->vk_fences));
-    ret &= vkd3d_array_reserve((void **)&worker->fences, &worker->fences_size,
-            count, sizeof(*worker->fences));
-    if (!ret)
+    if (vr != VK_SUCCESS)
     {
-        ERR("Failed to reserve memory.\n");
+        ERR("Failed to wait for Vulkan timeline semaphore, vr %d.\n", vr);
         return;
     }
 
-    for (i = 0; i < worker->enqueued_fence_count; ++i)
-    {
-        struct vkd3d_enqueued_fence *current = &worker->enqueued_fences[i];
+    TRACE("Signaling fence %p value %#"PRIx64".\n", waiting_fence->fence, waiting_fence->value);
+    d3d12_fence_signal_timeline_semaphore(waiting_fence->fence, waiting_fence->value);
 
-        worker->vk_fences[worker->fence_count] = current->vk_fence;
-        worker->fences[worker->fence_count] = current->waiting_fence;
-        ++worker->fence_count;
-    }
-    assert(worker->fence_count == count);
-    worker->enqueued_fence_count = 0;
+    d3d12_fence_decref(waiting_fence->fence);
 }
 
-static void vkd3d_wait_for_gpu_fences(struct vkd3d_fence_worker *worker)
+static void vkd3d_wait_for_gpu_fence(struct vkd3d_fence_worker *worker,
+        const struct vkd3d_waiting_fence *waiting_fence)
 {
     struct d3d12_device *device = worker->device;
     const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
-    unsigned int i, j;
-    VkFence vk_fence;
     HRESULT hr;
     int vr;
 
-    if (!worker->fence_count)
-        return;
-
-    vr = VK_CALL(vkWaitForFences(device->vk_device,
-            worker->fence_count, worker->vk_fences, VK_FALSE, ~(uint64_t)0));
+    vr = VK_CALL(vkWaitForFences(device->vk_device, 1, &waiting_fence->u.vk_fence, VK_FALSE, ~(uint64_t)0));
     if (vr == VK_TIMEOUT)
         return;
     if (vr != VK_SUCCESS)
     {
-        ERR("Failed to wait for Vulkan fences, vr %d.\n", vr);
+        ERR("Failed to wait for Vulkan fence, vr %d.\n", vr);
         return;
     }
 
-    for (i = 0, j = 0; i < worker->fence_count; ++i)
-    {
-        vk_fence = worker->vk_fences[i];
-        if (!(vr = VK_CALL(vkGetFenceStatus(device->vk_device, vk_fence))))
-        {
-            struct vkd3d_waiting_fence *current = &worker->fences[i];
-
-            TRACE("Signaling fence %p value %#"PRIx64".\n", current->fence, current->value);
-            if (FAILED(hr = d3d12_fence_signal(current->fence, current->value, vk_fence)))
-                ERR("Failed to signal D3D12 fence, hr %#x.\n", hr);
-
-            InterlockedDecrement(&current->fence->pending_worker_operation_count);
+    TRACE("Signaling fence %p value %#"PRIx64".\n", waiting_fence->fence, waiting_fence->value);
+    if (FAILED(hr = d3d12_fence_signal(waiting_fence->fence, waiting_fence->value, waiting_fence->u.vk_fence, false)))
+        ERR("Failed to signal D3D12 fence, hr %#x.\n", hr);
 
-            vkd3d_queue_update_sequence_number(current->queue, current->queue_sequence_number, device);
-            continue;
-        }
-
-        if (vr != VK_NOT_READY)
-            ERR("Failed to get Vulkan fence status, vr %d.\n", vr);
+    d3d12_fence_decref(waiting_fence->fence);
 
-        if (i != j)
-        {
-            worker->vk_fences[j] = worker->vk_fences[i];
-            worker->fences[j] = worker->fences[i];
-        }
-        ++j;
-    }
-    worker->fence_count = j;
+    vkd3d_queue_update_sequence_number(worker->queue, waiting_fence->queue_sequence_number, device);
 }
 
 static void *vkd3d_fence_worker_main(void *arg)
 {
+    size_t old_fences_size, cur_fences_size = 0, cur_fence_count = 0;
+    struct vkd3d_waiting_fence *old_fences, *cur_fences = NULL;
     struct vkd3d_fence_worker *worker = arg;
+    unsigned int i;
     int rc;
 
     vkd3d_set_thread_name("vkd3d_fence");
 
     for (;;)
     {
-        vkd3d_wait_for_gpu_fences(worker);
+        if ((rc = vkd3d_mutex_lock(&worker->mutex)))
+        {
+            ERR("Failed to lock mutex, error %d.\n", rc);
+            break;
+        }
 
-        if (!worker->fence_count || InterlockedAdd(&worker->enqueued_fence_count, 0))
+        if (!worker->fence_count && !worker->should_exit && (rc = vkd3d_cond_wait(&worker->cond, &worker->mutex)))
         {
-            if ((rc = vkd3d_mutex_lock(&worker->mutex)))
-            {
-                ERR("Failed to lock mutex, error %d.\n", rc);
-                break;
-            }
+            ERR("Failed to wait on condition variable, error %d.\n", rc);
+            vkd3d_mutex_unlock(&worker->mutex);
+            break;
+        }
 
-            if (worker->pending_fence_destruction)
-            {
-                vkd3d_cond_broadcast(&worker->fence_destruction_cond);
-                worker->pending_fence_destruction = false;
-            }
+        if (worker->should_exit)
+            break;
 
-            if (worker->enqueued_fence_count)
-            {
-                vkd3d_fence_worker_move_enqueued_fences_locked(worker);
-            }
-            else
-            {
-                if (worker->should_exit)
-                {
-                    vkd3d_mutex_unlock(&worker->mutex);
-                    break;
-                }
+        old_fences_size = cur_fences_size;
+        old_fences = cur_fences;
 
-                if ((rc = vkd3d_cond_wait(&worker->cond, &worker->mutex)))
-                {
-                    ERR("Failed to wait on condition variable, error %d.\n", rc);
-                    vkd3d_mutex_unlock(&worker->mutex);
-                    break;
-                }
-            }
+        cur_fence_count = worker->fence_count;
+        cur_fences_size = worker->fences_size;
+        cur_fences = worker->fences;
 
-            vkd3d_mutex_unlock(&worker->mutex);
-        }
+        worker->fence_count = 0;
+        worker->fences_size = old_fences_size;
+        worker->fences = old_fences;
+
+        vkd3d_mutex_unlock(&worker->mutex);
+
+        for (i = 0; i < cur_fence_count; ++i)
+            worker->wait_for_gpu_fence(worker, &cur_fences[i]);
     }
 
+    vkd3d_free(cur_fences);
     return NULL;
 }
 
-HRESULT vkd3d_fence_worker_start(struct vkd3d_fence_worker *worker,
-        struct d3d12_device *device)
+static HRESULT vkd3d_fence_worker_start(struct vkd3d_fence_worker *worker,
+        struct vkd3d_queue *queue, struct d3d12_device *device)
 {
     HRESULT hr;
     int rc;
@@ -460,20 +401,16 @@ HRESULT vkd3d_fence_worker_start(struct vkd3d_fence_worker *worker,
     TRACE("worker %p.\n", worker);
 
     worker->should_exit = false;
-    worker->pending_fence_destruction = false;
+    worker->queue = queue;
     worker->device = device;
 
-    worker->enqueued_fence_count = 0;
-    worker->enqueued_fences = NULL;
-    worker->enqueued_fences_size = 0;
-
     worker->fence_count = 0;
-
-    worker->vk_fences = NULL;
-    worker->vk_fences_size = 0;
     worker->fences = NULL;
     worker->fences_size = 0;
 
+    worker->wait_for_gpu_fence = device->vk_info.KHR_timeline_semaphore
+            ? vkd3d_wait_for_gpu_timeline_semaphore : vkd3d_wait_for_gpu_fence;
+
     if ((rc = vkd3d_mutex_init(&worker->mutex)))
     {
         ERR("Failed to initialize mutex, error %d.\n", rc);
@@ -506,7 +443,7 @@ HRESULT vkd3d_fence_worker_start(struct vkd3d_fence_worker *worker,
     return hr;
 }
 
-HRESULT vkd3d_fence_worker_stop(struct vkd3d_fence_worker *worker,
+static HRESULT vkd3d_fence_worker_stop(struct vkd3d_fence_worker *worker,
         struct d3d12_device *device)
 {
     HRESULT hr;
@@ -532,8 +469,6 @@ HRESULT vkd3d_fence_worker_stop(struct vkd3d_fence_worker *worker,
     vkd3d_cond_destroy(&worker->cond);
     vkd3d_cond_destroy(&worker->fence_destruction_cond);
 
-    vkd3d_free(worker->enqueued_fences);
-    vkd3d_free(worker->vk_fences);
     vkd3d_free(worker->fences);
 
     return S_OK;
@@ -629,32 +564,34 @@ static void d3d12_fence_garbage_collect_vk_semaphores_locked(struct d3d12_fence
 {
     struct d3d12_device *device = fence->device;
     const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
-    struct vkd3d_signaled_semaphore *current, *p;
-    unsigned int semaphore_count;
+    struct vkd3d_signaled_semaphore *current;
+    unsigned int i, semaphore_count;
 
     semaphore_count = fence->semaphore_count;
     if (!destroy_all && semaphore_count < VKD3D_MAX_VK_SYNC_OBJECTS)
         return;
 
-    LIST_FOR_EACH_ENTRY_SAFE(current, p, &fence->semaphores, struct vkd3d_signaled_semaphore, entry)
+    i = 0;
+    while (i < fence->semaphore_count)
     {
         if (!destroy_all && fence->semaphore_count < VKD3D_MAX_VK_SYNC_OBJECTS)
             break;
 
+        current = &fence->semaphores[i];
         /* The semaphore doesn't have a pending signal operation if the fence
          * was signaled. */
-        if ((current->vk_fence || current->is_acquired) && !destroy_all)
+        if ((current->u.binary.vk_fence || current->u.binary.is_acquired) && !destroy_all)
+        {
+            ++i;
             continue;
+        }
 
-        if (current->vk_fence)
+        if (current->u.binary.vk_fence)
             WARN("Destroying potentially pending semaphore.\n");
-        assert(!current->is_acquired);
+        assert(!current->u.binary.is_acquired);
 
-        VK_CALL(vkDestroySemaphore(device->vk_device, current->vk_semaphore, NULL));
-        list_remove(&current->entry);
-        vkd3d_free(current);
-
-        --fence->semaphore_count;
+        VK_CALL(vkDestroySemaphore(device->vk_device, current->u.binary.vk_semaphore, NULL));
+        fence->semaphores[i] = fence->semaphores[--fence->semaphore_count];
     }
 
     if (semaphore_count != fence->semaphore_count)
@@ -684,33 +621,29 @@ static void d3d12_fence_destroy_vk_objects(struct d3d12_fence *fence)
     }
 
     d3d12_fence_garbage_collect_vk_semaphores_locked(fence, true);
+    VK_CALL(vkDestroySemaphore(device->vk_device, fence->timeline_semaphore, NULL));
 
     vkd3d_mutex_unlock(&fence->mutex);
 }
 
-static struct vkd3d_signaled_semaphore *d3d12_fence_acquire_vk_semaphore(struct d3d12_fence *fence,
+static struct vkd3d_signaled_semaphore *d3d12_fence_acquire_vk_semaphore_locked(struct d3d12_fence *fence,
         uint64_t value, uint64_t *completed_value)
 {
     struct vkd3d_signaled_semaphore *semaphore;
     struct vkd3d_signaled_semaphore *current;
     uint64_t semaphore_value;
-    int rc;
+    unsigned int i;
 
     TRACE("fence %p, value %#"PRIx64".\n", fence, value);
 
-    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
-    {
-        ERR("Failed to lock mutex, error %d.\n", rc);
-        return VK_NULL_HANDLE;
-    }
-
     semaphore = NULL;
     semaphore_value = ~(uint64_t)0;
 
-    LIST_FOR_EACH_ENTRY(current, &fence->semaphores, struct vkd3d_signaled_semaphore, entry)
+    for (i = 0; i < fence->semaphore_count; ++i)
     {
+        current = &fence->semaphores[i];
         /* Prefer a semaphore with the smallest value. */
-        if (!current->is_acquired && current->value >= value && semaphore_value >= current->value)
+        if (!current->u.binary.is_acquired && current->value >= value && semaphore_value >= current->value)
         {
             semaphore = current;
             semaphore_value = current->value;
@@ -720,12 +653,10 @@ static struct vkd3d_signaled_semaphore *d3d12_fence_acquire_vk_semaphore(struct
     }
 
     if (semaphore)
-        semaphore->is_acquired = true;
+        semaphore->u.binary.is_acquired = true;
 
     *completed_value = fence->value;
 
-    vkd3d_mutex_unlock(&fence->mutex);
-
     return semaphore;
 }
 
@@ -739,12 +670,9 @@ static void d3d12_fence_remove_vk_semaphore(struct d3d12_fence *fence, struct vk
         return;
     }
 
-    assert(semaphore->is_acquired);
-
-    list_remove(&semaphore->entry);
-    vkd3d_free(semaphore);
+    assert(semaphore->u.binary.is_acquired);
 
-    --fence->semaphore_count;
+    *semaphore = fence->semaphores[--fence->semaphore_count];
 
     vkd3d_mutex_unlock(&fence->mutex);
 }
@@ -759,74 +687,174 @@ static void d3d12_fence_release_vk_semaphore(struct d3d12_fence *fence, struct v
         return;
     }
 
-    assert(semaphore->is_acquired);
-    semaphore->is_acquired = false;
+    assert(semaphore->u.binary.is_acquired);
+    semaphore->u.binary.is_acquired = false;
 
     vkd3d_mutex_unlock(&fence->mutex);
 }
 
-static HRESULT d3d12_fence_add_vk_semaphore(struct d3d12_fence *fence,
-        VkSemaphore vk_semaphore, VkFence vk_fence, uint64_t value)
+static void d3d12_fence_update_pending_value_locked(struct d3d12_fence *fence)
+{
+    uint64_t new_max_pending_value;
+    unsigned int i;
+
+    for (i = 0, new_max_pending_value = 0; i < fence->semaphore_count; ++i)
+        new_max_pending_value = max(fence->semaphores[i].value, new_max_pending_value);
+
+    fence->max_pending_value = max(fence->value, new_max_pending_value);
+}
+
+static HRESULT d3d12_fence_update_pending_value(struct d3d12_fence *fence)
+{
+    int rc;
+
+    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+
+    d3d12_fence_update_pending_value_locked(fence);
+
+    vkd3d_mutex_unlock(&fence->mutex);
+
+    return S_OK;
+}
+
+static HRESULT d3d12_device_add_blocked_command_queues(struct d3d12_device *device,
+        struct d3d12_command_queue * const *command_queues, unsigned int count)
 {
-    struct vkd3d_signaled_semaphore *semaphore;
     HRESULT hr = S_OK;
+    unsigned int i;
     int rc;
 
-    TRACE("fence %p, value %#"PRIx64".\n", fence, value);
+    if ((rc = vkd3d_mutex_lock(&device->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
 
-    if (!(semaphore = vkd3d_malloc(sizeof(*semaphore))))
+    if ((i = ARRAY_SIZE(device->blocked_queues) - device->blocked_queue_count) < count)
     {
-        ERR("Failed to add semaphore.\n");
-        return E_OUTOFMEMORY;
+        FIXME("Failed to add %u blocked command queue(s) to device %p.\n", count - i, device);
+        count = i;
+        hr = E_FAIL;
     }
 
-    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    for (i = 0; i < count; ++i)
+        device->blocked_queues[device->blocked_queue_count++] = command_queues[i];
+
+    vkd3d_mutex_unlock(&device->mutex);
+    return hr;
+}
+
+static HRESULT d3d12_device_flush_blocked_queues_once(struct d3d12_device *device, bool *flushed_any)
+{
+    struct d3d12_command_queue *blocked_queues[VKD3D_MAX_DEVICE_BLOCKED_QUEUES];
+    unsigned int i, blocked_queue_count;
+    int rc;
+
+    *flushed_any = false;
+
+    if ((rc = vkd3d_mutex_lock(&device->mutex)))
     {
         ERR("Failed to lock mutex, error %d.\n", rc);
-        vkd3d_free(semaphore);
-        return E_FAIL;
+        return hresult_from_errno(rc);
     }
 
-    d3d12_fence_garbage_collect_vk_semaphores_locked(fence, false);
+    /* Flush any ops unblocked by a new pending value. These cannot be flushed
+     * with the device locked, so move the queue pointers to a local array. */
+    blocked_queue_count = device->blocked_queue_count;
+    memcpy(blocked_queues, device->blocked_queues, blocked_queue_count * sizeof(blocked_queues[0]));
+    device->blocked_queue_count = 0;
 
-    semaphore->value = value;
-    semaphore->vk_semaphore = vk_semaphore;
-    semaphore->vk_fence = vk_fence;
-    semaphore->is_acquired = false;
+    vkd3d_mutex_unlock(&device->mutex);
 
-    list_add_tail(&fence->semaphores, &semaphore->entry);
-    ++fence->semaphore_count;
+    i = 0;
+    while (i < blocked_queue_count)
+    {
+        if (d3d12_command_queue_flush_ops(blocked_queues[i], flushed_any))
+            blocked_queues[i] = blocked_queues[--blocked_queue_count];
+        else
+            ++i;
+    }
 
-    vkd3d_mutex_unlock(&fence->mutex);
+    /* None of these queues could have been re-added during the above loop because
+     * blocked queues always have a nonzero op count. */
+    return d3d12_device_add_blocked_command_queues(device, blocked_queues, blocked_queue_count);
+}
 
-    return hr;
+static HRESULT d3d12_device_flush_blocked_queues(struct d3d12_device *device)
+{
+    bool flushed_any;
+    HRESULT hr;
+
+    /* Executing an op on one queue may unblock another, so repeat until nothing is flushed. */
+    do
+    {
+        if (!device->blocked_queue_count)
+            return S_OK;
+        if (FAILED(hr = d3d12_device_flush_blocked_queues_once(device, &flushed_any)))
+            return hr;
+    }
+    while (flushed_any);
+
+    return S_OK;
 }
 
-static HRESULT d3d12_fence_signal(struct d3d12_fence *fence, uint64_t value, VkFence vk_fence)
+static HRESULT d3d12_fence_add_vk_semaphore(struct d3d12_fence *fence, VkSemaphore vk_semaphore,
+        VkFence vk_fence, uint64_t value, const struct vkd3d_queue *signalling_queue)
 {
-    struct d3d12_device *device = fence->device;
-    struct vkd3d_signaled_semaphore *current;
-    bool signal_null_event_cond = false;
-    unsigned int i, j;
+    struct vkd3d_signaled_semaphore *semaphore;
     int rc;
 
+    TRACE("fence %p, value %#"PRIx64".\n", fence, value);
+
     if ((rc = vkd3d_mutex_lock(&fence->mutex)))
     {
         ERR("Failed to lock mutex, error %d.\n", rc);
         return hresult_from_errno(rc);
     }
 
-    fence->value = value;
+    d3d12_fence_garbage_collect_vk_semaphores_locked(fence, false);
+
+    if (!vkd3d_array_reserve((void**)&fence->semaphores, &fence->semaphores_size,
+            fence->semaphore_count + 1, sizeof(*fence->semaphores)))
+    {
+        ERR("Failed to add semaphore.\n");
+        vkd3d_mutex_unlock(&fence->mutex);
+        return E_OUTOFMEMORY;
+    }
+
+    semaphore = &fence->semaphores[fence->semaphore_count++];
+    semaphore->value = value;
+    semaphore->u.binary.vk_semaphore = vk_semaphore;
+    semaphore->u.binary.vk_fence = vk_fence;
+    semaphore->u.binary.is_acquired = false;
+    semaphore->signalling_queue = signalling_queue;
+
+    d3d12_fence_update_pending_value_locked(fence);
+
+    vkd3d_mutex_unlock(&fence->mutex);
+
+    return d3d12_device_flush_blocked_queues(fence->device);
+}
+
+static void d3d12_fence_signal_external_events_locked(struct d3d12_fence *fence)
+{
+    struct d3d12_device *device = fence->device;
+    bool signal_null_event_cond = false;
+    unsigned int i, j;
 
     for (i = 0, j = 0; i < fence->event_count; ++i)
     {
         struct vkd3d_waiting_event *current = &fence->events[i];
 
-        if (current->value <= value)
+        if (current->value <= fence->value)
         {
             if (current->event)
             {
-                fence->device->signal_event(current->event);
+                device->signal_event(current->event);
             }
             else
             {
@@ -841,19 +869,39 @@ static HRESULT d3d12_fence_signal(struct d3d12_fence *fence, uint64_t value, VkF
             ++j;
         }
     }
+
     fence->event_count = j;
 
     if (signal_null_event_cond)
         vkd3d_cond_broadcast(&fence->null_event_cond);
+}
+
+static HRESULT d3d12_fence_signal(struct d3d12_fence *fence, uint64_t value, VkFence vk_fence, bool on_cpu)
+{
+    struct d3d12_device *device = fence->device;
+    struct vkd3d_signaled_semaphore *current;
+    unsigned int i;
+    int rc;
+
+    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+
+    fence->value = value;
+
+    d3d12_fence_signal_external_events_locked(fence);
 
     if (vk_fence)
     {
         const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
 
-        LIST_FOR_EACH_ENTRY(current, &fence->semaphores, struct vkd3d_signaled_semaphore, entry)
+        for (i = 0; i < fence->semaphore_count; ++i)
         {
-            if (current->vk_fence == vk_fence)
-                current->vk_fence = VK_NULL_HANDLE;
+            current = &fence->semaphores[i];
+            if (current->u.binary.vk_fence == vk_fence)
+                current->u.binary.vk_fence = VK_NULL_HANDLE;
         }
 
         for (i = 0; i < ARRAY_SIZE(fence->old_vk_fences); ++i)
@@ -870,9 +918,101 @@ static HRESULT d3d12_fence_signal(struct d3d12_fence *fence, uint64_t value, VkF
             VK_CALL(vkDestroyFence(device->vk_device, vk_fence, NULL));
     }
 
+    d3d12_fence_update_pending_value_locked(fence);
+
     vkd3d_mutex_unlock(&fence->mutex);
 
-    return S_OK;
+    return on_cpu ? d3d12_device_flush_blocked_queues(device) : S_OK;
+}
+
+static uint64_t d3d12_fence_add_pending_timeline_signal(struct d3d12_fence *fence, uint64_t virtual_value,
+        const struct vkd3d_queue *signalling_queue)
+{
+    struct vkd3d_signaled_semaphore *semaphore;
+    int rc;
+
+    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+
+    if (!vkd3d_array_reserve((void **)&fence->semaphores, &fence->semaphores_size,
+            fence->semaphore_count + 1, sizeof(*fence->semaphores)))
+    {
+        return 0;
+    }
+
+    semaphore = &fence->semaphores[fence->semaphore_count++];
+    semaphore->value = virtual_value;
+    semaphore->u.timeline_value = ++fence->pending_timeline_value;
+    semaphore->signalling_queue = signalling_queue;
+
+    vkd3d_mutex_unlock(&fence->mutex);
+
+    return fence->pending_timeline_value;
+}
+
+static uint64_t d3d12_fence_get_timeline_wait_value_locked(struct d3d12_fence *fence, uint64_t virtual_value)
+{
+    uint64_t target_timeline_value = UINT64_MAX;
+    unsigned int i;
+
+    /* Find the smallest physical value which is at least the virtual value. */
+    for (i = 0; i < fence->semaphore_count; ++i)
+    {
+        if (virtual_value <= fence->semaphores[i].value)
+            target_timeline_value = min(target_timeline_value, fence->semaphores[i].u.timeline_value);
+    }
+
+    /* No timeline value will be found if it was already signaled on the GPU and handled in
+     * the worker thread. A wait must still be emitted as a barrier against command re-ordering. */
+    return (target_timeline_value == UINT64_MAX) ? 0 : target_timeline_value;
+}
+
+static void d3d12_fence_signal_timeline_semaphore(struct d3d12_fence *fence, uint64_t timeline_value)
+{
+    bool did_signal;
+    unsigned int i;
+    int rc;
+
+    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return;
+    }
+
+    /* With multiple fence workers, it is possible that signal calls are out of
+     * order. The physical value itself is monotonic, but we need to make sure
+     * that all signals happen in correct order if there are fence rewinds.
+     * We don't expect the loop to run more than once, but there might be
+     * extreme edge cases where we signal 2 or more. */
+    while (fence->timeline_value < timeline_value)
+    {
+        ++fence->timeline_value;
+        did_signal = false;
+
+        for (i = 0; i < fence->semaphore_count; ++i)
+        {
+            if (fence->timeline_value == fence->semaphores[i].u.timeline_value)
+            {
+                fence->value = fence->semaphores[i].value;
+                d3d12_fence_signal_external_events_locked(fence);
+                fence->semaphores[i] = fence->semaphores[--fence->semaphore_count];
+                did_signal = true;
+                break;
+            }
+        }
+
+        if (!did_signal)
+            FIXME("Did not signal a virtual value.\n");
+    }
+
+    /* If a rewind remains queued, the virtual value deleted above may be
+     * greater than any pending value, so update the max pending value. */
+    d3d12_fence_update_pending_value_locked(fence);
+
+    vkd3d_mutex_unlock(&fence->mutex);
 }
 
 static HRESULT STDMETHODCALLTYPE d3d12_fence_QueryInterface(ID3D12Fence *iface,
@@ -907,25 +1047,39 @@ static ULONG STDMETHODCALLTYPE d3d12_fence_AddRef(ID3D12Fence *iface)
     return refcount;
 }
 
+static void d3d12_fence_incref(struct d3d12_fence *fence)
+{
+    InterlockedIncrement(&fence->internal_refcount);
+}
+
 static ULONG STDMETHODCALLTYPE d3d12_fence_Release(ID3D12Fence *iface)
 {
     struct d3d12_fence *fence = impl_from_ID3D12Fence(iface);
     ULONG refcount = InterlockedDecrement(&fence->refcount);
-    int rc;
 
     TRACE("%p decreasing refcount to %u.\n", fence, refcount);
 
     if (!refcount)
+        d3d12_fence_decref(fence);
+
+    return refcount;
+}
+
+static void d3d12_fence_decref(struct d3d12_fence *fence)
+{
+    ULONG internal_refcount = InterlockedDecrement(&fence->internal_refcount);
+    int rc;
+
+    if (!internal_refcount)
     {
         struct d3d12_device *device = fence->device;
 
         vkd3d_private_store_destroy(&fence->private_store);
 
-        vkd3d_fence_worker_remove_fence(&device->fence_worker, fence);
-
         d3d12_fence_destroy_vk_objects(fence);
 
         vkd3d_free(fence->events);
+        vkd3d_free(fence->semaphores);
         if ((rc = vkd3d_mutex_destroy(&fence->mutex)))
             ERR("Failed to destroy mutex, error %d.\n", rc);
         vkd3d_cond_destroy(&fence->null_event_cond);
@@ -933,8 +1087,6 @@ static ULONG STDMETHODCALLTYPE d3d12_fence_Release(ID3D12Fence *iface)
 
         d3d12_device_release(device);
     }
-
-    return refcount;
 }
 
 static HRESULT STDMETHODCALLTYPE d3d12_fence_GetPrivateData(ID3D12Fence *iface,
@@ -1069,13 +1221,34 @@ static HRESULT STDMETHODCALLTYPE d3d12_fence_SetEventOnCompletion(ID3D12Fence *i
     return S_OK;
 }
 
+static HRESULT d3d12_fence_signal_cpu_timeline_semaphore(struct d3d12_fence *fence, uint64_t value)
+{
+    int rc;
+
+    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+
+    fence->value = value;
+    d3d12_fence_signal_external_events_locked(fence);
+    d3d12_fence_update_pending_value_locked(fence);
+
+    vkd3d_mutex_unlock(&fence->mutex);
+
+    return d3d12_device_flush_blocked_queues(fence->device);
+}
+
 static HRESULT STDMETHODCALLTYPE d3d12_fence_Signal(ID3D12Fence *iface, UINT64 value)
 {
     struct d3d12_fence *fence = impl_from_ID3D12Fence(iface);
 
     TRACE("iface %p, value %#"PRIx64".\n", iface, value);
 
-    return d3d12_fence_signal(fence, value, VK_NULL_HANDLE);
+    if (fence->timeline_semaphore)
+        return d3d12_fence_signal_cpu_timeline_semaphore(fence, value);
+    return d3d12_fence_signal(fence, value, VK_NULL_HANDLE, true);
 }
 
 static const struct ID3D12FenceVtbl d3d12_fence_vtbl =
@@ -1108,13 +1281,17 @@ static struct d3d12_fence *unsafe_impl_from_ID3D12Fence(ID3D12Fence *iface)
 static HRESULT d3d12_fence_init(struct d3d12_fence *fence, struct d3d12_device *device,
         UINT64 initial_value, D3D12_FENCE_FLAGS flags)
 {
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkResult vr;
     HRESULT hr;
     int rc;
 
     fence->ID3D12Fence_iface.lpVtbl = &d3d12_fence_vtbl;
+    fence->internal_refcount = 1;
     fence->refcount = 1;
 
     fence->value = initial_value;
+    fence->max_pending_value = initial_value;
 
     if ((rc = vkd3d_mutex_init(&fence->mutex)))
     {
@@ -1125,8 +1302,8 @@ static HRESULT d3d12_fence_init(struct d3d12_fence *fence, struct d3d12_device *
     if ((rc = vkd3d_cond_init(&fence->null_event_cond)))
     {
         ERR("Failed to initialize cond variable, error %d.\n", rc);
-        vkd3d_mutex_destroy(&fence->mutex);
-        return hresult_from_errno(rc);
+        hr = hresult_from_errno(rc);
+        goto fail_destroy_mutex;
     }
 
     if (flags)
@@ -1136,23 +1313,40 @@ static HRESULT d3d12_fence_init(struct d3d12_fence *fence, struct d3d12_device *
     fence->events_size = 0;
     fence->event_count = 0;
 
-    list_init(&fence->semaphores);
+    fence->timeline_semaphore = VK_NULL_HANDLE;
+    fence->timeline_value = 0;
+    fence->pending_timeline_value = 0;
+    if (device->vk_info.KHR_timeline_semaphore && (vr = vkd3d_create_timeline_semaphore(device, 0,
+            &fence->timeline_semaphore)) < 0)
+    {
+        WARN("Failed to create timeline semaphore, vr %d.\n", vr);
+        hr = hresult_from_vk_result(vr);
+        goto fail_destroy_null_cond;
+    }
+
+    fence->semaphores = NULL;
+    fence->semaphores_size = 0;
     fence->semaphore_count = 0;
 
     memset(fence->old_vk_fences, 0, sizeof(fence->old_vk_fences));
 
-    fence->pending_worker_operation_count = 0;
-
     if (FAILED(hr = vkd3d_private_store_init(&fence->private_store)))
     {
-        vkd3d_mutex_destroy(&fence->mutex);
-        vkd3d_cond_destroy(&fence->null_event_cond);
-        return hr;
+        goto fail_destroy_timeline_semaphore;
     }
 
     d3d12_device_add_ref(fence->device = device);
 
     return S_OK;
+
+fail_destroy_timeline_semaphore:
+    VK_CALL(vkDestroySemaphore(device->vk_device, fence->timeline_semaphore, NULL));
+fail_destroy_null_cond:
+    vkd3d_cond_destroy(&fence->null_event_cond);
+fail_destroy_mutex:
+    vkd3d_mutex_destroy(&fence->mutex);
+
+    return hr;
 }
 
 HRESULT d3d12_fence_create(struct d3d12_device *device,
@@ -1172,6 +1366,25 @@ HRESULT d3d12_fence_create(struct d3d12_device *device,
     return S_OK;
 }
 
+VkResult vkd3d_create_timeline_semaphore(const struct d3d12_device *device, uint64_t initial_value,
+        VkSemaphore *timeline_semaphore)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkSemaphoreTypeCreateInfoKHR type_info;
+    VkSemaphoreCreateInfo info;
+
+    info.sType = VK_STRUCTURE_TYPE_SEMAPHORE_CREATE_INFO;
+    info.pNext = &type_info;
+    info.flags = 0;
+
+    type_info.sType = VK_STRUCTURE_TYPE_SEMAPHORE_TYPE_CREATE_INFO_KHR;
+    type_info.pNext = NULL;
+    type_info.semaphoreType = VK_SEMAPHORE_TYPE_TIMELINE_KHR;
+    type_info.initialValue = initial_value;
+
+    return VK_CALL(vkCreateSemaphore(device->vk_device, &info, NULL, timeline_semaphore));
+}
+
 /* Command buffers */
 static void d3d12_command_list_mark_as_invalid(struct d3d12_command_list *list,
         const char *message, ...)
@@ -1921,6 +2134,8 @@ static void d3d12_command_list_invalidate_root_parameters(struct d3d12_command_l
     bindings->descriptor_set_count = 0;
     bindings->descriptor_table_dirty_mask = bindings->descriptor_table_active_mask & bindings->root_signature->descriptor_table_mask;
     bindings->push_descriptor_dirty_mask = bindings->push_descriptor_active_mask & bindings->root_signature->push_descriptor_mask;
+    bindings->cbv_srv_uav_heap_id = 0;
+    bindings->sampler_heap_id = 0;
 }
 
 static bool vk_barrier_parameters_from_d3d12_resource_state(unsigned int state, unsigned int stencil_state,
@@ -2653,7 +2868,7 @@ static bool vk_write_descriptor_set_from_d3d12_desc(VkWriteDescriptorSet *vk_des
         unsigned int index, bool use_array)
 {
     uint32_t descriptor_range_magic = range->descriptor_magic;
-    const struct vkd3d_view *view = descriptor->u.view;
+    const struct vkd3d_view *view = descriptor->u.view_info.view;
     uint32_t vk_binding = range->binding;
     uint32_t set = range->set;
 
@@ -2791,7 +3006,7 @@ static void d3d12_command_list_update_descriptor_table(struct d3d12_command_list
                             && state->uav_counters.bindings[k].register_index == register_idx)
                     {
                         VkBufferView vk_counter_view = descriptor->magic == VKD3D_DESCRIPTOR_MAGIC_UAV
-                                ? descriptor->u.view->vk_counter_view : VK_NULL_HANDLE;
+                                ? descriptor->u.view_info.view->vk_counter_view : VK_NULL_HANDLE;
                         if (bindings->vk_uav_counter_views[k] != vk_counter_view)
                             bindings->uav_counters_dirty = true;
                         bindings->vk_uav_counter_views[k] = vk_counter_view;
@@ -3021,6 +3236,146 @@ static void d3d12_command_list_update_descriptors(struct d3d12_command_list *lis
     d3d12_command_list_update_uav_counter_descriptors(list, bind_point);
 }
 
+static unsigned int d3d12_command_list_bind_descriptor_table(struct d3d12_command_list *list,
+        struct vkd3d_pipeline_bindings *bindings, unsigned int index,
+        struct d3d12_descriptor_heap **cbv_srv_uav_heap, struct d3d12_descriptor_heap **sampler_heap)
+{
+    struct d3d12_descriptor_heap *heap;
+    const struct d3d12_desc *desc;
+    unsigned int offset;
+
+    if (!(desc = bindings->descriptor_tables[index]))
+        return 0;
+
+    /* AMD, Nvidia and Intel drivers on Windows work if SetDescriptorHeaps()
+     * is not called, so we bind heaps from the tables instead. No NULL check is
+     * needed here because it's checked when descriptor tables are set. */
+    heap = vkd3d_gpu_descriptor_allocator_heap_from_descriptor(&list->device->gpu_descriptor_allocator, desc);
+    offset = desc - (const struct d3d12_desc *)heap->descriptors;
+
+    if (heap->desc.Type == D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV)
+    {
+        if (*cbv_srv_uav_heap)
+        {
+            if (heap == *cbv_srv_uav_heap)
+                return offset;
+            /* This occurs occasionally in Rise of the Tomb Raider apparently due to a race
+             * condition (one of several), but adding a mutex for table updates has no effect. */
+            WARN("List %p uses descriptors from more than one CBV/SRV/UAV heap.\n", list);
+        }
+        *cbv_srv_uav_heap = heap;
+    }
+    else
+    {
+        if (*sampler_heap)
+        {
+            if (heap == *sampler_heap)
+                return offset;
+            WARN("List %p uses descriptors from more than one sampler heap.\n", list);
+        }
+        *sampler_heap = heap;
+    }
+
+    return offset;
+}
+
+static void d3d12_command_list_update_descriptor_tables(struct d3d12_command_list *list,
+        struct vkd3d_pipeline_bindings *bindings, struct d3d12_descriptor_heap **cbv_srv_uav_heap,
+        struct d3d12_descriptor_heap **sampler_heap)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &list->device->vk_procs;
+    const struct d3d12_root_signature *rs = bindings->root_signature;
+    unsigned int offsets[D3D12_MAX_ROOT_COST];
+    unsigned int i, j;
+
+    for (i = 0, j = 0; i < ARRAY_SIZE(bindings->descriptor_tables); ++i)
+    {
+        if (!(rs->descriptor_table_mask & ((uint64_t)1 << i)))
+            continue;
+        offsets[j++] = d3d12_command_list_bind_descriptor_table(list, bindings, i,
+                cbv_srv_uav_heap, sampler_heap);
+    }
+    if (j)
+    {
+        VK_CALL(vkCmdPushConstants(list->vk_command_buffer, rs->vk_pipeline_layout, VK_SHADER_STAGE_ALL,
+                rs->descriptor_table_offset, j * sizeof(uint32_t), offsets));
+    }
+}
+
+static void d3d12_command_list_bind_descriptor_heap(struct d3d12_command_list *list,
+        enum vkd3d_pipeline_bind_point bind_point, struct d3d12_descriptor_heap *heap)
+{
+    struct vkd3d_pipeline_bindings *bindings = &list->pipeline_bindings[bind_point];
+    const struct vkd3d_vk_device_procs *vk_procs = &list->device->vk_procs;
+    const struct d3d12_root_signature *rs = bindings->root_signature;
+    enum vkd3d_vk_descriptor_set_index set;
+
+    if (!heap)
+        return;
+
+    if (heap->desc.Type == D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV)
+    {
+        if (heap->serial_id == bindings->cbv_srv_uav_heap_id)
+            return;
+        bindings->cbv_srv_uav_heap_id = heap->serial_id;
+    }
+    else
+    {
+        if (heap->serial_id == bindings->sampler_heap_id)
+            return;
+        bindings->sampler_heap_id = heap->serial_id;
+    }
+
+    /* These sets can be shared across multiple command lists, and therefore binding must
+     * be synchronised. On an experimental branch in which caching of Vk descriptor writes
+     * greatly increased the chance of multiple threads arriving here at the same time,
+     * GRID 2019 crashed without the mutex lock. */
+    vkd3d_mutex_lock(&heap->vk_sets_mutex);
+
+    for (set = 0; set < ARRAY_SIZE(heap->vk_descriptor_sets); ++set)
+    {
+        VkDescriptorSet vk_descriptor_set = heap->vk_descriptor_sets[set].vk_set;
+
+        if (!vk_descriptor_set)
+            continue;
+
+        VK_CALL(vkCmdBindDescriptorSets(list->vk_command_buffer, bindings->vk_bind_point, rs->vk_pipeline_layout,
+                rs->vk_set_count + set, 1, &vk_descriptor_set, 0, NULL));
+    }
+
+    vkd3d_mutex_unlock(&heap->vk_sets_mutex);
+}
+
+static void d3d12_command_list_update_heap_descriptors(struct d3d12_command_list *list,
+        enum vkd3d_pipeline_bind_point bind_point)
+{
+    struct vkd3d_pipeline_bindings *bindings = &list->pipeline_bindings[bind_point];
+    struct d3d12_descriptor_heap *cbv_srv_uav_heap = NULL, *sampler_heap = NULL;
+    const struct vkd3d_vk_device_procs *vk_procs = &list->device->vk_procs;
+    const struct d3d12_root_signature *rs = bindings->root_signature;
+
+    if (!rs)
+        return;
+
+    if (bindings->descriptor_table_dirty_mask || bindings->push_descriptor_dirty_mask)
+        d3d12_command_list_prepare_descriptors(list, bind_point);
+    if (bindings->descriptor_table_dirty_mask)
+        d3d12_command_list_update_descriptor_tables(list, bindings, &cbv_srv_uav_heap, &sampler_heap);
+    bindings->descriptor_table_dirty_mask = 0;
+
+    d3d12_command_list_update_push_descriptors(list, bind_point);
+
+    if (bindings->descriptor_set_count)
+    {
+        VK_CALL(vkCmdBindDescriptorSets(list->vk_command_buffer, bindings->vk_bind_point, rs->vk_pipeline_layout,
+                rs->main_set, bindings->descriptor_set_count, bindings->descriptor_sets, 0, NULL));
+        bindings->in_use = true;
+    }
+
+    d3d12_command_list_bind_descriptor_heap(list, bind_point, cbv_srv_uav_heap);
+    d3d12_command_list_bind_descriptor_heap(list, bind_point, sampler_heap);
+}
+
 static bool d3d12_command_list_update_compute_state(struct d3d12_command_list *list)
 {
     d3d12_command_list_end_current_render_pass(list);
@@ -3028,7 +3383,7 @@ static bool d3d12_command_list_update_compute_state(struct d3d12_command_list *l
     if (!d3d12_command_list_update_compute_pipeline(list))
         return false;
 
-    d3d12_command_list_update_descriptors(list, VKD3D_PIPELINE_BIND_POINT_COMPUTE);
+    list->update_descriptors(list, VKD3D_PIPELINE_BIND_POINT_COMPUTE);
 
     return true;
 }
@@ -3045,7 +3400,7 @@ static bool d3d12_command_list_begin_render_pass(struct d3d12_command_list *list
     if (!d3d12_command_list_update_current_framebuffer(list))
         return false;
 
-    d3d12_command_list_update_descriptors(list, VKD3D_PIPELINE_BIND_POINT_GRAPHICS);
+    list->update_descriptors(list, VKD3D_PIPELINE_BIND_POINT_GRAPHICS);
 
     if (list->current_render_pass != VK_NULL_HANDLE)
         return true;
@@ -4113,6 +4468,8 @@ static void STDMETHODCALLTYPE d3d12_command_list_SetDescriptorHeaps(ID3D12Graphi
     TRACE("iface %p, heap_count %u, heaps %p.\n", iface, heap_count, heaps);
 
     /* Our current implementation does not need this method.
+     * In Windows it doesn't need to be called at all for correct operation, and
+     * at least on AMD the wrong heaps can be set here and tests still succeed.
      *
      * It could be used to validate descriptor tables but we do not have an
      * equivalent of the D3D12 Debug Layer. */
@@ -5108,7 +5465,7 @@ static void STDMETHODCALLTYPE d3d12_command_list_ClearUnorderedAccessViewUint(ID
             iface, gpu_handle.ptr, cpu_handle.ptr, resource, values, rect_count, rects);
 
     resource_impl = unsafe_impl_from_ID3D12Resource(resource);
-    view = d3d12_desc_from_cpu_handle(cpu_handle)->u.view;
+    view = d3d12_desc_from_cpu_handle(cpu_handle)->u.view_info.view;
     memcpy(colour.uint32, values, sizeof(colour.uint32));
 
     if (view->format->type != VKD3D_FORMAT_TYPE_UINT)
@@ -5167,7 +5524,7 @@ static void STDMETHODCALLTYPE d3d12_command_list_ClearUnorderedAccessViewFloat(I
             iface, gpu_handle.ptr, cpu_handle.ptr, resource, values, rect_count, rects);
 
     resource_impl = unsafe_impl_from_ID3D12Resource(resource);
-    view = d3d12_desc_from_cpu_handle(cpu_handle)->u.view;
+    view = d3d12_desc_from_cpu_handle(cpu_handle)->u.view_info.view;
     memcpy(colour.float32, values, sizeof(colour.float32));
 
     d3d12_command_list_clear_uav(list, resource_impl, view, &colour, rect_count, rects);
@@ -5706,6 +6063,9 @@ static HRESULT d3d12_command_list_init(struct d3d12_command_list *list, struct d
 
     list->allocator = allocator;
 
+    list->update_descriptors = device->use_vk_heaps ? d3d12_command_list_update_heap_descriptors
+            : d3d12_command_list_update_descriptors;
+
     if (SUCCEEDED(hr = d3d12_command_allocator_allocate_command_buffer(allocator, list)))
     {
         list->pipeline_bindings[VKD3D_PIPELINE_BIND_POINT_GRAPHICS].vk_uav_counter_views = NULL;
@@ -5809,6 +6169,11 @@ static ULONG STDMETHODCALLTYPE d3d12_command_queue_Release(ID3D12CommandQueue *i
     {
         struct d3d12_device *device = command_queue->device;
 
+        vkd3d_fence_worker_stop(&command_queue->fence_worker, device);
+
+        vkd3d_mutex_destroy(&command_queue->op_mutex);
+        vkd3d_free(command_queue->ops);
+
         vkd3d_private_store_destroy(&command_queue->private_store);
 
         vkd3d_free(command_queue);
@@ -5875,7 +6240,15 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_GetDevice(ID3D12CommandQueu
 
     TRACE("iface %p, iid %s, device %p.\n", iface, debugstr_guid(iid), device);
 
-    return d3d12_device_query_interface(command_queue->device, iid, device);
+    return d3d12_device_query_interface(command_queue->device, iid, device);
+}
+
+static struct vkd3d_cs_op_data *d3d12_command_queue_require_space_locked(struct d3d12_command_queue *queue)
+{
+    if (!vkd3d_array_reserve((void **)&queue->ops, &queue->ops_size, queue->ops_count + 1, sizeof(*queue->ops)))
+        return NULL;
+
+    return &queue->ops[queue->ops_count++];
 }
 
 static void STDMETHODCALLTYPE d3d12_command_queue_UpdateTileMappings(ID3D12CommandQueue *iface,
@@ -5909,22 +6282,50 @@ static void STDMETHODCALLTYPE d3d12_command_queue_CopyTileMappings(ID3D12Command
             src_region_start_coordinate, region_size, flags);
 }
 
+static void d3d12_command_queue_execute(struct d3d12_command_queue *command_queue,
+        VkCommandBuffer *buffers, unsigned int count)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &command_queue->device->vk_procs;
+    struct vkd3d_queue *vkd3d_queue = command_queue->vkd3d_queue;
+    VkSubmitInfo submit_desc;
+    VkQueue vk_queue;
+    VkResult vr;
+
+    memset(&submit_desc, 0, sizeof(submit_desc));
+
+    if (!(vk_queue = vkd3d_queue_acquire(vkd3d_queue)))
+    {
+        ERR("Failed to acquire queue %p.\n", vkd3d_queue);
+        return;
+    }
+
+    submit_desc.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
+    submit_desc.commandBufferCount = count;
+    submit_desc.pCommandBuffers = buffers;
+
+    if ((vr = VK_CALL(vkQueueSubmit(vk_queue, 1, &submit_desc, VK_NULL_HANDLE))) < 0)
+        ERR("Failed to submit queue(s), vr %d.\n", vr);
+
+    vkd3d_queue_release(vkd3d_queue);
+
+    vkd3d_free(buffers);
+}
+
 static void STDMETHODCALLTYPE d3d12_command_queue_ExecuteCommandLists(ID3D12CommandQueue *iface,
         UINT command_list_count, ID3D12CommandList * const *command_lists)
 {
     struct d3d12_command_queue *command_queue = impl_from_ID3D12CommandQueue(iface);
-    const struct vkd3d_vk_device_procs *vk_procs;
     struct d3d12_command_list *cmd_list;
-    struct VkSubmitInfo submit_desc;
+    struct vkd3d_cs_op_data *op;
     VkCommandBuffer *buffers;
-    VkQueue vk_queue;
     unsigned int i;
-    VkResult vr;
+    int rc;
 
     TRACE("iface %p, command_list_count %u, command_lists %p.\n",
             iface, command_list_count, command_lists);
 
-    vk_procs = &command_queue->device->vk_procs;
+    if (!command_list_count)
+        return;
 
     if (!(buffers = vkd3d_calloc(command_list_count, sizeof(*buffers))))
     {
@@ -5947,29 +6348,30 @@ static void STDMETHODCALLTYPE d3d12_command_queue_ExecuteCommandLists(ID3D12Comm
         buffers[i] = cmd_list->vk_command_buffer;
     }
 
-    submit_desc.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
-    submit_desc.pNext = NULL;
-    submit_desc.waitSemaphoreCount = 0;
-    submit_desc.pWaitSemaphores = NULL;
-    submit_desc.pWaitDstStageMask = NULL;
-    submit_desc.commandBufferCount = command_list_count;
-    submit_desc.pCommandBuffers = buffers;
-    submit_desc.signalSemaphoreCount = 0;
-    submit_desc.pSignalSemaphores = NULL;
-
-    if (!(vk_queue = vkd3d_queue_acquire(command_queue->vkd3d_queue)))
+    if ((rc = vkd3d_mutex_lock(&command_queue->op_mutex)))
     {
-        ERR("Failed to acquire queue %p.\n", command_queue->vkd3d_queue);
-        vkd3d_free(buffers);
+        ERR("Failed to lock mutex, error %d.\n", rc);
         return;
     }
 
-    if ((vr = VK_CALL(vkQueueSubmit(vk_queue, 1, &submit_desc, VK_NULL_HANDLE))) < 0)
-        ERR("Failed to submit queue(s), vr %d.\n", vr);
+    if (!command_queue->ops_count)
+    {
+        d3d12_command_queue_execute(command_queue, buffers, command_list_count);
+        vkd3d_mutex_unlock(&command_queue->op_mutex);
+        return;
+    }
 
-    vkd3d_queue_release(command_queue->vkd3d_queue);
+    if (!(op = d3d12_command_queue_require_space_locked(command_queue)))
+    {
+        ERR("Failed to add op.\n");
+        return;
+    }
+    op->opcode = VKD3D_CS_OP_EXECUTE;
+    op->u.execute.buffers = buffers;
+    op->u.execute.buffer_count = command_list_count;
 
-    vkd3d_free(buffers);
+    vkd3d_mutex_unlock(&command_queue->op_mutex);
+    return;
 }
 
 static void STDMETHODCALLTYPE d3d12_command_queue_SetMarker(ID3D12CommandQueue *iface,
@@ -5982,7 +6384,7 @@ static void STDMETHODCALLTYPE d3d12_command_queue_SetMarker(ID3D12CommandQueue *
 static void STDMETHODCALLTYPE d3d12_command_queue_BeginEvent(ID3D12CommandQueue *iface,
         UINT metadata, const void *data, UINT size)
 {
-    FIXME("iface %p, metatdata %#x, data %p, size %u stub!\n",
+    FIXME("iface %p, metadata %#x, data %p, size %u stub!\n",
             iface, metadata, data, size);
 }
 
@@ -5991,34 +6393,118 @@ static void STDMETHODCALLTYPE d3d12_command_queue_EndEvent(ID3D12CommandQueue *i
     FIXME("iface %p stub!\n", iface);
 }
 
+static HRESULT vkd3d_enqueue_timeline_semaphore(struct vkd3d_fence_worker *worker, VkSemaphore vk_semaphore,
+        struct d3d12_fence *fence, uint64_t value, struct vkd3d_queue *queue)
+{
+    struct vkd3d_waiting_fence *waiting_fence;
+    int rc;
+
+    TRACE("worker %p, fence %p, value %#"PRIx64".\n", worker, fence, value);
+
+    if ((rc = vkd3d_mutex_lock(&worker->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+
+    if (!vkd3d_array_reserve((void **)&worker->fences, &worker->fences_size,
+            worker->fence_count + 1, sizeof(*worker->fences)))
+    {
+        ERR("Failed to add GPU timeline semaphore.\n");
+        vkd3d_mutex_unlock(&worker->mutex);
+        return E_OUTOFMEMORY;
+    }
+
+    waiting_fence = &worker->fences[worker->fence_count++];
+    waiting_fence->fence = fence;
+    waiting_fence->value = value;
+    waiting_fence->u.vk_semaphore = vk_semaphore;
+
+    d3d12_fence_incref(fence);
+
+    vkd3d_cond_signal(&worker->cond);
+    vkd3d_mutex_unlock(&worker->mutex);
+
+    return S_OK;
+}
+
 static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Signal(ID3D12CommandQueue *iface,
         ID3D12Fence *fence_iface, UINT64 value)
 {
     struct d3d12_command_queue *command_queue = impl_from_ID3D12CommandQueue(iface);
+    struct d3d12_fence *fence = unsafe_impl_from_ID3D12Fence(fence_iface);
+    struct vkd3d_cs_op_data *op;
+    HRESULT hr = S_OK;
+    int rc;
+
+    TRACE("iface %p, fence %p, value %#"PRIx64".\n", iface, fence_iface, value);
+
+    if ((rc = vkd3d_mutex_lock(&command_queue->op_mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+
+    if (!command_queue->ops_count)
+    {
+        hr = d3d12_command_queue_signal(command_queue, fence, value);
+        goto done;
+    }
+
+    if (!(op = d3d12_command_queue_require_space_locked(command_queue)))
+    {
+        hr = E_OUTOFMEMORY;
+        goto done;
+    }
+    op->opcode = VKD3D_CS_OP_SIGNAL;
+    op->u.signal.fence = fence;
+    op->u.signal.value = value;
+
+    d3d12_fence_incref(fence);
+
+done:
+    vkd3d_mutex_unlock(&command_queue->op_mutex);
+    return hr;
+}
+
+static HRESULT d3d12_command_queue_signal(struct d3d12_command_queue *command_queue,
+        struct d3d12_fence *fence, uint64_t value)
+{
+    VkTimelineSemaphoreSubmitInfoKHR timeline_submit_info;
     const struct vkd3d_vk_device_procs *vk_procs;
     VkSemaphore vk_semaphore = VK_NULL_HANDLE;
     VkFence vk_fence = VK_NULL_HANDLE;
     struct vkd3d_queue *vkd3d_queue;
+    uint64_t sequence_number = 0;
+    uint64_t timeline_value = 0;
     struct d3d12_device *device;
-    struct d3d12_fence *fence;
     VkSubmitInfo submit_info;
-    uint64_t sequence_number;
     VkQueue vk_queue;
     VkResult vr;
     HRESULT hr;
 
-    TRACE("iface %p, fence %p, value %#"PRIx64".\n", iface, fence_iface, value);
-
     device = command_queue->device;
     vk_procs = &device->vk_procs;
     vkd3d_queue = command_queue->vkd3d_queue;
 
-    fence = unsafe_impl_from_ID3D12Fence(fence_iface);
+    if (device->vk_info.KHR_timeline_semaphore)
+    {
+        if (!(timeline_value = d3d12_fence_add_pending_timeline_signal(fence, value, vkd3d_queue)))
+        {
+            ERR("Failed to add pending signal.\n");
+            return E_OUTOFMEMORY;
+        }
 
-    if ((vr = d3d12_fence_create_vk_fence(fence, &vk_fence)) < 0)
+        vk_semaphore = fence->timeline_semaphore;
+        assert(vk_semaphore);
+    }
+    else
     {
-        WARN("Failed to create Vulkan fence, vr %d.\n", vr);
-        goto fail_vkresult;
+        if ((vr = d3d12_fence_create_vk_fence(fence, &vk_fence)) < 0)
+        {
+            WARN("Failed to create Vulkan fence, vr %d.\n", vr);
+            goto fail_vkresult;
+        }
     }
 
     if (!(vk_queue = vkd3d_queue_acquire(vkd3d_queue)))
@@ -6028,7 +6514,8 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Signal(ID3D12CommandQueue *
         goto fail;
     }
 
-    if ((vr = vkd3d_queue_create_vk_semaphore_locked(vkd3d_queue, device, &vk_semaphore)) < 0)
+    if (!device->vk_info.KHR_timeline_semaphore && (vr = vkd3d_queue_create_vk_semaphore_locked(vkd3d_queue,
+            device, &vk_semaphore)) < 0)
     {
         ERR("Failed to create Vulkan semaphore, vr %d.\n", vr);
         vk_semaphore = VK_NULL_HANDLE;
@@ -6044,7 +6531,19 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Signal(ID3D12CommandQueue *
     submit_info.signalSemaphoreCount = vk_semaphore ? 1 : 0;
     submit_info.pSignalSemaphores = &vk_semaphore;
 
-    if ((vr = VK_CALL(vkQueueSubmit(vk_queue, 1, &submit_info, vk_fence))) >= 0)
+    if (device->vk_info.KHR_timeline_semaphore)
+    {
+        timeline_submit_info.sType = VK_STRUCTURE_TYPE_TIMELINE_SEMAPHORE_SUBMIT_INFO_KHR;
+        timeline_submit_info.pNext = NULL;
+        timeline_submit_info.pSignalSemaphoreValues = &timeline_value;
+        timeline_submit_info.signalSemaphoreValueCount = submit_info.signalSemaphoreCount;
+        timeline_submit_info.waitSemaphoreValueCount = 0;
+        timeline_submit_info.pWaitSemaphoreValues = NULL;
+        submit_info.pNext = &timeline_submit_info;
+    }
+
+    vr = VK_CALL(vkQueueSubmit(vk_queue, 1, &submit_info, vk_fence));
+    if (!device->vk_info.KHR_timeline_semaphore && vr >= 0)
     {
         sequence_number = ++vkd3d_queue->submitted_sequence_number;
 
@@ -6061,19 +6560,37 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Signal(ID3D12CommandQueue *
         goto fail_vkresult;
     }
 
-    if (vk_semaphore && SUCCEEDED(hr = d3d12_fence_add_vk_semaphore(fence, vk_semaphore, vk_fence, value)))
+    if (device->vk_info.KHR_timeline_semaphore)
+    {
+        if (FAILED(hr = d3d12_fence_update_pending_value(fence)))
+            return hr;
+
+        if (FAILED(hr = d3d12_device_flush_blocked_queues(device)))
+            return hr;
+
+        vk_semaphore = fence->timeline_semaphore;
+        assert(vk_semaphore);
+
+        return vkd3d_enqueue_timeline_semaphore(&command_queue->fence_worker,
+                vk_semaphore, fence, timeline_value, vkd3d_queue);
+    }
+
+    if (vk_semaphore && SUCCEEDED(hr = d3d12_fence_add_vk_semaphore(fence, vk_semaphore, vk_fence, value, vkd3d_queue)))
         vk_semaphore = VK_NULL_HANDLE;
 
     vr = VK_CALL(vkGetFenceStatus(device->vk_device, vk_fence));
     if (vr == VK_NOT_READY)
     {
-        if (SUCCEEDED(hr = vkd3d_enqueue_gpu_fence(&device->fence_worker, vk_fence, fence, value, vkd3d_queue, sequence_number)))
+        if (SUCCEEDED(hr = vkd3d_enqueue_gpu_fence(&command_queue->fence_worker,
+                vk_fence, fence, value, vkd3d_queue, sequence_number)))
+        {
             vk_fence = VK_NULL_HANDLE;
+        }
     }
     else if (vr == VK_SUCCESS)
     {
         TRACE("Already signaled %p, value %#"PRIx64".\n", fence, value);
-        hr = d3d12_fence_signal(fence, value, vk_fence);
+        hr = d3d12_fence_signal(fence, value, vk_fence, false);
         vk_fence = VK_NULL_HANDLE;
         vkd3d_queue_update_sequence_number(vkd3d_queue, sequence_number, device);
     }
@@ -6096,33 +6613,31 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Signal(ID3D12CommandQueue *
     hr = hresult_from_vk_result(vr);
 fail:
     VK_CALL(vkDestroyFence(device->vk_device, vk_fence, NULL));
-    VK_CALL(vkDestroySemaphore(device->vk_device, vk_semaphore, NULL));
+    if (!device->vk_info.KHR_timeline_semaphore)
+        VK_CALL(vkDestroySemaphore(device->vk_device, vk_semaphore, NULL));
     return hr;
 }
 
-static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Wait(ID3D12CommandQueue *iface,
-        ID3D12Fence *fence_iface, UINT64 value)
+static HRESULT d3d12_command_queue_wait_binary_semaphore_locked(struct d3d12_command_queue *command_queue,
+        struct d3d12_fence *fence, uint64_t value)
 {
     static const VkPipelineStageFlagBits wait_stage_mask = VK_PIPELINE_STAGE_ALL_COMMANDS_BIT;
-    struct d3d12_command_queue *command_queue = impl_from_ID3D12CommandQueue(iface);
     const struct vkd3d_vk_device_procs *vk_procs;
     struct vkd3d_signaled_semaphore *semaphore;
     uint64_t completed_value = 0;
     struct vkd3d_queue *queue;
-    struct d3d12_fence *fence;
     VkSubmitInfo submit_info;
     VkQueue vk_queue;
     VkResult vr;
     HRESULT hr;
 
-    TRACE("iface %p, fence %p, value %#"PRIx64".\n", iface, fence_iface, value);
-
     vk_procs = &command_queue->device->vk_procs;
     queue = command_queue->vkd3d_queue;
 
-    fence = unsafe_impl_from_ID3D12Fence(fence_iface);
+    semaphore = d3d12_fence_acquire_vk_semaphore_locked(fence, value, &completed_value);
+
+    vkd3d_mutex_unlock(&fence->mutex);
 
-    semaphore = d3d12_fence_acquire_vk_semaphore(fence, value, &completed_value);
     if (!semaphore && completed_value >= value)
     {
         /* We don't get a Vulkan semaphore if the fence was signaled on CPU. */
@@ -6145,7 +6660,7 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Wait(ID3D12CommandQueue *if
         }
         else
         {
-            FIXME("Failed to acquire Vulkan semaphore for fence %p, value %#"PRIx64
+            WARN("Failed to acquire Vulkan semaphore for fence %p, value %#"PRIx64
                     ", completed value %#"PRIx64".\n", fence, value, completed_value);
         }
 
@@ -6156,7 +6671,7 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Wait(ID3D12CommandQueue *if
     submit_info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
     submit_info.pNext = NULL;
     submit_info.waitSemaphoreCount = 1;
-    submit_info.pWaitSemaphores = &semaphore->vk_semaphore;
+    submit_info.pWaitSemaphores = &semaphore->u.binary.vk_semaphore;
     submit_info.pWaitDstStageMask = &wait_stage_mask;
     submit_info.commandBufferCount = 0;
     submit_info.pCommandBuffers = NULL;
@@ -6174,7 +6689,7 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Wait(ID3D12CommandQueue *if
 
     if ((vr = VK_CALL(vkQueueSubmit(vk_queue, 1, &submit_info, VK_NULL_HANDLE))) >= 0)
     {
-        queue->semaphores[queue->semaphore_count].vk_semaphore = semaphore->vk_semaphore;
+        queue->semaphores[queue->semaphore_count].vk_semaphore = semaphore->u.binary.vk_semaphore;
         queue->semaphores[queue->semaphore_count].sequence_number = queue->submitted_sequence_number + 1;
         ++queue->semaphore_count;
 
@@ -6199,6 +6714,126 @@ static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Wait(ID3D12CommandQueue *if
     return hr;
 }
 
+static HRESULT d3d12_command_queue_wait_locked(struct d3d12_command_queue *command_queue,
+        struct d3d12_fence *fence, uint64_t value)
+{
+    static const VkPipelineStageFlagBits wait_stage_mask = VK_PIPELINE_STAGE_ALL_COMMANDS_BIT;
+    VkTimelineSemaphoreSubmitInfoKHR timeline_submit_info;
+    const struct vkd3d_vk_device_procs *vk_procs;
+    struct vkd3d_queue *queue;
+    VkSubmitInfo submit_info;
+    uint64_t wait_value;
+    VkQueue vk_queue;
+    VkResult vr;
+
+    vk_procs = &command_queue->device->vk_procs;
+    queue = command_queue->vkd3d_queue;
+
+    if (!command_queue->device->vk_info.KHR_timeline_semaphore)
+        return d3d12_command_queue_wait_binary_semaphore_locked(command_queue, fence, value);
+
+    wait_value = d3d12_fence_get_timeline_wait_value_locked(fence, value);
+
+    /* We can unlock the fence here. The queue semaphore will not be signalled to signal_value
+     * until we have submitted, so the semaphore cannot be destroyed before the call to vkQueueSubmit. */
+    vkd3d_mutex_unlock(&fence->mutex);
+
+    assert(fence->timeline_semaphore);
+    timeline_submit_info.sType = VK_STRUCTURE_TYPE_TIMELINE_SEMAPHORE_SUBMIT_INFO_KHR;
+    timeline_submit_info.pNext = NULL;
+    timeline_submit_info.waitSemaphoreValueCount = 1;
+    timeline_submit_info.pWaitSemaphoreValues = &wait_value;
+    timeline_submit_info.signalSemaphoreValueCount = 0;
+    timeline_submit_info.pSignalSemaphoreValues = NULL;
+
+    submit_info.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
+    submit_info.pNext = &timeline_submit_info;
+    submit_info.waitSemaphoreCount = 1;
+    submit_info.pWaitSemaphores = &fence->timeline_semaphore;
+    submit_info.pWaitDstStageMask = &wait_stage_mask;
+    submit_info.commandBufferCount = 0;
+    submit_info.pCommandBuffers = NULL;
+    submit_info.signalSemaphoreCount = 0;
+    submit_info.pSignalSemaphores = NULL;
+
+    if (!(vk_queue = vkd3d_queue_acquire(queue)))
+    {
+        ERR("Failed to acquire queue %p.\n", queue);
+        return E_FAIL;
+    }
+
+    vr = VK_CALL(vkQueueSubmit(vk_queue, 1, &submit_info, VK_NULL_HANDLE));
+
+    vkd3d_queue_release(queue);
+
+    if (vr < 0)
+    {
+        WARN("Failed to submit wait operation, vr %d.\n", vr);
+        return hresult_from_vk_result(vr);
+    }
+
+    return S_OK;
+}
+
+static HRESULT STDMETHODCALLTYPE d3d12_command_queue_Wait(ID3D12CommandQueue *iface,
+        ID3D12Fence *fence_iface, UINT64 value)
+{
+    struct d3d12_command_queue *command_queue = impl_from_ID3D12CommandQueue(iface);
+    struct d3d12_fence *fence = unsafe_impl_from_ID3D12Fence(fence_iface);
+    struct vkd3d_cs_op_data *op;
+    HRESULT hr = S_OK;
+    int rc;
+
+    TRACE("iface %p, fence %p, value %#"PRIx64".\n", iface, fence_iface, value);
+
+    if ((rc = vkd3d_mutex_lock(&command_queue->op_mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return hresult_from_errno(rc);
+    }
+    if ((rc = vkd3d_mutex_lock(&fence->mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        hr = hresult_from_errno(rc);
+        goto done;
+    }
+
+    if (!command_queue->ops_count && value <= fence->max_pending_value)
+    {
+        hr = d3d12_command_queue_wait_locked(command_queue, fence, value);
+        goto done;
+    }
+
+    vkd3d_mutex_unlock(&fence->mutex);
+
+    /* This is the critical part required to support out-of-order signal.
+     * Normally we would be able to submit waits and signals out of order, but
+     * we don't have virtualized queues in Vulkan, so we need to handle the case
+     * where multiple queues alias over the same physical queue, so effectively,
+     * we need to manage out-of-order submits ourselves. */
+
+    if (!command_queue->ops_count)
+        hr = d3d12_device_add_blocked_command_queues(command_queue->device, &command_queue, 1);
+
+    if (FAILED(hr))
+        goto done;
+
+    if (!(op = d3d12_command_queue_require_space_locked(command_queue)))
+    {
+        hr = E_OUTOFMEMORY;
+        goto done;
+    }
+    op->opcode = VKD3D_CS_OP_WAIT;
+    op->u.wait.fence = fence;
+    op->u.wait.value = value;
+
+    d3d12_fence_incref(fence);
+
+done:
+    vkd3d_mutex_unlock(&command_queue->op_mutex);
+    return hr;
+}
+
 static HRESULT STDMETHODCALLTYPE d3d12_command_queue_GetTimestampFrequency(ID3D12CommandQueue *iface,
         UINT64 *frequency)
 {
@@ -6320,10 +6955,82 @@ static const struct ID3D12CommandQueueVtbl d3d12_command_queue_vtbl =
     d3d12_command_queue_GetDesc,
 };
 
+/* flushed_any is initialised by the caller. */
+static bool d3d12_command_queue_flush_ops(struct d3d12_command_queue *queue, bool *flushed_any)
+{
+    struct vkd3d_cs_op_data *op;
+    struct d3d12_fence *fence;
+    bool flushed_all = false;
+    unsigned int i;
+    int rc;
+
+    if (!queue->ops_count)
+        return true;
+
+    /* This function may be re-entered during a call below to d3d12_command_queue_signal().
+     * We return true because the first caller is responsible for re-adding this queue to
+     * the flush list if it ends up returning false. */
+    if (queue->is_flushing)
+        return true;
+
+    if ((rc = vkd3d_mutex_lock(&queue->op_mutex)))
+    {
+        ERR("Failed to lock mutex, error %d.\n", rc);
+        return true;
+    }
+
+    /* Currently only required for d3d12_command_queue_signal(), but set it here anyway. */
+    queue->is_flushing = true;
+
+    for (i = 0; i < queue->ops_count; ++i)
+    {
+        op = &queue->ops[i];
+        switch (op->opcode)
+        {
+            case VKD3D_CS_OP_WAIT:
+                fence = op->u.wait.fence;
+                if (op->u.wait.value > fence->max_pending_value)
+                {
+                    queue->ops_count -= i;
+                    memmove(queue->ops, op, queue->ops_count * sizeof(*op));
+                    goto done;
+                }
+                vkd3d_mutex_lock(&fence->mutex);
+                d3d12_command_queue_wait_locked(queue, fence, op->u.wait.value);
+                d3d12_fence_decref(fence);
+                break;
+
+            case VKD3D_CS_OP_SIGNAL:
+                d3d12_command_queue_signal(queue, op->u.signal.fence, op->u.signal.value);
+                d3d12_fence_decref(op->u.signal.fence);
+                break;
+
+            case VKD3D_CS_OP_EXECUTE:
+                d3d12_command_queue_execute(queue, op->u.execute.buffers, op->u.execute.buffer_count);
+                break;
+
+            default:
+                FIXME("Unhandled op type %u.\n", op->opcode);
+                break;
+        }
+        *flushed_any |= true;
+    }
+
+    queue->ops_count = 0;
+    flushed_all = true;
+
+done:
+    queue->is_flushing = false;
+
+    vkd3d_mutex_unlock(&queue->op_mutex);
+    return flushed_all;
+}
+
 static HRESULT d3d12_command_queue_init(struct d3d12_command_queue *queue,
         struct d3d12_device *device, const D3D12_COMMAND_QUEUE_DESC *desc)
 {
     HRESULT hr;
+    int rc;
 
     queue->ID3D12CommandQueue_iface.lpVtbl = &d3d12_command_queue_vtbl;
     queue->refcount = 1;
@@ -6338,6 +7045,11 @@ static HRESULT d3d12_command_queue_init(struct d3d12_command_queue *queue,
     queue->last_waited_fence = NULL;
     queue->last_waited_fence_value = 0;
 
+    queue->ops = NULL;
+    queue->ops_count = 0;
+    queue->ops_size = 0;
+    queue->is_flushing = false;
+
     if (desc->Priority == D3D12_COMMAND_QUEUE_PRIORITY_GLOBAL_REALTIME)
     {
         FIXME("Global realtime priority is not implemented.\n");
@@ -6352,9 +7064,24 @@ static HRESULT d3d12_command_queue_init(struct d3d12_command_queue *queue,
     if (FAILED(hr = vkd3d_private_store_init(&queue->private_store)))
         return hr;
 
+    if ((rc = vkd3d_mutex_init(&queue->op_mutex)) < 0)
+    {
+        hr = hresult_from_errno(rc);
+        goto fail_destroy_private_store;
+    }
+
+    if (FAILED(hr = vkd3d_fence_worker_start(&queue->fence_worker, queue->vkd3d_queue, device)))
+        goto fail_destroy_op_mutex;
+
     d3d12_device_add_ref(queue->device = device);
 
     return S_OK;
+
+fail_destroy_op_mutex:
+    vkd3d_mutex_destroy(&queue->op_mutex);
+fail_destroy_private_store:
+    vkd3d_private_store_destroy(&queue->private_store);
+    return hr;
 }
 
 HRESULT d3d12_command_queue_create(struct d3d12_device *device,
@@ -6389,8 +7116,12 @@ uint32_t vkd3d_get_vk_queue_family_index(ID3D12CommandQueue *queue)
 VkQueue vkd3d_acquire_vk_queue(ID3D12CommandQueue *queue)
 {
     struct d3d12_command_queue *d3d12_queue = impl_from_ID3D12CommandQueue(queue);
+    VkQueue vk_queue = vkd3d_queue_acquire(d3d12_queue->vkd3d_queue);
+
+    if (d3d12_queue->ops_count)
+        WARN("Acquired command queue %p with %zu remaining ops.\n", d3d12_queue, d3d12_queue->ops_count);
 
-    return vkd3d_queue_acquire(d3d12_queue->vkd3d_queue);
+    return vk_queue;
 }
 
 void vkd3d_release_vk_queue(ID3D12CommandQueue *queue)
diff --git a/libs/vkd3d/libs/vkd3d/device.c b/libs/vkd3d/libs/vkd3d/device.c
index 59fa9af9b..eaedc4446 100644
--- a/libs/vkd3d/libs/vkd3d/device.c
+++ b/libs/vkd3d/libs/vkd3d/device.c
@@ -19,51 +19,6 @@
 #include "vkd3d_private.h"
 #include "vkd3d_version.h"
 
-#ifdef HAVE_DLFCN_H
-#include <dlfcn.h>
-
-static void *vkd3d_dlopen(const char *name)
-{
-    return dlopen(name, RTLD_NOW);
-}
-
-static void *vkd3d_dlsym(void *handle, const char *symbol)
-{
-    return dlsym(handle, symbol);
-}
-
-static int vkd3d_dlclose(void *handle)
-{
-    return dlclose(handle);
-}
-
-static const char *vkd3d_dlerror(void)
-{
-    return dlerror();
-}
-#else
-static void *vkd3d_dlopen(const char *name)
-{
-    FIXME("Not implemented for this platform.\n");
-    return NULL;
-}
-
-static void *vkd3d_dlsym(void *handle, const char *symbol)
-{
-    return NULL;
-}
-
-static int vkd3d_dlclose(void *handle)
-{
-    return 0;
-}
-
-static const char *vkd3d_dlerror(void)
-{
-    return "Not implemented for this platform.\n";
-}
-#endif
-
 struct vkd3d_struct
 {
     enum vkd3d_structure_type type;
@@ -129,12 +84,14 @@ static const struct vkd3d_optional_extension_info optional_device_extensions[] =
     VK_EXTENSION(KHR_MAINTENANCE3, KHR_maintenance3),
     VK_EXTENSION(KHR_PUSH_DESCRIPTOR, KHR_push_descriptor),
     VK_EXTENSION(KHR_SAMPLER_MIRROR_CLAMP_TO_EDGE, KHR_sampler_mirror_clamp_to_edge),
+    VK_EXTENSION(KHR_TIMELINE_SEMAPHORE, KHR_timeline_semaphore),
     /* EXT extensions */
     VK_EXTENSION(EXT_CALIBRATED_TIMESTAMPS, EXT_calibrated_timestamps),
     VK_EXTENSION(EXT_CONDITIONAL_RENDERING, EXT_conditional_rendering),
     VK_EXTENSION(EXT_DEBUG_MARKER, EXT_debug_marker),
     VK_EXTENSION(EXT_DEPTH_CLIP_ENABLE, EXT_depth_clip_enable),
     VK_EXTENSION(EXT_DESCRIPTOR_INDEXING, EXT_descriptor_indexing),
+    VK_EXTENSION(EXT_ROBUSTNESS_2, EXT_robustness2),
     VK_EXTENSION(EXT_SHADER_DEMOTE_TO_HELPER_INVOCATION, EXT_shader_demote_to_helper_invocation),
     VK_EXTENSION(EXT_SHADER_STENCIL_EXPORT, EXT_shader_stencil_export),
     VK_EXTENSION(EXT_TEXEL_BUFFER_ALIGNMENT, EXT_texel_buffer_alignment),
@@ -142,6 +99,112 @@ static const struct vkd3d_optional_extension_info optional_device_extensions[] =
     VK_EXTENSION(EXT_VERTEX_ATTRIBUTE_DIVISOR, EXT_vertex_attribute_divisor),
 };
 
+static HRESULT vkd3d_create_vk_descriptor_heap_layout(struct d3d12_device *device, unsigned int index)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkDescriptorSetLayoutBindingFlagsCreateInfoEXT flags_info;
+    VkDescriptorSetLayoutCreateInfo set_desc;
+    VkDescriptorBindingFlagsEXT set_flags;
+    VkDescriptorSetLayoutBinding binding;
+    VkResult vr;
+
+    binding.binding = 0;
+    binding.descriptorType = device->vk_descriptor_heap_layouts[index].type;
+    binding.descriptorCount = device->vk_descriptor_heap_layouts[index].count;
+    binding.stageFlags = VK_SHADER_STAGE_ALL;
+    binding.pImmutableSamplers = NULL;
+
+    set_desc.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
+    set_desc.pNext = &flags_info;
+    set_desc.flags = VK_DESCRIPTOR_SET_LAYOUT_CREATE_UPDATE_AFTER_BIND_POOL_BIT_EXT;
+    set_desc.bindingCount = 1;
+    set_desc.pBindings = &binding;
+
+    set_flags = VK_DESCRIPTOR_BINDING_VARIABLE_DESCRIPTOR_COUNT_BIT_EXT
+            | VK_DESCRIPTOR_BINDING_PARTIALLY_BOUND_BIT_EXT | VK_DESCRIPTOR_BINDING_UPDATE_AFTER_BIND_BIT_EXT
+            | VK_DESCRIPTOR_BINDING_UPDATE_UNUSED_WHILE_PENDING_BIT_EXT;
+
+    flags_info.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_BINDING_FLAGS_CREATE_INFO_EXT;
+    flags_info.pNext = NULL;
+    flags_info.bindingCount = 1;
+    flags_info.pBindingFlags = &set_flags;
+
+    if ((vr = VK_CALL(vkCreateDescriptorSetLayout(device->vk_device, &set_desc, NULL,
+            &device->vk_descriptor_heap_layouts[index].vk_set_layout))) < 0)
+    {
+        WARN("Failed to create Vulkan descriptor set layout, vr %d.\n", vr);
+        return hresult_from_vk_result(vr);
+    }
+
+    return S_OK;
+}
+
+static void vkd3d_vk_descriptor_heap_layouts_cleanup(struct d3d12_device *device)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    enum vkd3d_vk_descriptor_set_index set;
+
+    for (set = 0; set < ARRAY_SIZE(device->vk_descriptor_heap_layouts); ++set)
+        VK_CALL(vkDestroyDescriptorSetLayout(device->vk_device, device->vk_descriptor_heap_layouts[set].vk_set_layout,
+                NULL));
+}
+
+static HRESULT vkd3d_vk_descriptor_heap_layouts_init(struct d3d12_device *device)
+{
+    static const struct vkd3d_vk_descriptor_heap_layout vk_descriptor_heap_layouts[VKD3D_SET_INDEX_COUNT] =
+    {
+        {VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER, true, D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV},
+        {VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER, true, D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV},
+        {VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE, false, D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV},
+        {VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, true, D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV},
+        {VK_DESCRIPTOR_TYPE_STORAGE_IMAGE, false, D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV},
+        {VK_DESCRIPTOR_TYPE_SAMPLER, false, D3D12_DESCRIPTOR_HEAP_TYPE_SAMPLER},
+        /* UAV counters */
+        {VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER, true, D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV},
+    };
+    const struct vkd3d_device_descriptor_limits *limits = &device->vk_info.descriptor_limits;
+    enum vkd3d_vk_descriptor_set_index set;
+    HRESULT hr;
+
+    for (set = 0; set < ARRAY_SIZE(device->vk_descriptor_heap_layouts); ++set)
+        device->vk_descriptor_heap_layouts[set] = vk_descriptor_heap_layouts[set];
+
+    if (!device->use_vk_heaps)
+        return S_OK;
+
+    for (set = 0; set < ARRAY_SIZE(device->vk_descriptor_heap_layouts); ++set)
+    {
+        switch (device->vk_descriptor_heap_layouts[set].type)
+        {
+            case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+                device->vk_descriptor_heap_layouts[set].count = limits->uniform_buffer_max_descriptors;
+                break;
+            case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+            case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+                device->vk_descriptor_heap_layouts[set].count = limits->sampled_image_max_descriptors;
+                break;
+            case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+            case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+                device->vk_descriptor_heap_layouts[set].count = limits->storage_image_max_descriptors;
+                break;
+            case VK_DESCRIPTOR_TYPE_SAMPLER:
+                device->vk_descriptor_heap_layouts[set].count = limits->sampler_max_descriptors;
+                break;
+            default:
+                ERR("Unhandled descriptor type %#x.\n", device->vk_descriptor_heap_layouts[set].type);
+                break;
+        }
+
+        if (FAILED(hr = vkd3d_create_vk_descriptor_heap_layout(device, set)))
+        {
+            vkd3d_vk_descriptor_heap_layouts_cleanup(device);
+            return hr;
+        }
+    }
+
+    return S_OK;
+}
+
 static unsigned int get_spec_version(const VkExtensionProperties *extensions,
         unsigned int count, const char *extension_name)
 {
@@ -431,6 +494,7 @@ static void vkd3d_init_debug_report(struct vkd3d_instance *instance)
 
 static const struct vkd3d_debug_option vkd3d_config_options[] =
 {
+    {"virtual_heaps", VKD3D_CONFIG_FLAG_VIRTUAL_HEAPS}, /* always use virtual descriptor heaps */
     {"vk_debug", VKD3D_CONFIG_FLAG_VULKAN_DEBUG}, /* enable Vulkan debug extensions */
 };
 
@@ -690,10 +754,12 @@ struct vkd3d_physical_device_info
     VkPhysicalDeviceConditionalRenderingFeaturesEXT conditional_rendering_features;
     VkPhysicalDeviceDepthClipEnableFeaturesEXT depth_clip_features;
     VkPhysicalDeviceDescriptorIndexingFeaturesEXT descriptor_indexing_features;
+    VkPhysicalDeviceRobustness2FeaturesEXT robustness2_features;
     VkPhysicalDeviceShaderDemoteToHelperInvocationFeaturesEXT demote_features;
     VkPhysicalDeviceTexelBufferAlignmentFeaturesEXT texel_buffer_alignment_features;
     VkPhysicalDeviceTransformFeedbackFeaturesEXT xfb_features;
     VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT vertex_divisor_features;
+    VkPhysicalDeviceTimelineSemaphoreFeaturesKHR timeline_semaphore_features;
 
     VkPhysicalDeviceFeatures2 features2;
 };
@@ -706,9 +772,11 @@ static void vkd3d_physical_device_info_init(struct vkd3d_physical_device_info *i
     VkPhysicalDeviceVertexAttributeDivisorPropertiesEXT *vertex_divisor_properties;
     VkPhysicalDeviceTexelBufferAlignmentPropertiesEXT *buffer_alignment_properties;
     VkPhysicalDeviceDescriptorIndexingFeaturesEXT *descriptor_indexing_features;
+    VkPhysicalDeviceRobustness2FeaturesEXT *robustness2_features;
     VkPhysicalDeviceVertexAttributeDivisorFeaturesEXT *vertex_divisor_features;
     VkPhysicalDeviceTexelBufferAlignmentFeaturesEXT *buffer_alignment_features;
     VkPhysicalDeviceShaderDemoteToHelperInvocationFeaturesEXT *demote_features;
+    VkPhysicalDeviceTimelineSemaphoreFeaturesKHR *timeline_semaphore_features;
     VkPhysicalDeviceDepthClipEnableFeaturesEXT *depth_clip_features;
     VkPhysicalDeviceMaintenance3Properties *maintenance3_properties;
     VkPhysicalDeviceTransformFeedbackPropertiesEXT *xfb_properties;
@@ -720,6 +788,7 @@ static void vkd3d_physical_device_info_init(struct vkd3d_physical_device_info *i
     conditional_rendering_features = &info->conditional_rendering_features;
     depth_clip_features = &info->depth_clip_features;
     descriptor_indexing_features = &info->descriptor_indexing_features;
+    robustness2_features = &info->robustness2_features;
     descriptor_indexing_properties = &info->descriptor_indexing_properties;
     maintenance3_properties = &info->maintenance3_properties;
     demote_features = &info->demote_features;
@@ -727,6 +796,7 @@ static void vkd3d_physical_device_info_init(struct vkd3d_physical_device_info *i
     buffer_alignment_properties = &info->texel_buffer_alignment_properties;
     vertex_divisor_features = &info->vertex_divisor_features;
     vertex_divisor_properties = &info->vertex_divisor_properties;
+    timeline_semaphore_features = &info->timeline_semaphore_features;
     xfb_features = &info->xfb_features;
     xfb_properties = &info->xfb_properties;
 
@@ -738,6 +808,8 @@ static void vkd3d_physical_device_info_init(struct vkd3d_physical_device_info *i
     vk_prepend_struct(&info->features2, depth_clip_features);
     descriptor_indexing_features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_DESCRIPTOR_INDEXING_FEATURES_EXT;
     vk_prepend_struct(&info->features2, descriptor_indexing_features);
+    robustness2_features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_ROBUSTNESS_2_FEATURES_EXT;
+    vk_prepend_struct(&info->features2, robustness2_features);
     demote_features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_SHADER_DEMOTE_TO_HELPER_INVOCATION_FEATURES_EXT;
     vk_prepend_struct(&info->features2, demote_features);
     buffer_alignment_features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TEXEL_BUFFER_ALIGNMENT_FEATURES_EXT;
@@ -746,6 +818,8 @@ static void vkd3d_physical_device_info_init(struct vkd3d_physical_device_info *i
     vk_prepend_struct(&info->features2, xfb_features);
     vertex_divisor_features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_VERTEX_ATTRIBUTE_DIVISOR_FEATURES_EXT;
     vk_prepend_struct(&info->features2, vertex_divisor_features);
+    timeline_semaphore_features->sType = VK_STRUCTURE_TYPE_PHYSICAL_DEVICE_TIMELINE_SEMAPHORE_FEATURES_KHR;
+    vk_prepend_struct(&info->features2, timeline_semaphore_features);
 
     if (vulkan_info->KHR_get_physical_device_properties2)
         VK_CALL(vkGetPhysicalDeviceFeatures2KHR(physical_device, &info->features2));
@@ -1287,6 +1361,36 @@ static void vkd3d_device_descriptor_limits_init(struct vkd3d_device_descriptor_l
     limits->sampler_max_descriptors = min(device_limits->maxDescriptorSetSamplers, VKD3D_MAX_DESCRIPTOR_SET_SAMPLERS);
 }
 
+static void vkd3d_device_vk_heaps_descriptor_limits_init(struct vkd3d_device_descriptor_limits *limits,
+        const VkPhysicalDeviceDescriptorIndexingPropertiesEXT *properties)
+{
+    const unsigned int root_provision = D3D12_MAX_ROOT_COST / 2;
+    unsigned int srv_divisor = 1, uav_divisor = 1;
+
+    /* The total number of populated sampled image or storage image descriptors never exceeds the size of
+     * one set (or two sets if every UAV has a counter), but the total size of bound layouts will exceed
+     * device limits if each set size is maxDescriptorSet*, because of the D3D12 buffer + image allowance
+     * (and UAV counters). Breaking limits for layouts seems to work with RADV and Nvidia drivers at
+     * least, but let's try to stay within them if limits are high enough. */
+    if (properties->maxDescriptorSetUpdateAfterBindSampledImages >= (1u << 21))
+    {
+        srv_divisor = 2;
+        uav_divisor = properties->maxDescriptorSetUpdateAfterBindSampledImages >= (3u << 20) ? 3 : 2;
+    }
+
+    limits->uniform_buffer_max_descriptors = min(properties->maxDescriptorSetUpdateAfterBindUniformBuffers,
+            properties->maxPerStageDescriptorUpdateAfterBindUniformBuffers - root_provision);
+    limits->sampled_image_max_descriptors = min(properties->maxDescriptorSetUpdateAfterBindSampledImages,
+            properties->maxPerStageDescriptorUpdateAfterBindSampledImages / srv_divisor - root_provision);
+    limits->storage_buffer_max_descriptors = min(properties->maxDescriptorSetUpdateAfterBindStorageBuffers,
+            properties->maxPerStageDescriptorUpdateAfterBindStorageBuffers - root_provision);
+    limits->storage_image_max_descriptors = min(properties->maxDescriptorSetUpdateAfterBindStorageImages,
+            properties->maxPerStageDescriptorUpdateAfterBindStorageImages / uav_divisor - root_provision);
+    limits->sampler_max_descriptors = min(properties->maxDescriptorSetUpdateAfterBindSamplers,
+            properties->maxPerStageDescriptorUpdateAfterBindSamplers - root_provision);
+    limits->sampler_max_descriptors = min(limits->sampler_max_descriptors, VKD3D_MAX_DESCRIPTOR_SET_SAMPLERS);
+}
+
 static HRESULT vkd3d_init_device_caps(struct d3d12_device *device,
         const struct vkd3d_device_create_info *create_info,
         struct vkd3d_physical_device_info *physical_device_info,
@@ -1440,10 +1544,14 @@ static HRESULT vkd3d_init_device_caps(struct d3d12_device *device,
         vulkan_info->EXT_conditional_rendering = false;
     if (!physical_device_info->depth_clip_features.depthClipEnable)
         vulkan_info->EXT_depth_clip_enable = false;
+    if (!physical_device_info->robustness2_features.nullDescriptor)
+        vulkan_info->EXT_robustness2 = false;
     if (!physical_device_info->demote_features.shaderDemoteToHelperInvocation)
         vulkan_info->EXT_shader_demote_to_helper_invocation = false;
     if (!physical_device_info->texel_buffer_alignment_features.texelBufferAlignment)
         vulkan_info->EXT_texel_buffer_alignment = false;
+    if (!physical_device_info->timeline_semaphore_features.timelineSemaphore)
+        vulkan_info->KHR_timeline_semaphore = false;
 
     vulkan_info->texel_buffer_alignment_properties = physical_device_info->texel_buffer_alignment_properties;
 
@@ -1492,16 +1600,13 @@ static HRESULT vkd3d_init_device_caps(struct d3d12_device *device,
     features->shaderTessellationAndGeometryPointSize = VK_FALSE;
 
     descriptor_indexing = &physical_device_info->descriptor_indexing_features;
-    if (descriptor_indexing)
-    {
-        descriptor_indexing->shaderInputAttachmentArrayDynamicIndexing = VK_FALSE;
-        descriptor_indexing->shaderInputAttachmentArrayNonUniformIndexing = VK_FALSE;
+    descriptor_indexing->shaderInputAttachmentArrayDynamicIndexing = VK_FALSE;
+    descriptor_indexing->shaderInputAttachmentArrayNonUniformIndexing = VK_FALSE;
 
-        /* We do not use storage buffers currently. */
-        features->shaderStorageBufferArrayDynamicIndexing = VK_FALSE;
-        descriptor_indexing->shaderStorageBufferArrayNonUniformIndexing = VK_FALSE;
-        descriptor_indexing->descriptorBindingStorageBufferUpdateAfterBind = VK_FALSE;
-    }
+    /* We do not use storage buffers currently. */
+    features->shaderStorageBufferArrayDynamicIndexing = VK_FALSE;
+    descriptor_indexing->shaderStorageBufferArrayNonUniformIndexing = VK_FALSE;
+    descriptor_indexing->descriptorBindingStorageBufferUpdateAfterBind = VK_FALSE;
 
     if (vulkan_info->EXT_descriptor_indexing && descriptor_indexing
             && (descriptor_indexing->descriptorBindingUniformBufferUpdateAfterBind
@@ -1514,8 +1619,25 @@ static HRESULT vkd3d_init_device_caps(struct d3d12_device *device,
         features->robustBufferAccess = VK_FALSE;
     }
 
-    vkd3d_device_descriptor_limits_init(&vulkan_info->descriptor_limits,
-            &physical_device_info->properties2.properties.limits);
+    /* Select descriptor heap implementation. Forcing virtual heaps may be useful if
+     * a client allocates descriptor heaps too large for the Vulkan device, or the
+     * root signature cost exceeds the available push constant size. Virtual heaps
+     * use only enough descriptors for the descriptor tables of the currently bound
+     * root signature, and don't require a 32-bit push constant for each table. */
+    device->use_vk_heaps = vulkan_info->EXT_descriptor_indexing
+            && !(device->vkd3d_instance->config_flags & VKD3D_CONFIG_FLAG_VIRTUAL_HEAPS)
+            && descriptor_indexing->descriptorBindingUniformBufferUpdateAfterBind
+            && descriptor_indexing->descriptorBindingSampledImageUpdateAfterBind
+            && descriptor_indexing->descriptorBindingStorageImageUpdateAfterBind
+            && descriptor_indexing->descriptorBindingUniformTexelBufferUpdateAfterBind
+            && descriptor_indexing->descriptorBindingStorageTexelBufferUpdateAfterBind;
+
+    if (device->use_vk_heaps)
+        vkd3d_device_vk_heaps_descriptor_limits_init(&vulkan_info->descriptor_limits,
+                &physical_device_info->descriptor_indexing_properties);
+    else
+        vkd3d_device_descriptor_limits_init(&vulkan_info->descriptor_limits,
+                &physical_device_info->properties2.properties.limits);
 
     return S_OK;
 }
@@ -2504,12 +2626,12 @@ static ULONG STDMETHODCALLTYPE d3d12_device_Release(ID3D12Device *iface)
         vkd3d_private_store_destroy(&device->private_store);
 
         vkd3d_cleanup_format_info(device);
+        vkd3d_vk_descriptor_heap_layouts_cleanup(device);
         vkd3d_uav_clear_state_cleanup(&device->uav_clear_state, device);
         vkd3d_destroy_null_resources(&device->null_resources, device);
         vkd3d_gpu_va_allocator_cleanup(&device->gpu_va_allocator);
         vkd3d_gpu_descriptor_allocator_cleanup(&device->gpu_descriptor_allocator);
         vkd3d_render_pass_cache_cleanup(&device->render_pass_cache, device);
-        vkd3d_fence_worker_stop(&device->fence_worker, device);
         d3d12_device_destroy_pipeline_cache(device);
         d3d12_device_destroy_vkd3d_queues(device);
         for (i = 0; i < ARRAY_SIZE(device->desc_mutex); ++i)
@@ -3406,6 +3528,134 @@ static void STDMETHODCALLTYPE d3d12_device_CreateSampler(ID3D12Device *iface,
     d3d12_desc_write_atomic(d3d12_desc_from_cpu_handle(descriptor), &tmp, device);
 }
 
+static void flush_desc_writes(struct d3d12_desc_copy_location locations[][VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE],
+        struct d3d12_desc_copy_info *infos, struct d3d12_descriptor_heap *descriptor_heap, struct d3d12_device *device)
+{
+    enum vkd3d_vk_descriptor_set_index set;
+    for (set = 0; set < VKD3D_SET_INDEX_COUNT; ++set)
+    {
+        if (!infos[set].count)
+            continue;
+        d3d12_desc_copy_vk_heap_range(locations[set], &infos[set], descriptor_heap, set, device);
+        infos[set].count = 0;
+        infos[set].uav_counter = false;
+    }
+}
+
+static void d3d12_desc_buffered_copy_atomic(struct d3d12_desc *dst, const struct d3d12_desc *src,
+        struct d3d12_desc_copy_location locations[][VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE],
+        struct d3d12_desc_copy_info *infos, struct d3d12_descriptor_heap *descriptor_heap, struct d3d12_device *device)
+{
+    struct d3d12_desc_copy_location *location;
+    enum vkd3d_vk_descriptor_set_index set;
+    struct vkd3d_mutex *mutex;
+
+    mutex = d3d12_device_get_descriptor_mutex(device, src);
+    vkd3d_mutex_lock(mutex);
+
+    if (src->magic == VKD3D_DESCRIPTOR_MAGIC_FREE)
+    {
+        /* Source must be unlocked first, and therefore can't be used as a null source. */
+        static const struct d3d12_desc null = {0};
+        vkd3d_mutex_unlock(mutex);
+        d3d12_desc_write_atomic(dst, &null, device);
+        return;
+    }
+
+    set = vkd3d_vk_descriptor_set_index_from_vk_descriptor_type(src->vk_descriptor_type);
+    location = &locations[set][infos[set].count++];
+
+    location->src = *src;
+
+    if (location->src.magic & VKD3D_DESCRIPTOR_MAGIC_HAS_VIEW)
+        vkd3d_view_incref(location->src.u.view_info.view);
+
+    vkd3d_mutex_unlock(mutex);
+
+    infos[set].uav_counter |= (location->src.magic == VKD3D_DESCRIPTOR_MAGIC_UAV)
+            && !!location->src.u.view_info.view->vk_counter_view;
+    location->dst = dst;
+
+    if (infos[set].count == ARRAY_SIZE(locations[0]))
+    {
+        d3d12_desc_copy_vk_heap_range(locations[set], &infos[set], descriptor_heap, set, device);
+        infos[set].count = 0;
+        infos[set].uav_counter = false;
+    }
+}
+
+/* Some games, e.g. Control, copy a large number of descriptors per frame, so the
+ * speed of this function is critical. */
+static void d3d12_device_vk_heaps_copy_descriptors(struct d3d12_device *device,
+        UINT dst_descriptor_range_count, const D3D12_CPU_DESCRIPTOR_HANDLE *dst_descriptor_range_offsets,
+        const UINT *dst_descriptor_range_sizes,
+        UINT src_descriptor_range_count, const D3D12_CPU_DESCRIPTOR_HANDLE *src_descriptor_range_offsets,
+        const UINT *src_descriptor_range_sizes)
+{
+    struct d3d12_desc_copy_location locations[VKD3D_SET_INDEX_COUNT][VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE];
+    unsigned int dst_range_idx, dst_idx, src_range_idx, src_idx;
+    /* The locations array is relatively large, and often mostly empty. Keeping these
+     * values together in a separate array will likely result in fewer cache misses. */
+    struct d3d12_desc_copy_info infos[VKD3D_SET_INDEX_COUNT];
+    struct d3d12_descriptor_heap *descriptor_heap = NULL;
+    const struct d3d12_desc *src, *heap_base, *heap_end;
+    unsigned int dst_range_size, src_range_size;
+    struct d3d12_desc *dst;
+
+    descriptor_heap = vkd3d_gpu_descriptor_allocator_heap_from_descriptor(&device->gpu_descriptor_allocator,
+            d3d12_desc_from_cpu_handle(dst_descriptor_range_offsets[0]));
+    heap_base = (const struct d3d12_desc *)descriptor_heap->descriptors;
+    heap_end = heap_base + descriptor_heap->desc.NumDescriptors;
+
+    memset(infos, 0, sizeof(infos));
+    dst_range_idx = dst_idx = 0;
+    src_range_idx = src_idx = 0;
+    while (dst_range_idx < dst_descriptor_range_count && src_range_idx < src_descriptor_range_count)
+    {
+        dst_range_size = dst_descriptor_range_sizes ? dst_descriptor_range_sizes[dst_range_idx] : 1;
+        src_range_size = src_descriptor_range_sizes ? src_descriptor_range_sizes[src_range_idx] : 1;
+
+        dst = d3d12_desc_from_cpu_handle(dst_descriptor_range_offsets[dst_range_idx]);
+        src = d3d12_desc_from_cpu_handle(src_descriptor_range_offsets[src_range_idx]);
+
+        if (dst < heap_base || dst >= heap_end)
+        {
+            flush_desc_writes(locations, infos, descriptor_heap, device);
+            descriptor_heap = vkd3d_gpu_descriptor_allocator_heap_from_descriptor(&device->gpu_descriptor_allocator,
+                    dst);
+            heap_base = (const struct d3d12_desc *)descriptor_heap->descriptors;
+            heap_end = heap_base + descriptor_heap->desc.NumDescriptors;
+        }
+
+        for (; dst_idx < dst_range_size && src_idx < src_range_size; src_idx++, dst_idx++)
+        {
+            /* We don't need to lock either descriptor for the identity check. The descriptor
+             * mutex is only intended to prevent use-after-free of the vkd3d_view caused by a
+             * race condition in the calling app. It is unnecessary to protect this test as it's
+             * the app's race condition, not ours. */
+            if (dst[dst_idx].magic == src[src_idx].magic && (dst[dst_idx].magic & VKD3D_DESCRIPTOR_MAGIC_HAS_VIEW)
+                    && dst[dst_idx].u.view_info.written_serial_id == src[src_idx].u.view_info.view->serial_id)
+                continue;
+            d3d12_desc_buffered_copy_atomic(&dst[dst_idx], &src[src_idx], locations, infos, descriptor_heap, device);
+        }
+
+        if (dst_idx >= dst_range_size)
+        {
+            ++dst_range_idx;
+            dst_idx = 0;
+        }
+        if (src_idx >= src_range_size)
+        {
+            ++src_range_idx;
+            src_idx = 0;
+        }
+    }
+
+    flush_desc_writes(locations, infos, descriptor_heap, device);
+}
+
+#define VKD3D_DESCRIPTOR_OPTIMISED_COPY_MIN_COUNT 8
+
 static void STDMETHODCALLTYPE d3d12_device_CopyDescriptors(ID3D12Device *iface,
         UINT dst_descriptor_range_count, const D3D12_CPU_DESCRIPTOR_HANDLE *dst_descriptor_range_offsets,
         const UINT *dst_descriptor_range_sizes,
@@ -3434,6 +3684,18 @@ static void STDMETHODCALLTYPE d3d12_device_CopyDescriptors(ID3D12Device *iface,
         return;
     }
 
+    if (!dst_descriptor_range_count)
+        return;
+
+    if (device->use_vk_heaps && (dst_descriptor_range_count > 1 || (dst_descriptor_range_sizes
+            && dst_descriptor_range_sizes[0] >= VKD3D_DESCRIPTOR_OPTIMISED_COPY_MIN_COUNT)))
+    {
+        d3d12_device_vk_heaps_copy_descriptors(device, dst_descriptor_range_count, dst_descriptor_range_offsets,
+                dst_descriptor_range_sizes, src_descriptor_range_count, src_descriptor_range_offsets,
+                src_descriptor_range_sizes);
+        return;
+    }
+
     dst_range_idx = dst_idx = 0;
     src_range_idx = src_idx = 0;
     while (dst_range_idx < dst_descriptor_range_count && src_range_idx < src_descriptor_range_count)
@@ -3470,6 +3732,17 @@ static void STDMETHODCALLTYPE d3d12_device_CopyDescriptorsSimple(ID3D12Device *i
             iface, descriptor_count, dst_descriptor_range_offset.ptr, src_descriptor_range_offset.ptr,
             descriptor_heap_type);
 
+    if (descriptor_count >= VKD3D_DESCRIPTOR_OPTIMISED_COPY_MIN_COUNT)
+    {
+        struct d3d12_device *device = impl_from_ID3D12Device(iface);
+        if (device->use_vk_heaps)
+        {
+            d3d12_device_vk_heaps_copy_descriptors(device, 1, &dst_descriptor_range_offset,
+                    &descriptor_count, 1, &src_descriptor_range_offset, &descriptor_count);
+            return;
+        }
+    }
+
     d3d12_device_CopyDescriptors(iface, 1, &dst_descriptor_range_offset, &descriptor_count,
             1, &src_descriptor_range_offset, &descriptor_count, descriptor_heap_type);
 }
@@ -3736,7 +4009,7 @@ static HRESULT STDMETHODCALLTYPE d3d12_device_CreateFence(ID3D12Device *iface,
     struct d3d12_fence *object;
     HRESULT hr;
 
-    TRACE("iface %p, intial_value %#"PRIx64", flags %#x, riid %s, fence %p.\n",
+    TRACE("iface %p, initial_value %#"PRIx64", flags %#x, riid %s, fence %p.\n",
             iface, initial_value, flags, debugstr_guid(riid), fence);
 
     if (FAILED(hr = d3d12_fence_create(device, initial_value, flags, &object)))
@@ -3993,11 +4266,8 @@ static HRESULT d3d12_device_init(struct d3d12_device *device,
     if (FAILED(hr = vkd3d_private_store_init(&device->private_store)))
         goto out_free_pipeline_cache;
 
-    if (FAILED(hr = vkd3d_fence_worker_start(&device->fence_worker, device)))
-        goto out_free_private_store;
-
     if (FAILED(hr = vkd3d_init_format_info(device)))
-        goto out_stop_fence_worker;
+        goto out_free_private_store;
 
     if (FAILED(hr = vkd3d_init_null_resources(&device->null_resources, device)))
         goto out_cleanup_format_info;
@@ -4005,11 +4275,16 @@ static HRESULT d3d12_device_init(struct d3d12_device *device,
     if (FAILED(hr = vkd3d_uav_clear_state_init(&device->uav_clear_state, device)))
         goto out_destroy_null_resources;
 
+    if (FAILED(hr = vkd3d_vk_descriptor_heap_layouts_init(device)))
+        goto out_cleanup_uav_clear_state;
+
     vkd3d_render_pass_cache_init(&device->render_pass_cache);
     vkd3d_gpu_descriptor_allocator_init(&device->gpu_descriptor_allocator);
     vkd3d_gpu_va_allocator_init(&device->gpu_va_allocator);
     vkd3d_time_domains_init(device);
 
+    device->blocked_queue_count = 0;
+
     for (i = 0; i < ARRAY_SIZE(device->desc_mutex); ++i)
         vkd3d_mutex_init(&device->desc_mutex[i]);
 
@@ -4020,12 +4295,12 @@ static HRESULT d3d12_device_init(struct d3d12_device *device,
 
     return S_OK;
 
+out_cleanup_uav_clear_state:
+    vkd3d_uav_clear_state_cleanup(&device->uav_clear_state, device);
 out_destroy_null_resources:
     vkd3d_destroy_null_resources(&device->null_resources, device);
 out_cleanup_format_info:
     vkd3d_cleanup_format_info(device);
-out_stop_fence_worker:
-    vkd3d_fence_worker_stop(&device->fence_worker, device);
 out_free_private_store:
     vkd3d_private_store_destroy(&device->private_store);
 out_free_pipeline_cache:
diff --git a/libs/vkd3d/libs/vkd3d/resource.c b/libs/vkd3d/libs/vkd3d/resource.c
index 4c48e22e1..68c28cd1c 100644
--- a/libs/vkd3d/libs/vkd3d/resource.c
+++ b/libs/vkd3d/libs/vkd3d/resource.c
@@ -22,6 +22,8 @@
 #define VKD3D_NULL_BUFFER_SIZE 16
 #define VKD3D_NULL_VIEW_FORMAT DXGI_FORMAT_R8G8B8A8_UNORM
 
+LONG64 object_global_serial_id;
+
 static inline bool is_cpu_accessible_heap(const D3D12_HEAP_PROPERTIES *properties)
 {
     if (properties->Type == D3D12_HEAP_TYPE_DEFAULT)
@@ -2085,6 +2087,7 @@ static struct vkd3d_view *vkd3d_view_create(enum vkd3d_view_type type)
     {
         view->refcount = 1;
         view->type = type;
+        view->serial_id = InterlockedIncrement64(&object_global_serial_id);
         view->vk_counter_view = VK_NULL_HANDLE;
     }
     return view;
@@ -2128,6 +2131,183 @@ void vkd3d_view_decref(struct vkd3d_view *view, struct d3d12_device *device)
         vkd3d_view_destroy(view, device);
 }
 
+/* TODO: write null descriptors to all applicable sets (invalid behaviour workaround). */
+static void d3d12_descriptor_heap_write_vk_descriptor_range(struct d3d12_descriptor_heap_vk_set *descriptor_set,
+        struct d3d12_desc_copy_location *locations, unsigned int write_count)
+{
+    unsigned int i, info_index = 0, write_index = 0;
+
+    switch (locations[0].src.vk_descriptor_type)
+    {
+        case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+            for (; write_index < write_count; ++write_index)
+            {
+                descriptor_set->vk_descriptor_writes[write_index].pBufferInfo = &descriptor_set->vk_buffer_infos[info_index];
+                for (i = 0; i < descriptor_set->vk_descriptor_writes[write_index].descriptorCount; ++i, ++info_index)
+                    descriptor_set->vk_buffer_infos[info_index] = locations[info_index].src.u.vk_cbv_info;
+            }
+            break;
+        case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+        case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+            for (; write_index < write_count; ++write_index)
+            {
+                descriptor_set->vk_descriptor_writes[write_index].pImageInfo = &descriptor_set->vk_image_infos[info_index];
+                for (i = 0; i < descriptor_set->vk_descriptor_writes[write_index].descriptorCount; ++i, ++info_index)
+                    descriptor_set->vk_image_infos[info_index].imageView = locations[info_index].src.u.view_info.view->u.vk_image_view;
+            }
+            break;
+        case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+        case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+            for (; write_index < write_count; ++write_index)
+            {
+                descriptor_set->vk_descriptor_writes[write_index].pTexelBufferView = &descriptor_set->vk_buffer_views[info_index];
+                for (i = 0; i < descriptor_set->vk_descriptor_writes[write_index].descriptorCount; ++i, ++info_index)
+                    descriptor_set->vk_buffer_views[info_index] = locations[info_index].src.u.view_info.view->u.vk_buffer_view;
+            }
+            break;
+        case VK_DESCRIPTOR_TYPE_SAMPLER:
+            for (; write_index < write_count; ++write_index)
+            {
+                descriptor_set->vk_descriptor_writes[write_index].pImageInfo = &descriptor_set->vk_image_infos[info_index];
+                for (i = 0; i < descriptor_set->vk_descriptor_writes[write_index].descriptorCount; ++i, ++info_index)
+                    descriptor_set->vk_image_infos[info_index].sampler = locations[info_index].src.u.view_info.view->u.vk_sampler;
+            }
+            break;
+        default:
+            ERR("Unhandled descriptor type %#x.\n", locations[0].src.vk_descriptor_type);
+            break;
+    }
+}
+
+static void d3d12_desc_write_vk_heap_null_descriptor(struct d3d12_descriptor_heap *descriptor_heap,
+        uint32_t dst_array_element, const struct d3d12_device *device)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    struct d3d12_descriptor_heap_vk_set *descriptor_set;
+    VkBufferView vk_buffer_view = VK_NULL_HANDLE;
+    enum vkd3d_vk_descriptor_set_index i;
+    VkDescriptorBufferInfo vk_cbv_info;
+
+    vk_cbv_info.buffer = VK_NULL_HANDLE;
+    vk_cbv_info.offset = 0;
+    vk_cbv_info.range = VK_WHOLE_SIZE;
+
+    /* Binding a shader with the wrong null descriptor type works in Windows.
+     * To support that here we must write one to all applicable Vulkan sets. */
+    for (i = VKD3D_SET_INDEX_UNIFORM_BUFFER; i <= VKD3D_SET_INDEX_STORAGE_IMAGE; ++i)
+    {
+        descriptor_set = &descriptor_heap->vk_descriptor_sets[i];
+        descriptor_set->vk_descriptor_writes[0].dstArrayElement = dst_array_element;
+        descriptor_set->vk_descriptor_writes[0].descriptorCount = 1;
+        switch (i)
+        {
+            case VKD3D_SET_INDEX_UNIFORM_BUFFER:
+                descriptor_set->vk_descriptor_writes[0].pBufferInfo = &vk_cbv_info;
+                break;
+            case VKD3D_SET_INDEX_SAMPLED_IMAGE:
+            case VKD3D_SET_INDEX_STORAGE_IMAGE:
+                descriptor_set->vk_image_infos[0].imageView = VK_NULL_HANDLE;
+                break;
+            case VKD3D_SET_INDEX_UNIFORM_TEXEL_BUFFER:
+            case VKD3D_SET_INDEX_STORAGE_TEXEL_BUFFER:
+                descriptor_set->vk_descriptor_writes[0].pTexelBufferView = &vk_buffer_view;
+                break;
+            default:
+                assert(false);
+                break;
+        }
+        VK_CALL(vkUpdateDescriptorSets(device->vk_device, 1, descriptor_set->vk_descriptor_writes, 0, NULL));
+    }
+}
+
+/* dst and src contain the same data unless another thread overwrites dst. The array index is
+ * calculated from dst, and src is thread safe. */
+static void d3d12_desc_write_vk_heap(const struct d3d12_desc *dst, const struct d3d12_desc *src,
+        struct d3d12_device *device)
+{
+    struct d3d12_descriptor_heap_vk_set *descriptor_set;
+    struct d3d12_descriptor_heap *descriptor_heap;
+    const struct vkd3d_vk_device_procs *vk_procs;
+    bool is_null = false;
+
+    descriptor_heap = vkd3d_gpu_descriptor_allocator_heap_from_descriptor(&device->gpu_descriptor_allocator, dst);
+    descriptor_set = &descriptor_heap->vk_descriptor_sets[vkd3d_vk_descriptor_set_index_from_vk_descriptor_type(
+            src->vk_descriptor_type)];
+    vk_procs = &device->vk_procs;
+
+    vkd3d_mutex_lock(&descriptor_heap->vk_sets_mutex);
+
+    descriptor_set->vk_descriptor_writes[0].dstArrayElement = dst
+            - (const struct d3d12_desc *)descriptor_heap->descriptors;
+    descriptor_set->vk_descriptor_writes[0].descriptorCount = 1;
+    switch (src->vk_descriptor_type)
+    {
+        case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+            descriptor_set->vk_descriptor_writes[0].pBufferInfo = &src->u.vk_cbv_info;
+            is_null = !src->u.vk_cbv_info.buffer;
+            break;
+        case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+        case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+            is_null = !(descriptor_set->vk_image_infos[0].imageView = src->u.view_info.view->u.vk_image_view);
+            break;
+        case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+        case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+            descriptor_set->vk_descriptor_writes[0].pTexelBufferView = &src->u.view_info.view->u.vk_buffer_view;
+            is_null = !src->u.view_info.view->u.vk_buffer_view;
+            break;
+        case VK_DESCRIPTOR_TYPE_SAMPLER:
+            descriptor_set->vk_image_infos[0].sampler = src->u.view_info.view->u.vk_sampler;
+            break;
+        default:
+            ERR("Unhandled descriptor type %#x.\n", src->vk_descriptor_type);
+            break;
+    }
+    if (is_null && device->vk_info.EXT_robustness2)
+    {
+        d3d12_desc_write_vk_heap_null_descriptor(descriptor_heap,
+                descriptor_set->vk_descriptor_writes[0].dstArrayElement, device);
+        vkd3d_mutex_unlock(&descriptor_heap->vk_sets_mutex);
+        return;
+    }
+
+    VK_CALL(vkUpdateDescriptorSets(device->vk_device, 1, descriptor_set->vk_descriptor_writes, 0, NULL));
+
+    if (src->magic == VKD3D_DESCRIPTOR_MAGIC_UAV && src->u.view_info.view->vk_counter_view)
+    {
+        descriptor_set = &descriptor_heap->vk_descriptor_sets[VKD3D_SET_INDEX_UAV_COUNTER];
+        descriptor_set->vk_descriptor_writes[0].dstArrayElement = dst
+            - (const struct d3d12_desc *)descriptor_heap->descriptors;
+        descriptor_set->vk_descriptor_writes[0].descriptorCount = 1;
+        descriptor_set->vk_descriptor_writes[0].pTexelBufferView = &src->u.view_info.view->vk_counter_view;
+        VK_CALL(vkUpdateDescriptorSets(device->vk_device, 1, descriptor_set->vk_descriptor_writes, 0, NULL));
+    }
+
+    vkd3d_mutex_unlock(&descriptor_heap->vk_sets_mutex);
+}
+
+static void d3d12_desc_write_atomic_d3d12_only(struct d3d12_desc *dst, const struct d3d12_desc *src, struct d3d12_device *device)
+{
+    struct vkd3d_view *defunct_view;
+    struct vkd3d_mutex *mutex;
+
+    mutex = d3d12_device_get_descriptor_mutex(device, dst);
+    vkd3d_mutex_lock(mutex);
+
+    if (!(dst->magic & VKD3D_DESCRIPTOR_MAGIC_HAS_VIEW) || InterlockedDecrement(&dst->u.view_info.view->refcount))
+    {
+        *dst = *src;
+        vkd3d_mutex_unlock(mutex);
+        return;
+    }
+
+    defunct_view = dst->u.view_info.view;
+    *dst = *src;
+    vkd3d_mutex_unlock(mutex);
+
+    /* Destroy the view after unlocking to reduce wait time. */
+    vkd3d_view_destroy(defunct_view, device);
+}
+
 void d3d12_desc_write_atomic(struct d3d12_desc *dst, const struct d3d12_desc *src,
         struct d3d12_device *device)
 {
@@ -2139,8 +2319,8 @@ void d3d12_desc_write_atomic(struct d3d12_desc *dst, const struct d3d12_desc *sr
 
     /* Nothing to do for VKD3D_DESCRIPTOR_MAGIC_CBV. */
     if ((dst->magic & VKD3D_DESCRIPTOR_MAGIC_HAS_VIEW)
-            && !InterlockedDecrement(&dst->u.view->refcount))
-        defunct_view = dst->u.view;
+            && !InterlockedDecrement(&dst->u.view_info.view->refcount))
+        defunct_view = dst->u.view_info.view;
 
     *dst = *src;
 
@@ -2149,6 +2329,9 @@ void d3d12_desc_write_atomic(struct d3d12_desc *dst, const struct d3d12_desc *sr
     /* Destroy the view after unlocking to reduce wait time. */
     if (defunct_view)
         vkd3d_view_destroy(defunct_view, device);
+
+    if (device->use_vk_heaps && dst->magic)
+        d3d12_desc_write_vk_heap(dst, src, device);
 }
 
 static void d3d12_desc_destroy(struct d3d12_desc *descriptor, struct d3d12_device *device)
@@ -2158,6 +2341,56 @@ static void d3d12_desc_destroy(struct d3d12_desc *descriptor, struct d3d12_devic
     d3d12_desc_write_atomic(descriptor, &null_desc, device);
 }
 
+void d3d12_desc_copy_vk_heap_range(struct d3d12_desc_copy_location *locations, const struct d3d12_desc_copy_info *info,
+        struct d3d12_descriptor_heap *descriptor_heap, enum vkd3d_vk_descriptor_set_index set,
+        struct d3d12_device *device)
+{
+    struct d3d12_descriptor_heap_vk_set *descriptor_set = &descriptor_heap->vk_descriptor_sets[set];
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    unsigned int i, write_count;
+
+    vkd3d_mutex_lock(&descriptor_heap->vk_sets_mutex);
+
+    for (i = 0, write_count = 0; i < info->count; ++i)
+    {
+        d3d12_desc_write_atomic_d3d12_only(locations[i].dst, &locations[i].src, device);
+
+        if (i && locations[i].dst == locations[i - 1].dst + 1)
+        {
+            ++descriptor_set->vk_descriptor_writes[write_count - 1].descriptorCount;
+            continue;
+        }
+        descriptor_set->vk_descriptor_writes[write_count].dstArrayElement = locations[i].dst
+                - (const struct d3d12_desc *)descriptor_heap->descriptors;
+        descriptor_set->vk_descriptor_writes[write_count++].descriptorCount = 1;
+    }
+    d3d12_descriptor_heap_write_vk_descriptor_range(descriptor_set, locations, write_count);
+    /* We could pass a VkCopyDescriptorSet array instead, but that would require also storing a src array index
+     * for each location, which means querying the src descriptor heap. Contiguous copies require contiguous src
+     * descriptors as well as dst, which is less likely to occur. And client race conditions may break it. */
+    VK_CALL(vkUpdateDescriptorSets(device->vk_device, write_count, descriptor_set->vk_descriptor_writes, 0, NULL));
+
+    if (!info->uav_counter)
+        goto done;
+
+    descriptor_set = &descriptor_heap->vk_descriptor_sets[VKD3D_SET_INDEX_UAV_COUNTER];
+
+    for (i = 0, write_count = 0; i < info->count; ++i)
+    {
+        if (!locations[i].src.u.view_info.view->vk_counter_view)
+            continue;
+        descriptor_set->vk_buffer_views[write_count] = locations[i].src.u.view_info.view->vk_counter_view;
+        descriptor_set->vk_descriptor_writes[write_count].pTexelBufferView = &descriptor_set->vk_buffer_views[write_count];
+        descriptor_set->vk_descriptor_writes[write_count].dstArrayElement = locations[i].dst
+                - (const struct d3d12_desc *)descriptor_heap->descriptors;
+        descriptor_set->vk_descriptor_writes[write_count++].descriptorCount = 1;
+    }
+    VK_CALL(vkUpdateDescriptorSets(device->vk_device, write_count, descriptor_set->vk_descriptor_writes, 0, NULL));
+
+done:
+    vkd3d_mutex_unlock(&descriptor_heap->vk_sets_mutex);
+}
+
 void d3d12_desc_copy(struct d3d12_desc *dst, const struct d3d12_desc *src,
         struct d3d12_device *device)
 {
@@ -2172,7 +2405,7 @@ void d3d12_desc_copy(struct d3d12_desc *dst, const struct d3d12_desc *src,
     vkd3d_mutex_lock(mutex);
 
     if (src->magic & VKD3D_DESCRIPTOR_MAGIC_HAS_VIEW)
-        vkd3d_view_incref(src->u.view);
+        vkd3d_view_incref(src->u.view_info.view);
 
     tmp = *src;
 
@@ -2243,10 +2476,10 @@ bool vkd3d_create_buffer_view(struct d3d12_device *device, VkBuffer vk_buffer, c
         VkDeviceSize offset, VkDeviceSize size, struct vkd3d_view **view)
 {
     const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkBufferView vk_view = VK_NULL_HANDLE;
     struct vkd3d_view *object;
-    VkBufferView vk_view;
 
-    if (!vkd3d_create_vk_buffer_view(device, vk_buffer, format, offset, size, &vk_view))
+    if (vk_buffer && !vkd3d_create_vk_buffer_view(device, vk_buffer, format, offset, size, &vk_view))
         return false;
 
     if (!(object = vkd3d_view_create(VKD3D_VIEW_TYPE_BUFFER)))
@@ -2531,28 +2764,31 @@ bool vkd3d_create_texture_view(struct d3d12_device *device, VkImage vk_image,
     const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
     const struct vkd3d_format *format = desc->format;
     struct VkImageViewCreateInfo view_desc;
+    VkImageView vk_view = VK_NULL_HANDLE;
     struct vkd3d_view *object;
-    VkImageView vk_view;
     VkResult vr;
 
-    view_desc.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
-    view_desc.pNext = NULL;
-    view_desc.flags = 0;
-    view_desc.image = vk_image;
-    view_desc.viewType = desc->view_type;
-    view_desc.format = format->vk_format;
-    vkd3d_set_view_swizzle_for_format(&view_desc.components, format, desc->allowed_swizzle);
-    if (desc->allowed_swizzle)
-        vk_component_mapping_compose(&view_desc.components, &desc->components);
-    view_desc.subresourceRange.aspectMask = desc->vk_image_aspect;
-    view_desc.subresourceRange.baseMipLevel = desc->miplevel_idx;
-    view_desc.subresourceRange.levelCount = desc->miplevel_count;
-    view_desc.subresourceRange.baseArrayLayer = desc->layer_idx;
-    view_desc.subresourceRange.layerCount = desc->layer_count;
-    if ((vr = VK_CALL(vkCreateImageView(device->vk_device, &view_desc, NULL, &vk_view))) < 0)
-    {
-        WARN("Failed to create Vulkan image view, vr %d.\n", vr);
-        return false;
+    if (vk_image)
+    {
+        view_desc.sType = VK_STRUCTURE_TYPE_IMAGE_VIEW_CREATE_INFO;
+        view_desc.pNext = NULL;
+        view_desc.flags = 0;
+        view_desc.image = vk_image;
+        view_desc.viewType = desc->view_type;
+        view_desc.format = format->vk_format;
+        vkd3d_set_view_swizzle_for_format(&view_desc.components, format, desc->allowed_swizzle);
+        if (desc->allowed_swizzle)
+            vk_component_mapping_compose(&view_desc.components, &desc->components);
+        view_desc.subresourceRange.aspectMask = desc->vk_image_aspect;
+        view_desc.subresourceRange.baseMipLevel = desc->miplevel_idx;
+        view_desc.subresourceRange.levelCount = desc->miplevel_count;
+        view_desc.subresourceRange.baseArrayLayer = desc->layer_idx;
+        view_desc.subresourceRange.layerCount = desc->layer_count;
+        if ((vr = VK_CALL(vkCreateImageView(device->vk_device, &view_desc, NULL, &vk_view))) < 0)
+        {
+            WARN("Failed to create Vulkan image view, vr %d.\n", vr);
+            return false;
+        }
     }
 
     if (!(object = vkd3d_view_create(VKD3D_VIEW_TYPE_IMAGE)))
@@ -2602,7 +2838,7 @@ void d3d12_desc_create_cbv(struct d3d12_desc *descriptor,
         /* NULL descriptor */
         buffer_info->buffer = device->null_resources.vk_buffer;
         buffer_info->offset = 0;
-        buffer_info->range = VKD3D_NULL_BUFFER_SIZE;
+        buffer_info->range = VK_WHOLE_SIZE;
     }
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_CBV;
@@ -2635,7 +2871,8 @@ static void vkd3d_create_null_srv(struct d3d12_desc *descriptor,
     switch (desc->ViewDimension)
     {
         case D3D12_SRV_DIMENSION_BUFFER:
-            WARN("Creating NULL buffer SRV %#x.\n", desc->Format);
+            if (!device->vk_info.EXT_robustness2)
+                WARN("Creating NULL buffer SRV %#x.\n", desc->Format);
 
             if (vkd3d_create_buffer_view(device, null_resources->vk_buffer,
                     vkd3d_get_format(device, DXGI_FORMAT_R32_UINT, false),
@@ -2643,7 +2880,8 @@ static void vkd3d_create_null_srv(struct d3d12_desc *descriptor,
             {
                 descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_SRV;
                 descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER;
-                descriptor->u.view = view;
+                descriptor->u.view_info.view = view;
+                descriptor->u.view_info.written_serial_id = view->serial_id;
             }
             return;
 
@@ -2657,11 +2895,19 @@ static void vkd3d_create_null_srv(struct d3d12_desc *descriptor,
             break;
 
         default:
+            if (device->vk_info.EXT_robustness2)
+            {
+                vk_image = VK_NULL_HANDLE;
+                /* view_type is not used for Vulkan null descriptors, but make it valid. */
+                vkd3d_desc.view_type = VK_IMAGE_VIEW_TYPE_2D;
+                break;
+            }
             FIXME("Unhandled view dimension %#x.\n", desc->ViewDimension);
             return;
     }
 
-    WARN("Creating NULL SRV %#x.\n", desc->ViewDimension);
+    if (!device->vk_info.EXT_robustness2)
+        WARN("Creating NULL SRV %#x.\n", desc->ViewDimension);
 
     vkd3d_desc.format = vkd3d_get_format(device, VKD3D_NULL_VIEW_FORMAT, false);
     vkd3d_desc.miplevel_idx = 0;
@@ -2679,7 +2925,8 @@ static void vkd3d_create_null_srv(struct d3d12_desc *descriptor,
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_SRV;
     descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE;
-    descriptor->u.view = view;
+    descriptor->u.view_info.view = view;
+    descriptor->u.view_info.written_serial_id = view->serial_id;
 }
 
 static void vkd3d_create_buffer_srv(struct d3d12_desc *descriptor,
@@ -2709,7 +2956,8 @@ static void vkd3d_create_buffer_srv(struct d3d12_desc *descriptor,
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_SRV;
     descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER;
-    descriptor->u.view = view;
+    descriptor->u.view_info.view = view;
+    descriptor->u.view_info.written_serial_id = view->serial_id;
 }
 
 static VkImageAspectFlags vk_image_aspect_flags_from_d3d12_plane_slice(const struct vkd3d_format *format,
@@ -2839,7 +3087,8 @@ void d3d12_desc_create_srv(struct d3d12_desc *descriptor,
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_SRV;
     descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE;
-    descriptor->u.view = view;
+    descriptor->u.view_info.view = view;
+    descriptor->u.view_info.written_serial_id = view->serial_id;
 }
 
 static unsigned int vkd3d_view_flags_from_d3d12_buffer_uav_flags(D3D12_BUFFER_UAV_FLAGS flags)
@@ -2868,7 +3117,8 @@ static void vkd3d_create_null_uav(struct d3d12_desc *descriptor,
     switch (desc->ViewDimension)
     {
         case D3D12_UAV_DIMENSION_BUFFER:
-            WARN("Creating NULL buffer UAV %#x.\n", desc->Format);
+            if (!device->vk_info.EXT_robustness2)
+                WARN("Creating NULL buffer UAV %#x.\n", desc->Format);
 
             if (vkd3d_create_buffer_view(device, null_resources->vk_storage_buffer,
                     vkd3d_get_format(device, DXGI_FORMAT_R32_UINT, false),
@@ -2876,7 +3126,8 @@ static void vkd3d_create_null_uav(struct d3d12_desc *descriptor,
             {
                 descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_UAV;
                 descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER;
-                descriptor->u.view = view;
+                descriptor->u.view_info.view = view;
+                descriptor->u.view_info.written_serial_id = view->serial_id;
             }
             return;
 
@@ -2890,11 +3141,19 @@ static void vkd3d_create_null_uav(struct d3d12_desc *descriptor,
             break;
 
         default:
+            if (device->vk_info.EXT_robustness2)
+            {
+                vk_image = VK_NULL_HANDLE;
+                /* view_type is not used for Vulkan null descriptors, but make it valid. */
+                vkd3d_desc.view_type = VK_IMAGE_VIEW_TYPE_2D;
+                break;
+            }
             FIXME("Unhandled view dimension %#x.\n", desc->ViewDimension);
             return;
     }
 
-    WARN("Creating NULL UAV %#x.\n", desc->ViewDimension);
+    if (!device->vk_info.EXT_robustness2)
+        WARN("Creating NULL UAV %#x.\n", desc->ViewDimension);
 
     vkd3d_desc.format = vkd3d_get_format(device, VKD3D_NULL_VIEW_FORMAT, false);
     vkd3d_desc.miplevel_idx = 0;
@@ -2912,7 +3171,8 @@ static void vkd3d_create_null_uav(struct d3d12_desc *descriptor,
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_UAV;
     descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_STORAGE_IMAGE;
-    descriptor->u.view = view;
+    descriptor->u.view_info.view = view;
+    descriptor->u.view_info.written_serial_id = view->serial_id;
 }
 
 static void vkd3d_create_buffer_uav(struct d3d12_desc *descriptor, struct d3d12_device *device,
@@ -2942,7 +3202,8 @@ static void vkd3d_create_buffer_uav(struct d3d12_desc *descriptor, struct d3d12_
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_UAV;
     descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER;
-    descriptor->u.view = view;
+    descriptor->u.view_info.view = view;
+    descriptor->u.view_info.written_serial_id = view->serial_id;
 
     if (counter_resource)
     {
@@ -3017,7 +3278,8 @@ static void vkd3d_create_texture_uav(struct d3d12_desc *descriptor,
 
     descriptor->magic = VKD3D_DESCRIPTOR_MAGIC_UAV;
     descriptor->vk_descriptor_type = VK_DESCRIPTOR_TYPE_STORAGE_IMAGE;
-    descriptor->u.view = view;
+    descriptor->u.view_info.view = view;
+    descriptor->u.view_info.written_serial_id = view->serial_id;
 }
 
 void d3d12_desc_create_uav(struct d3d12_desc *descriptor, struct d3d12_device *device,
@@ -3179,7 +3441,8 @@ void d3d12_desc_create_sampler(struct d3d12_desc *sampler,
 
     sampler->magic = VKD3D_DESCRIPTOR_MAGIC_SAMPLER;
     sampler->vk_descriptor_type = VK_DESCRIPTOR_TYPE_SAMPLER;
-    sampler->u.view = view;
+    sampler->u.view_info.view = view;
+    sampler->u.view_info.written_serial_id = view->serial_id;
 }
 
 HRESULT vkd3d_create_static_sampler(struct d3d12_device *device,
@@ -3425,9 +3688,12 @@ static ULONG STDMETHODCALLTYPE d3d12_descriptor_heap_Release(ID3D12DescriptorHea
 
     if (!refcount)
     {
+        const struct vkd3d_vk_device_procs *vk_procs;
         struct d3d12_device *device = heap->device;
         unsigned int i;
 
+        vk_procs = &device->vk_procs;
+
         vkd3d_private_store_destroy(&heap->private_store);
 
         switch (heap->desc.Type)
@@ -3474,6 +3740,9 @@ static ULONG STDMETHODCALLTYPE d3d12_descriptor_heap_Release(ID3D12DescriptorHea
                 break;
         }
 
+        VK_CALL(vkDestroyDescriptorPool(device->vk_device, heap->vk_descriptor_pool, NULL));
+        vkd3d_mutex_destroy(&heap->vk_sets_mutex);
+
         vkd3d_free(heap);
 
         d3d12_device_release(device);
@@ -3584,6 +3853,152 @@ static const struct ID3D12DescriptorHeapVtbl d3d12_descriptor_heap_vtbl =
     d3d12_descriptor_heap_GetGPUDescriptorHandleForHeapStart,
 };
 
+const enum vkd3d_vk_descriptor_set_index vk_descriptor_set_index_table[] =
+{
+    VKD3D_SET_INDEX_SAMPLER,
+    VKD3D_SET_INDEX_COUNT,
+    VKD3D_SET_INDEX_SAMPLED_IMAGE,
+    VKD3D_SET_INDEX_STORAGE_IMAGE,
+    VKD3D_SET_INDEX_UNIFORM_TEXEL_BUFFER,
+    VKD3D_SET_INDEX_STORAGE_TEXEL_BUFFER,
+    VKD3D_SET_INDEX_UNIFORM_BUFFER,
+};
+
+static HRESULT d3d12_descriptor_heap_create_descriptor_pool(struct d3d12_descriptor_heap *descriptor_heap,
+        struct d3d12_device *device, const D3D12_DESCRIPTOR_HEAP_DESC *desc)
+{
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkDescriptorPoolSize pool_sizes[VKD3D_SET_INDEX_COUNT];
+    struct VkDescriptorPoolCreateInfo pool_desc;
+    VkDevice vk_device = device->vk_device;
+    enum vkd3d_vk_descriptor_set_index set;
+    VkResult vr;
+
+    for (set = 0, pool_desc.poolSizeCount = 0; set < ARRAY_SIZE(device->vk_descriptor_heap_layouts); ++set)
+    {
+        if (device->vk_descriptor_heap_layouts[set].applicable_heap_type == desc->Type)
+        {
+            pool_sizes[pool_desc.poolSizeCount].type = device->vk_descriptor_heap_layouts[set].type;
+            pool_sizes[pool_desc.poolSizeCount++].descriptorCount = desc->NumDescriptors;
+        }
+    }
+
+    pool_desc.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
+    pool_desc.pNext = NULL;
+    pool_desc.flags = VK_DESCRIPTOR_POOL_CREATE_UPDATE_AFTER_BIND_BIT_EXT;
+    pool_desc.maxSets = pool_desc.poolSizeCount;
+    pool_desc.pPoolSizes = pool_sizes;
+    if ((vr = VK_CALL(vkCreateDescriptorPool(vk_device, &pool_desc, NULL, &descriptor_heap->vk_descriptor_pool))) < 0)
+        ERR("Failed to create descriptor pool, vr %d.\n", vr);
+
+    return hresult_from_vk_result(vr);
+}
+
+static HRESULT d3d12_descriptor_heap_create_descriptor_set(struct d3d12_descriptor_heap *descriptor_heap,
+        struct d3d12_device *device, unsigned int set)
+{
+    struct d3d12_descriptor_heap_vk_set *descriptor_set = &descriptor_heap->vk_descriptor_sets[set];
+    uint32_t variable_binding_size = descriptor_heap->desc.NumDescriptors;
+    const struct vkd3d_vk_device_procs *vk_procs = &device->vk_procs;
+    VkDescriptorSetVariableDescriptorCountAllocateInfoEXT set_size;
+    VkDescriptorSetAllocateInfo set_desc;
+    unsigned int i;
+    VkResult vr;
+
+    set_desc.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
+    set_desc.pNext = &set_size;
+    set_desc.descriptorPool = descriptor_heap->vk_descriptor_pool;
+    set_desc.descriptorSetCount = 1;
+    set_desc.pSetLayouts = &device->vk_descriptor_heap_layouts[set].vk_set_layout;
+    set_size.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_VARIABLE_DESCRIPTOR_COUNT_ALLOCATE_INFO_EXT;
+    set_size.pNext = NULL;
+    set_size.descriptorSetCount = 1;
+    set_size.pDescriptorCounts = &variable_binding_size;
+    if ((vr = VK_CALL(vkAllocateDescriptorSets(device->vk_device, &set_desc, &descriptor_set->vk_set))) >= 0)
+    {
+        for (i = 0; i < ARRAY_SIZE(descriptor_set->vk_descriptor_writes); ++i)
+            descriptor_set->vk_descriptor_writes[i].dstSet = descriptor_set->vk_set;
+        return S_OK;
+    }
+
+    ERR("Failed to allocate descriptor set, vr %d.\n", vr);
+    return hresult_from_vk_result(vr);
+}
+
+static HRESULT d3d12_descriptor_heap_vk_descriptor_sets_init(struct d3d12_descriptor_heap *descriptor_heap,
+        struct d3d12_device *device, const D3D12_DESCRIPTOR_HEAP_DESC *desc)
+{
+    enum vkd3d_vk_descriptor_set_index set;
+    HRESULT hr;
+
+    descriptor_heap->vk_descriptor_pool = VK_NULL_HANDLE;
+    memset(descriptor_heap->vk_descriptor_sets, 0, sizeof(descriptor_heap->vk_descriptor_sets));
+    vkd3d_mutex_init(&descriptor_heap->vk_sets_mutex);
+
+    if (!device->use_vk_heaps || (desc->Type != D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV
+            && desc->Type != D3D12_DESCRIPTOR_HEAP_TYPE_SAMPLER))
+        return S_OK;
+
+    if (FAILED(hr = d3d12_descriptor_heap_create_descriptor_pool(descriptor_heap, device, desc)))
+        return hr;
+
+    for (set = 0; set < ARRAY_SIZE(descriptor_heap->vk_descriptor_sets); ++set)
+    {
+        struct d3d12_descriptor_heap_vk_set *descriptor_set = &descriptor_heap->vk_descriptor_sets[set];
+        unsigned int i;
+
+        for (i = 0; i < ARRAY_SIZE(descriptor_set->vk_descriptor_writes); ++i)
+        {
+            descriptor_set->vk_descriptor_writes[i].sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
+            descriptor_set->vk_descriptor_writes[i].pNext = NULL;
+            descriptor_set->vk_descriptor_writes[i].dstBinding = 0;
+            descriptor_set->vk_descriptor_writes[i].descriptorType = device->vk_descriptor_heap_layouts[set].type;
+            descriptor_set->vk_descriptor_writes[i].pImageInfo = NULL;
+            descriptor_set->vk_descriptor_writes[i].pBufferInfo = NULL;
+            descriptor_set->vk_descriptor_writes[i].pTexelBufferView = NULL;
+        }
+        switch (device->vk_descriptor_heap_layouts[set].type)
+        {
+            case VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER:
+            case VK_DESCRIPTOR_TYPE_UNIFORM_TEXEL_BUFFER:
+            case VK_DESCRIPTOR_TYPE_STORAGE_TEXEL_BUFFER:
+                break;
+            case VK_DESCRIPTOR_TYPE_SAMPLED_IMAGE:
+                descriptor_set->vk_descriptor_writes[0].pImageInfo = &descriptor_set->vk_image_infos[0];
+                for (i = 0; i < ARRAY_SIZE(descriptor_set->vk_image_infos); ++i)
+                {
+                    descriptor_set->vk_image_infos[i].sampler = VK_NULL_HANDLE;
+                    descriptor_set->vk_image_infos[i].imageLayout = VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL;
+                }
+                break;
+            case VK_DESCRIPTOR_TYPE_STORAGE_IMAGE:
+                descriptor_set->vk_descriptor_writes[0].pImageInfo = &descriptor_set->vk_image_infos[0];
+                for (i = 0; i < ARRAY_SIZE(descriptor_set->vk_image_infos); ++i)
+                {
+                    descriptor_set->vk_image_infos[i].sampler = VK_NULL_HANDLE;
+                    descriptor_set->vk_image_infos[i].imageLayout = VK_IMAGE_LAYOUT_GENERAL;
+                }
+                break;
+            case VK_DESCRIPTOR_TYPE_SAMPLER:
+                descriptor_set->vk_descriptor_writes[0].pImageInfo = &descriptor_set->vk_image_infos[0];
+                for (i = 0; i < ARRAY_SIZE(descriptor_set->vk_image_infos); ++i)
+                {
+                    descriptor_set->vk_image_infos[i].imageView = VK_NULL_HANDLE;
+                    descriptor_set->vk_image_infos[i].imageLayout = VK_IMAGE_LAYOUT_UNDEFINED;
+                }
+                break;
+            default:
+                ERR("Unhandled descriptor type %#x.\n", device->vk_descriptor_heap_layouts[set].type);
+                return E_FAIL;
+        }
+        if (device->vk_descriptor_heap_layouts[set].applicable_heap_type == desc->Type
+                && FAILED(hr = d3d12_descriptor_heap_create_descriptor_set(descriptor_heap, device, set)))
+            return hr;
+    }
+
+    return S_OK;
+}
+
 static HRESULT d3d12_descriptor_heap_init(struct d3d12_descriptor_heap *descriptor_heap,
         struct d3d12_device *device, const D3D12_DESCRIPTOR_HEAP_DESC *desc)
 {
@@ -3591,12 +4006,15 @@ static HRESULT d3d12_descriptor_heap_init(struct d3d12_descriptor_heap *descript
 
     descriptor_heap->ID3D12DescriptorHeap_iface.lpVtbl = &d3d12_descriptor_heap_vtbl;
     descriptor_heap->refcount = 1;
+    descriptor_heap->serial_id = InterlockedIncrement64(&object_global_serial_id);
 
     descriptor_heap->desc = *desc;
 
     if (FAILED(hr = vkd3d_private_store_init(&descriptor_heap->private_store)))
         return hr;
 
+    d3d12_descriptor_heap_vk_descriptor_sets_init(descriptor_heap, device, desc);
+
     d3d12_device_add_ref(descriptor_heap->device = device);
 
     return S_OK;
@@ -4077,6 +4495,9 @@ HRESULT vkd3d_init_null_resources(struct vkd3d_null_resources *null_resources,
 
     memset(null_resources, 0, sizeof(*null_resources));
 
+    if (device->vk_info.EXT_robustness2)
+        return S_OK;
+
     memset(&heap_properties, 0, sizeof(heap_properties));
     heap_properties.Type = D3D12_HEAP_TYPE_DEFAULT;
 
diff --git a/libs/vkd3d/libs/vkd3d/state.c b/libs/vkd3d/libs/vkd3d/state.c
index 2d3954d29..7a29ade8b 100644
--- a/libs/vkd3d/libs/vkd3d/state.c
+++ b/libs/vkd3d/libs/vkd3d/state.c
@@ -92,6 +92,8 @@ static void d3d12_root_signature_cleanup(struct d3d12_root_signature *root_signa
     if (root_signature->descriptor_mapping)
         vkd3d_free(root_signature->descriptor_mapping);
     vkd3d_free(root_signature->descriptor_offsets);
+    vkd3d_free(root_signature->uav_counter_mapping);
+    vkd3d_free(root_signature->uav_counter_offsets);
     if (root_signature->root_constants)
         vkd3d_free(root_signature->root_constants);
 
@@ -327,6 +329,7 @@ static bool vk_binding_from_d3d12_descriptor_range(struct VkDescriptorSetLayoutB
 struct d3d12_root_signature_info
 {
     size_t binding_count;
+    size_t uav_range_count;
 
     size_t root_constant_count;
     size_t root_descriptor_count;
@@ -401,6 +404,7 @@ static HRESULT d3d12_root_signature_info_count_descriptors(struct d3d12_root_sig
                 info->binding_count += binding_count;
                 info->uav_count += count * 2u;
                 uav_unbounded_range |= unbounded;
+                ++info->uav_range_count;
                 break;
             case D3D12_DESCRIPTOR_RANGE_TYPE_CBV:
                 info->cbv_count += count;
@@ -495,6 +499,7 @@ static HRESULT d3d12_root_signature_init_push_constants(struct d3d12_root_signat
         uint32_t *push_constant_range_count)
 {
     uint32_t push_constants_offset[D3D12_SHADER_VISIBILITY_PIXEL + 1];
+    bool use_vk_heaps = root_signature->device->use_vk_heaps;
     unsigned int i, j, push_constant_count;
     uint32_t offset;
 
@@ -507,7 +512,8 @@ static HRESULT d3d12_root_signature_init_push_constants(struct d3d12_root_signat
             continue;
 
         assert(p->ShaderVisibility <= D3D12_SHADER_VISIBILITY_PIXEL);
-        push_constants[p->ShaderVisibility].stageFlags = stage_flags_from_visibility(p->ShaderVisibility);
+        push_constants[p->ShaderVisibility].stageFlags = use_vk_heaps ? VK_SHADER_STAGE_ALL
+                : stage_flags_from_visibility(p->ShaderVisibility);
         push_constants[p->ShaderVisibility].size += p->u.Constants.Num32BitValues * sizeof(uint32_t);
     }
     if (push_constants[D3D12_SHADER_VISIBILITY_ALL].size)
@@ -586,6 +592,8 @@ struct vkd3d_descriptor_set_context
     unsigned int table_index;
     unsigned int unbounded_offset;
     unsigned int descriptor_index;
+    unsigned int uav_counter_index;
+    unsigned int push_constant_index;
     uint32_t descriptor_binding;
 };
 
@@ -595,6 +603,7 @@ static bool vkd3d_validate_descriptor_set_count(struct d3d12_device *device, uns
 
     if (set_count > max_count)
     {
+        /* NOTE: If maxBoundDescriptorSets is < 9, try VKD3D_CONFIG=virtual_heaps */
         ERR("Required descriptor set count exceeds maximum allowed count of %u.\n", max_count);
         return false;
     }
@@ -802,6 +811,122 @@ static void d3d12_root_signature_map_vk_unbounded_binding(struct d3d12_root_sign
     offset->dynamic_offset_index = ~0u;
 }
 
+static unsigned int vk_heap_binding_count_from_descriptor_range(const struct d3d12_root_descriptor_table_range *range,
+        unsigned int descriptor_set_size)
+{
+    unsigned int max_count;
+
+    if (descriptor_set_size <= range->offset)
+    {
+        ERR("Descriptor range offset %u exceeds maximum available offset %u.\n", range->offset, descriptor_set_size - 1);
+        max_count = 0;
+    }
+    else
+    {
+        max_count = descriptor_set_size - range->offset;
+    }
+
+    if (range->descriptor_count != UINT_MAX)
+    {
+        if (range->descriptor_count > max_count)
+            ERR("Range size %u exceeds available descriptor count %u.\n", range->descriptor_count, max_count);
+        return range->descriptor_count;
+    }
+    else
+    {
+        /* Prefer an unsupported binding count vs a zero count, because shader compilation will fail
+         * to match a declaration to a zero binding, resulting in failure of pipline state creation. */
+        return max_count + !max_count;
+    }
+}
+
+static void vkd3d_descriptor_heap_binding_from_descriptor_range(const struct d3d12_root_descriptor_table_range *range,
+        bool is_buffer, const struct d3d12_root_signature *root_signature,
+        struct vkd3d_shader_descriptor_binding *binding)
+{
+    const struct vkd3d_device_descriptor_limits *descriptor_limits = &root_signature->device->vk_info.descriptor_limits;
+    unsigned int descriptor_set_size;
+
+    switch (range->type)
+    {
+        case VKD3D_SHADER_DESCRIPTOR_TYPE_SRV:
+            binding->set = is_buffer ? VKD3D_SET_INDEX_UNIFORM_TEXEL_BUFFER : VKD3D_SET_INDEX_SAMPLED_IMAGE;
+            descriptor_set_size = descriptor_limits->sampled_image_max_descriptors;
+            break;
+        case VKD3D_SHADER_DESCRIPTOR_TYPE_UAV:
+            binding->set = is_buffer ? VKD3D_SET_INDEX_STORAGE_TEXEL_BUFFER : VKD3D_SET_INDEX_STORAGE_IMAGE;
+            descriptor_set_size = descriptor_limits->storage_image_max_descriptors;
+            break;
+        case VKD3D_SHADER_DESCRIPTOR_TYPE_CBV:
+            binding->set = VKD3D_SET_INDEX_UNIFORM_BUFFER;
+            descriptor_set_size = descriptor_limits->uniform_buffer_max_descriptors;
+            break;
+        case VKD3D_SHADER_DESCRIPTOR_TYPE_SAMPLER:
+            binding->set = VKD3D_SET_INDEX_SAMPLER;
+            descriptor_set_size = descriptor_limits->sampler_max_descriptors;
+            break;
+        default:
+            FIXME("Unhandled descriptor range type type %#x.\n", range->type);
+            binding->set = VKD3D_SET_INDEX_SAMPLED_IMAGE;
+            descriptor_set_size = descriptor_limits->sampled_image_max_descriptors;
+            break;
+    }
+    binding->set += root_signature->vk_set_count;
+    binding->binding = 0;
+    binding->count = vk_heap_binding_count_from_descriptor_range(range, descriptor_set_size);
+}
+
+static void d3d12_root_signature_map_vk_heap_binding(struct d3d12_root_signature *root_signature,
+        const struct d3d12_root_descriptor_table_range *range, bool buffer_descriptor,
+        enum vkd3d_shader_visibility shader_visibility, struct vkd3d_descriptor_set_context *context)
+{
+    struct vkd3d_shader_resource_binding *mapping = &root_signature->descriptor_mapping[context->descriptor_index];
+    struct vkd3d_shader_descriptor_offset *offset = &root_signature->descriptor_offsets[context->descriptor_index++];
+
+    mapping->type = range->type;
+    mapping->register_space = range->register_space;
+    mapping->register_index = range->base_register_idx;
+    mapping->shader_visibility = shader_visibility;
+    mapping->flags = buffer_descriptor ? VKD3D_SHADER_BINDING_FLAG_BUFFER : VKD3D_SHADER_BINDING_FLAG_IMAGE;
+    vkd3d_descriptor_heap_binding_from_descriptor_range(range, buffer_descriptor, root_signature, &mapping->binding);
+    offset->static_offset = range->offset;
+    offset->dynamic_offset_index = context->push_constant_index;
+}
+
+static void d3d12_root_signature_map_vk_heap_uav_counter(struct d3d12_root_signature *root_signature,
+        const struct d3d12_root_descriptor_table_range *range, enum vkd3d_shader_visibility shader_visibility,
+        struct vkd3d_descriptor_set_context *context)
+{
+    struct vkd3d_shader_uav_counter_binding *mapping = &root_signature->uav_counter_mapping[context->uav_counter_index];
+    struct vkd3d_shader_descriptor_offset *offset = &root_signature->uav_counter_offsets[context->uav_counter_index++];
+
+    mapping->register_space = range->register_space;
+    mapping->register_index = range->base_register_idx;
+    mapping->shader_visibility = shader_visibility;
+    mapping->binding.set = root_signature->vk_set_count + VKD3D_SET_INDEX_UAV_COUNTER;
+    mapping->binding.binding = 0;
+    mapping->binding.count = vk_heap_binding_count_from_descriptor_range(range,
+            root_signature->device->vk_info.descriptor_limits.storage_image_max_descriptors);
+    offset->static_offset = range->offset;
+    offset->dynamic_offset_index = context->push_constant_index;
+}
+
+static void d3d12_root_signature_map_descriptor_heap_binding(struct d3d12_root_signature *root_signature,
+        const struct d3d12_root_descriptor_table_range *range, enum vkd3d_shader_visibility shader_visibility,
+        struct vkd3d_descriptor_set_context *context)
+{
+    bool is_buffer = range->type == VKD3D_SHADER_DESCRIPTOR_TYPE_CBV;
+
+    if (range->type == VKD3D_SHADER_DESCRIPTOR_TYPE_SRV || range->type == VKD3D_SHADER_DESCRIPTOR_TYPE_UAV)
+    {
+        d3d12_root_signature_map_vk_heap_binding(root_signature, range, true, shader_visibility, context);
+        if (range->type == VKD3D_SHADER_DESCRIPTOR_TYPE_UAV)
+            d3d12_root_signature_map_vk_heap_uav_counter(root_signature, range, shader_visibility, context);
+    }
+
+    d3d12_root_signature_map_vk_heap_binding(root_signature, range, is_buffer, shader_visibility, context);
+}
+
 static void d3d12_root_signature_map_descriptor_unbounded_binding(struct d3d12_root_signature *root_signature,
         const struct d3d12_root_descriptor_table_range *range, unsigned int descriptor_offset,
         enum vkd3d_shader_visibility shader_visibility, struct vkd3d_descriptor_set_context *context)
@@ -868,6 +993,7 @@ static HRESULT d3d12_root_signature_init_root_descriptor_tables(struct d3d12_roo
         struct vkd3d_descriptor_set_context *context)
 {
     const struct d3d12_device *device = root_signature->device;
+    bool use_vk_heaps = root_signature->device->use_vk_heaps;
     struct d3d12_root_descriptor_table *table;
     unsigned int i, j, k, range_count;
     uint32_t vk_binding;
@@ -935,6 +1061,16 @@ static HRESULT d3d12_root_signature_init_root_descriptor_tables(struct d3d12_roo
 
             range = &table->ranges[j];
 
+            if (use_vk_heaps)
+            {
+                 /* set, binding and vk_binding_count are not used. */
+                range->set = 0;
+                range->binding = 0;
+                range->vk_binding_count = 0;
+                d3d12_root_signature_map_descriptor_heap_binding(root_signature, range, shader_visibility, context);
+                continue;
+            }
+
             range->set = root_signature->vk_set_count - root_signature->main_set;
 
             if (root_signature->use_descriptor_arrays)
@@ -1014,6 +1150,7 @@ static HRESULT d3d12_root_signature_init_root_descriptor_tables(struct d3d12_roo
 
             context->current_binding = cur_binding;
         }
+        ++context->push_constant_index;
     }
 
     return S_OK;
@@ -1084,9 +1221,36 @@ static HRESULT d3d12_root_signature_init_static_samplers(struct d3d12_root_signa
     }
 
     context->current_binding = cur_binding;
+    if (device->use_vk_heaps)
+        return d3d12_root_signature_append_descriptor_set_layout(root_signature, context, 0);
+
     return S_OK;
 }
 
+static void d3d12_root_signature_init_descriptor_table_push_constants(struct d3d12_root_signature *root_signature,
+        const struct vkd3d_descriptor_set_context *context)
+{
+    root_signature->descriptor_table_offset = 0;
+    if ((root_signature->descriptor_table_count = context->push_constant_index))
+    {
+        VkPushConstantRange *range = &root_signature->push_constant_ranges[D3D12_SHADER_VISIBILITY_ALL];
+
+        root_signature->descriptor_table_offset = align(range->size, 16);
+        range->size = root_signature->descriptor_table_offset
+                + root_signature->descriptor_table_count * sizeof(uint32_t);
+
+        if (range->size > root_signature->device->vk_info.device_limits.maxPushConstantsSize)
+            FIXME("Push constants size %u exceeds maximum allowed size %u. Try VKD3D_CONFIG=virtual_heaps.\n",
+                    range->size, root_signature->device->vk_info.device_limits.maxPushConstantsSize);
+
+        if (!root_signature->push_constant_range_count)
+        {
+            root_signature->push_constant_range_count = 1;
+            range->stageFlags = VK_SHADER_STAGE_ALL;
+        }
+    }
+}
+
 static bool vk_binding_uses_partial_binding(const VkDescriptorSetLayoutBinding *binding)
 {
     if (binding->descriptorCount == 1)
@@ -1194,11 +1358,19 @@ static HRESULT vkd3d_create_pipeline_layout(struct d3d12_device *device,
 static unsigned int d3d12_root_signature_copy_descriptor_set_layouts(const struct d3d12_root_signature *root_signature,
         VkDescriptorSetLayout *vk_set_layouts)
 {
+    const struct d3d12_device *device = root_signature->device;
+    enum vkd3d_vk_descriptor_set_index set;
     unsigned int i;
 
     for (i = 0; i < root_signature->vk_set_count; ++i)
         vk_set_layouts[i] = root_signature->descriptor_set_layouts[i].vk_layout;
 
+    if (device->use_vk_heaps)
+    {
+        for (set = 0; set < ARRAY_SIZE(device->vk_descriptor_heap_layouts); ++set)
+            vk_set_layouts[i++] = device->vk_descriptor_heap_layouts[set].vk_set_layout;
+    }
+
     return i;
 }
 
@@ -1210,6 +1382,7 @@ static HRESULT d3d12_root_signature_init(struct d3d12_root_signature *root_signa
     struct vkd3d_descriptor_set_context context;
     VkDescriptorSetLayoutBinding *binding_desc;
     struct d3d12_root_signature_info info;
+    bool use_vk_heaps;
     unsigned int i;
     HRESULT hr;
 
@@ -1226,6 +1399,8 @@ static HRESULT d3d12_root_signature_init(struct d3d12_root_signature *root_signa
     root_signature->flags = desc->Flags;
     root_signature->descriptor_mapping = NULL;
     root_signature->descriptor_offsets = NULL;
+    root_signature->uav_counter_mapping = NULL;
+    root_signature->uav_counter_offsets = NULL;
     root_signature->static_sampler_count = 0;
     root_signature->static_samplers = NULL;
     root_signature->device = device;
@@ -1243,9 +1418,13 @@ static HRESULT d3d12_root_signature_init(struct d3d12_root_signature *root_signa
     }
 
     root_signature->binding_count = info.binding_count;
+    root_signature->uav_mapping_count = info.uav_range_count;
     root_signature->static_sampler_count = desc->NumStaticSamplers;
     root_signature->root_descriptor_count = info.root_descriptor_count;
     root_signature->use_descriptor_arrays = device->vk_info.EXT_descriptor_indexing;
+    root_signature->descriptor_table_count = 0;
+
+    use_vk_heaps = device->use_vk_heaps;
 
     hr = E_OUTOFMEMORY;
     root_signature->parameter_count = desc->NumParameters;
@@ -1255,6 +1434,11 @@ static HRESULT d3d12_root_signature_init(struct d3d12_root_signature *root_signa
     if (!(root_signature->descriptor_mapping = vkd3d_calloc(root_signature->binding_count,
             sizeof(*root_signature->descriptor_mapping))))
         goto fail;
+    if (use_vk_heaps && (!(root_signature->uav_counter_mapping = vkd3d_calloc(root_signature->uav_mapping_count,
+            sizeof(*root_signature->uav_counter_mapping)))
+            || !(root_signature->uav_counter_offsets = vkd3d_calloc(root_signature->uav_mapping_count,
+            sizeof(*root_signature->uav_counter_offsets)))))
+        goto fail;
     if (root_signature->use_descriptor_arrays && !(root_signature->descriptor_offsets = vkd3d_calloc(
             root_signature->binding_count, sizeof(*root_signature->descriptor_offsets))))
         goto fail;
@@ -1289,8 +1473,11 @@ static HRESULT d3d12_root_signature_init(struct d3d12_root_signature *root_signa
         goto fail;
     if (FAILED(hr = d3d12_root_signature_init_static_samplers(root_signature, device, desc, &context)))
         goto fail;
+    context.push_constant_index = 0;
     if (FAILED(hr = d3d12_root_signature_init_root_descriptor_tables(root_signature, desc, &info, &context)))
         goto fail;
+    if (use_vk_heaps)
+        d3d12_root_signature_init_descriptor_table_push_constants(root_signature, &context);
 
     if (FAILED(hr = d3d12_root_signature_append_descriptor_set_layout(root_signature, &context, 0)))
         goto fail;
@@ -1770,7 +1957,7 @@ static HRESULT create_shader_stage(struct d3d12_device *device,
 
     static const struct vkd3d_shader_compile_option options[] =
     {
-        {VKD3D_SHADER_COMPILE_OPTION_API_VERSION, VKD3D_SHADER_API_VERSION_1_3},
+        {VKD3D_SHADER_COMPILE_OPTION_API_VERSION, VKD3D_SHADER_API_VERSION_1_4},
     };
 
     stage_desc->sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
@@ -1821,7 +2008,7 @@ static int vkd3d_scan_dxbc(const D3D12_SHADER_BYTECODE *code,
 
     static const struct vkd3d_shader_compile_option options[] =
     {
-        {VKD3D_SHADER_COMPILE_OPTION_API_VERSION, VKD3D_SHADER_API_VERSION_1_3},
+        {VKD3D_SHADER_COMPILE_OPTION_API_VERSION, VKD3D_SHADER_API_VERSION_1_4},
     };
 
     compile_info.type = VKD3D_SHADER_STRUCTURE_TYPE_COMPILE_INFO;
@@ -1862,7 +2049,7 @@ static HRESULT vkd3d_create_compute_pipeline(struct d3d12_device *device,
     VK_CALL(vkDestroyShaderModule(device->vk_device, pipeline_info.stage.module, NULL));
     if (vr < 0)
     {
-        WARN("Failed to create Vulkan compute pipeline, hr %#x.", hr);
+        WARN("Failed to create Vulkan compute pipeline, hr %#x.\n", hr);
         return hresult_from_vk_result(vr);
     }
 
@@ -1978,6 +2165,9 @@ static HRESULT d3d12_pipeline_state_find_and_init_uav_counters(struct d3d12_pipe
     HRESULT hr;
     int ret;
 
+    if (device->use_vk_heaps)
+        return S_OK;
+
     shader_info.type = VKD3D_SHADER_STRUCTURE_TYPE_SCAN_DESCRIPTOR_INFO;
     shader_info.next = NULL;
     if ((ret = vkd3d_scan_dxbc(code, &shader_info)) < 0)
@@ -2030,10 +2220,10 @@ static HRESULT d3d12_pipeline_state_init_compute(struct d3d12_pipeline_state *st
     {
         offset_info.type = VKD3D_SHADER_STRUCTURE_TYPE_DESCRIPTOR_OFFSET_INFO;
         offset_info.next = NULL;
-        offset_info.descriptor_table_offset = 0;
-        offset_info.descriptor_table_count = 0;
+        offset_info.descriptor_table_offset = root_signature->descriptor_table_offset;
+        offset_info.descriptor_table_count = root_signature->descriptor_table_count;
         offset_info.binding_offsets = root_signature->descriptor_offsets;
-        offset_info.uav_counter_offsets = NULL;
+        offset_info.uav_counter_offsets = root_signature->uav_counter_offsets;
         vkd3d_prepend_struct(&target_info, &offset_info);
     }
 
@@ -2045,8 +2235,16 @@ static HRESULT d3d12_pipeline_state_init_compute(struct d3d12_pipeline_state *st
     shader_interface.push_constant_buffer_count = root_signature->root_constant_count;
     shader_interface.combined_samplers = NULL;
     shader_interface.combined_sampler_count = 0;
-    shader_interface.uav_counters = state->uav_counters.bindings;
-    shader_interface.uav_counter_count = state->uav_counters.binding_count;
+    if (root_signature->uav_counter_mapping)
+    {
+        shader_interface.uav_counters = root_signature->uav_counter_mapping;
+        shader_interface.uav_counter_count = root_signature->uav_mapping_count;
+    }
+    else
+    {
+        shader_interface.uav_counters = state->uav_counters.bindings;
+        shader_interface.uav_counter_count = state->uav_counters.binding_count;
+    }
 
     vk_pipeline_layout = state->uav_counters.vk_pipeline_layout
             ? state->uav_counters.vk_pipeline_layout : root_signature->vk_pipeline_layout;
@@ -2430,9 +2628,9 @@ static HRESULT compute_input_layout_offsets(const struct d3d12_device *device,
         if (e->AlignedByteOffset != D3D12_APPEND_ALIGNED_ELEMENT)
             offsets[i] = e->AlignedByteOffset;
         else
-            offsets[i] = input_slot_offsets[e->InputSlot];
+            offsets[i] = align(input_slot_offsets[e->InputSlot], min(4, format->byte_count));
 
-        input_slot_offsets[e->InputSlot] = align(offsets[i] + format->byte_count, 4);
+        input_slot_offsets[e->InputSlot] = offsets[i] + format->byte_count;
     }
 
     return S_OK;
@@ -2664,7 +2862,7 @@ static HRESULT d3d12_pipeline_state_init_graphics(struct d3d12_pipeline_state *s
         }
         if (rt_desc->BlendEnable && rt_desc->LogicOpEnable)
         {
-            WARN("Only one of BlendEnable or LogicOpEnable can be set to TRUE.");
+            WARN("Only one of BlendEnable or LogicOpEnable can be set to TRUE.\n");
             hr = E_INVALIDARG;
             goto fail;
         }
@@ -2807,10 +3005,10 @@ static HRESULT d3d12_pipeline_state_init_graphics(struct d3d12_pipeline_state *s
     {
         offset_info.type = VKD3D_SHADER_STRUCTURE_TYPE_DESCRIPTOR_OFFSET_INFO;
         offset_info.next = NULL;
-        offset_info.descriptor_table_offset = 0;
-        offset_info.descriptor_table_count = 0;
+        offset_info.descriptor_table_offset = root_signature->descriptor_table_offset;
+        offset_info.descriptor_table_count = root_signature->descriptor_table_count;
         offset_info.binding_offsets = root_signature->descriptor_offsets;
-        offset_info.uav_counter_offsets = NULL;
+        offset_info.uav_counter_offsets = root_signature->uav_counter_offsets;
     }
 
     for (i = 0; i < ARRAY_SIZE(shader_stages); ++i)
@@ -2852,8 +3050,10 @@ static HRESULT d3d12_pipeline_state_init_graphics(struct d3d12_pipeline_state *s
                 break;
 
             case VK_SHADER_STAGE_FRAGMENT_BIT:
-                shader_interface.uav_counters = state->uav_counters.bindings;
-                shader_interface.uav_counter_count = state->uav_counters.binding_count;
+                shader_interface.uav_counters = root_signature->uav_counter_mapping
+                        ? root_signature->uav_counter_mapping : state->uav_counters.bindings;
+                shader_interface.uav_counter_count = root_signature->uav_counter_mapping
+                        ? root_signature->uav_mapping_count : state->uav_counters.binding_count;
                 stage_target_info = &ps_target_info;
                 break;
 
diff --git a/libs/vkd3d/libs/vkd3d/vkd3d_main.c b/libs/vkd3d/libs/vkd3d/vkd3d_main.c
index 21d998bf9..88301fbb3 100644
--- a/libs/vkd3d/libs/vkd3d/vkd3d_main.c
+++ b/libs/vkd3d/libs/vkd3d/vkd3d_main.c
@@ -510,3 +510,9 @@ HRESULT vkd3d_serialize_versioned_root_signature(const D3D12_VERSIONED_ROOT_SIGN
     }
     return hr;
 }
+
+void vkd3d_set_log_callback(PFN_vkd3d_log callback)
+{
+    vkd3d_shader_set_log_callback(callback);
+    vkd3d_dbg_set_log_callback(callback);
+}
diff --git a/libs/vkd3d/libs/vkd3d/vkd3d_private.h b/libs/vkd3d/libs/vkd3d/vkd3d_private.h
index ac93245fe..cb08daae8 100644
--- a/libs/vkd3d/libs/vkd3d/vkd3d_private.h
+++ b/libs/vkd3d/libs/vkd3d/vkd3d_private.h
@@ -59,6 +59,7 @@
 #define VKD3D_MAX_SHADER_EXTENSIONS       3u
 #define VKD3D_MAX_SHADER_STAGES           5u
 #define VKD3D_MAX_VK_SYNC_OBJECTS         4u
+#define VKD3D_MAX_DEVICE_BLOCKED_QUEUES  16u
 #define VKD3D_MAX_DESCRIPTOR_SETS        64u
 /* D3D12 binding tier 3 has a limit of 2048 samplers. */
 #define VKD3D_MAX_DESCRIPTOR_SET_SAMPLERS 2048u
@@ -67,6 +68,8 @@
  * this number to prevent excessive pool memory use. */
 #define VKD3D_MAX_VIRTUAL_HEAP_DESCRIPTORS_PER_TYPE (16 * 1024u)
 
+extern LONG64 object_global_serial_id;
+
 struct d3d12_command_list;
 struct d3d12_device;
 struct d3d12_resource;
@@ -123,12 +126,14 @@ struct vkd3d_vulkan_info
     bool KHR_maintenance3;
     bool KHR_push_descriptor;
     bool KHR_sampler_mirror_clamp_to_edge;
+    bool KHR_timeline_semaphore;
     /* EXT device extensions */
     bool EXT_calibrated_timestamps;
     bool EXT_conditional_rendering;
     bool EXT_debug_marker;
     bool EXT_depth_clip_enable;
     bool EXT_descriptor_indexing;
+    bool EXT_robustness2;
     bool EXT_shader_demote_to_helper_invocation;
     bool EXT_shader_stencil_export;
     bool EXT_texel_buffer_alignment;
@@ -156,6 +161,7 @@ struct vkd3d_vulkan_info
 enum vkd3d_config_flags
 {
     VKD3D_CONFIG_FLAG_VULKAN_DEBUG = 0x00000001,
+    VKD3D_CONFIG_FLAG_VIRTUAL_HEAPS = 0x00000002,
 };
 
 struct vkd3d_instance
@@ -327,7 +333,11 @@ struct vkd3d_waiting_fence
 {
     struct d3d12_fence *fence;
     uint64_t value;
-    struct vkd3d_queue *queue;
+    union
+    {
+        VkFence vk_fence;
+        VkSemaphore vk_semaphore;
+    } u;
     uint64_t queue_sequence_number;
 };
 
@@ -338,28 +348,17 @@ struct vkd3d_fence_worker
     struct vkd3d_cond cond;
     struct vkd3d_cond fence_destruction_cond;
     bool should_exit;
-    bool pending_fence_destruction;
-
-    LONG enqueued_fence_count;
-    struct vkd3d_enqueued_fence
-    {
-        VkFence vk_fence;
-        struct vkd3d_waiting_fence waiting_fence;
-    } *enqueued_fences;
-    size_t enqueued_fences_size;
 
     size_t fence_count;
-    VkFence *vk_fences;
-    size_t vk_fences_size;
     struct vkd3d_waiting_fence *fences;
     size_t fences_size;
 
+    void (*wait_for_gpu_fence)(struct vkd3d_fence_worker *worker, const struct vkd3d_waiting_fence *enqueued_fence);
+
+    struct vkd3d_queue *queue;
     struct d3d12_device *device;
 };
 
-HRESULT vkd3d_fence_worker_start(struct vkd3d_fence_worker *worker, struct d3d12_device *device);
-HRESULT vkd3d_fence_worker_stop(struct vkd3d_fence_worker *worker, struct d3d12_device *device);
-
 struct vkd3d_gpu_va_allocation
 {
     D3D12_GPU_VIRTUAL_ADDRESS base;
@@ -500,20 +499,29 @@ HRESULT vkd3d_set_private_data_interface(struct vkd3d_private_store *store, cons
 
 struct vkd3d_signaled_semaphore
 {
-    struct list entry;
     uint64_t value;
-    VkSemaphore vk_semaphore;
-    VkFence vk_fence;
-    bool is_acquired;
+    union
+    {
+        struct
+        {
+            VkSemaphore vk_semaphore;
+            VkFence vk_fence;
+            bool is_acquired;
+        } binary;
+        uint64_t timeline_value;
+    } u;
+    const struct vkd3d_queue *signalling_queue;
 };
 
 /* ID3D12Fence */
 struct d3d12_fence
 {
     ID3D12Fence ID3D12Fence_iface;
+    LONG internal_refcount;
     LONG refcount;
 
     uint64_t value;
+    uint64_t max_pending_value;
     struct vkd3d_mutex mutex;
     struct vkd3d_cond null_event_cond;
 
@@ -526,10 +534,13 @@ struct d3d12_fence
     size_t events_size;
     size_t event_count;
 
-    struct list semaphores;
-    unsigned int semaphore_count;
+    VkSemaphore timeline_semaphore;
+    uint64_t timeline_value;
+    uint64_t pending_timeline_value;
 
-    LONG pending_worker_operation_count;
+    struct vkd3d_signaled_semaphore *semaphores;
+    size_t semaphores_size;
+    unsigned int semaphore_count;
 
     VkFence old_vk_fences[VKD3D_MAX_VK_SYNC_OBJECTS];
 
@@ -541,6 +552,9 @@ struct d3d12_fence
 HRESULT d3d12_fence_create(struct d3d12_device *device, uint64_t initial_value,
         D3D12_FENCE_FLAGS flags, struct d3d12_fence **fence);
 
+VkResult vkd3d_create_timeline_semaphore(const struct d3d12_device *device, uint64_t initial_value,
+        VkSemaphore *timeline_semaphore);
+
 /* ID3D12Heap */
 struct d3d12_heap
 {
@@ -648,6 +662,7 @@ struct vkd3d_view
 {
     LONG refcount;
     enum vkd3d_view_type type;
+    uint64_t serial_id;
     union
     {
         VkBufferView vk_buffer_view;
@@ -694,6 +709,12 @@ bool vkd3d_create_buffer_view(struct d3d12_device *device, VkBuffer vk_buffer, c
 bool vkd3d_create_texture_view(struct d3d12_device *device, VkImage vk_image,
         const struct vkd3d_texture_view_desc *desc, struct vkd3d_view **view);
 
+struct vkd3d_view_info
+{
+    uint64_t written_serial_id;
+    struct vkd3d_view *view;
+};
+
 struct d3d12_desc
 {
     uint32_t magic;
@@ -701,7 +722,7 @@ struct d3d12_desc
     union
     {
         VkDescriptorBufferInfo vk_cbv_info;
-        struct vkd3d_view *view;
+        struct vkd3d_view_info view_info;
     } u;
 };
 
@@ -772,11 +793,55 @@ static inline struct d3d12_dsv_desc *d3d12_dsv_desc_from_cpu_handle(D3D12_CPU_DE
 void d3d12_dsv_desc_create_dsv(struct d3d12_dsv_desc *dsv_desc, struct d3d12_device *device,
         struct d3d12_resource *resource, const D3D12_DEPTH_STENCIL_VIEW_DESC *desc);
 
+enum vkd3d_vk_descriptor_set_index
+{
+    VKD3D_SET_INDEX_UNIFORM_BUFFER = 0,
+    VKD3D_SET_INDEX_UNIFORM_TEXEL_BUFFER = 1,
+    VKD3D_SET_INDEX_SAMPLED_IMAGE = 2,
+    VKD3D_SET_INDEX_STORAGE_TEXEL_BUFFER = 3,
+    VKD3D_SET_INDEX_STORAGE_IMAGE = 4,
+    VKD3D_SET_INDEX_SAMPLER = 5,
+    VKD3D_SET_INDEX_UAV_COUNTER = 6,
+    VKD3D_SET_INDEX_COUNT = 7
+};
+
+extern const enum vkd3d_vk_descriptor_set_index vk_descriptor_set_index_table[];
+
+static inline enum vkd3d_vk_descriptor_set_index vkd3d_vk_descriptor_set_index_from_vk_descriptor_type(
+        VkDescriptorType type)
+{
+    assert(type <= VK_DESCRIPTOR_TYPE_UNIFORM_BUFFER);
+    assert(vk_descriptor_set_index_table[type] < VKD3D_SET_INDEX_COUNT);
+
+    return vk_descriptor_set_index_table[type];
+}
+
+struct vkd3d_vk_descriptor_heap_layout
+{
+    VkDescriptorType type;
+    bool buffer_dimension;
+    D3D12_DESCRIPTOR_HEAP_TYPE applicable_heap_type;
+    unsigned int count;
+    VkDescriptorSetLayout vk_set_layout;
+};
+
+#define VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE 64
+
+struct d3d12_descriptor_heap_vk_set
+{
+    VkDescriptorSet vk_set;
+    VkDescriptorBufferInfo vk_buffer_infos[VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE];
+    VkBufferView vk_buffer_views[VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE];
+    VkDescriptorImageInfo vk_image_infos[VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE];
+    VkWriteDescriptorSet vk_descriptor_writes[VKD3D_DESCRIPTOR_WRITE_BUFFER_SIZE];
+};
+
 /* ID3D12DescriptorHeap */
 struct d3d12_descriptor_heap
 {
     ID3D12DescriptorHeap ID3D12DescriptorHeap_iface;
     LONG refcount;
+    uint64_t serial_id;
 
     D3D12_DESCRIPTOR_HEAP_DESC desc;
 
@@ -784,12 +849,32 @@ struct d3d12_descriptor_heap
 
     struct vkd3d_private_store private_store;
 
+    VkDescriptorPool vk_descriptor_pool;
+    struct d3d12_descriptor_heap_vk_set vk_descriptor_sets[VKD3D_SET_INDEX_COUNT];
+    struct vkd3d_mutex vk_sets_mutex;
+
     BYTE descriptors[];
 };
 
 HRESULT d3d12_descriptor_heap_create(struct d3d12_device *device,
         const D3D12_DESCRIPTOR_HEAP_DESC *desc, struct d3d12_descriptor_heap **descriptor_heap);
 
+struct d3d12_desc_copy_location
+{
+    struct d3d12_desc src;
+    struct d3d12_desc *dst;
+};
+
+struct d3d12_desc_copy_info
+{
+    unsigned int count;
+    bool uav_counter;
+};
+
+void d3d12_desc_copy_vk_heap_range(struct d3d12_desc_copy_location *locations, const struct d3d12_desc_copy_info *info,
+        struct d3d12_descriptor_heap *descriptor_heap, enum vkd3d_vk_descriptor_set_index set,
+        struct d3d12_device *device);
+
 /* ID3D12QueryHeap */
 struct d3d12_query_heap
 {
@@ -898,8 +983,13 @@ struct d3d12_root_signature
     D3D12_ROOT_SIGNATURE_FLAGS flags;
 
     unsigned int binding_count;
+    unsigned int uav_mapping_count;
     struct vkd3d_shader_resource_binding *descriptor_mapping;
     struct vkd3d_shader_descriptor_offset *descriptor_offsets;
+    struct vkd3d_shader_uav_counter_binding *uav_counter_mapping;
+    struct vkd3d_shader_descriptor_offset *uav_counter_offsets;
+    unsigned int descriptor_table_offset;
+    unsigned int descriptor_table_count;
 
     unsigned int root_constant_count;
     struct vkd3d_shader_push_constant_buffer *root_constants;
@@ -1118,6 +1208,8 @@ struct vkd3d_pipeline_bindings
     struct d3d12_desc *descriptor_tables[D3D12_MAX_ROOT_COST];
     uint64_t descriptor_table_dirty_mask;
     uint64_t descriptor_table_active_mask;
+    uint64_t cbv_srv_uav_heap_id;
+    uint64_t sampler_heap_id;
 
     VkBufferView *vk_uav_counter_views;
     size_t vk_uav_counter_views_size;
@@ -1179,6 +1271,8 @@ struct d3d12_command_list
     VkBuffer so_counter_buffers[D3D12_SO_BUFFER_SLOT_COUNT];
     VkDeviceSize so_counter_buffer_offsets[D3D12_SO_BUFFER_SLOT_COUNT];
 
+    void (*update_descriptors)(struct d3d12_command_list *list, enum vkd3d_pipeline_bind_point bind_point);
+
     struct vkd3d_private_store private_store;
 };
 
@@ -1217,6 +1311,42 @@ HRESULT vkd3d_queue_create(struct d3d12_device *device, uint32_t family_index,
 void vkd3d_queue_destroy(struct vkd3d_queue *queue, struct d3d12_device *device);
 void vkd3d_queue_release(struct vkd3d_queue *queue);
 
+enum vkd3d_cs_op
+{
+    VKD3D_CS_OP_WAIT,
+    VKD3D_CS_OP_SIGNAL,
+    VKD3D_CS_OP_EXECUTE,
+};
+
+struct vkd3d_cs_wait
+{
+    struct d3d12_fence *fence;
+    uint64_t value;
+};
+
+struct vkd3d_cs_signal
+{
+    struct d3d12_fence *fence;
+    uint64_t value;
+};
+
+struct vkd3d_cs_execute
+{
+    VkCommandBuffer *buffers;
+    unsigned int buffer_count;
+};
+
+struct vkd3d_cs_op_data
+{
+    enum vkd3d_cs_op opcode;
+    union
+    {
+        struct vkd3d_cs_wait wait;
+        struct vkd3d_cs_signal signal;
+        struct vkd3d_cs_execute execute;
+    } u;
+};
+
 /* ID3D12CommandQueue */
 struct d3d12_command_queue
 {
@@ -1227,11 +1357,18 @@ struct d3d12_command_queue
 
     struct vkd3d_queue *vkd3d_queue;
 
+    struct vkd3d_fence_worker fence_worker;
     const struct d3d12_fence *last_waited_fence;
     uint64_t last_waited_fence_value;
 
     struct d3d12_device *device;
 
+    struct vkd3d_mutex op_mutex;
+    struct vkd3d_cs_op_data *ops;
+    size_t ops_count;
+    size_t ops_size;
+    bool is_flushing;
+
     struct vkd3d_private_store private_store;
 };
 
@@ -1329,7 +1466,6 @@ struct d3d12_device
 
     struct vkd3d_gpu_descriptor_allocator gpu_descriptor_allocator;
     struct vkd3d_gpu_va_allocator gpu_va_allocator;
-    struct vkd3d_fence_worker fence_worker;
 
     struct vkd3d_mutex mutex;
     struct vkd3d_mutex desc_mutex[8];
@@ -1354,6 +1490,9 @@ struct d3d12_device
     unsigned int queue_family_count;
     VkTimeDomainEXT vk_host_time_domain;
 
+    struct d3d12_command_queue *blocked_queues[VKD3D_MAX_DEVICE_BLOCKED_QUEUES];
+    unsigned int blocked_queue_count;
+
     struct vkd3d_instance *vkd3d_instance;
 
     IUnknown *parent;
@@ -1370,6 +1509,8 @@ struct d3d12_device
     struct vkd3d_uav_clear_state uav_clear_state;
 
     VkDescriptorPoolSize vk_pool_sizes[VKD3D_DESCRIPTOR_POOL_COUNT];
+    struct vkd3d_vk_descriptor_heap_layout vk_descriptor_heap_layouts[VKD3D_SET_INDEX_COUNT];
+    bool use_vk_heaps;
 };
 
 HRESULT d3d12_device_create(struct vkd3d_instance *instance,
diff --git a/libs/vkd3d/libs/vkd3d/vulkan_procs.h b/libs/vkd3d/libs/vkd3d/vulkan_procs.h
index 60556735f..34e0ab4be 100644
--- a/libs/vkd3d/libs/vkd3d/vulkan_procs.h
+++ b/libs/vkd3d/libs/vkd3d/vulkan_procs.h
@@ -195,6 +195,11 @@ VK_DEVICE_EXT_PFN(vkGetDescriptorSetLayoutSupportKHR)
 /* VK_KHR_push_descriptor */
 VK_DEVICE_EXT_PFN(vkCmdPushDescriptorSetKHR)
 
+/* VK_KHR_timeline_semaphore */
+VK_DEVICE_EXT_PFN(vkGetSemaphoreCounterValueKHR)
+VK_DEVICE_EXT_PFN(vkWaitSemaphoresKHR)
+VK_DEVICE_EXT_PFN(vkSignalSemaphoreKHR)
+
 /* VK_EXT_calibrated_timestamps */
 VK_DEVICE_EXT_PFN(vkGetCalibratedTimestampsEXT)
 
